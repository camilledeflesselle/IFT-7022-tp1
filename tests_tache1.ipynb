{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camilledeflesselle/IFT-7022-tp1/blob/main/tests_tache1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7oKWjjFPk_M",
        "outputId": "dd5de661-34a8-4412-8a6a-fac37fa79003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Laval/session3/IFT-7022 : Traitement automatique de la langue naturelle/IFT-7022-tp2/t1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlnwNepyP4HG",
        "outputId": "47700786-60a0-44f2-8ed7-c0935a019c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Laval/session3/IFT-7022 : Traitement automatique de la langue naturelle/IFT-7022-tp2/t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZyknkLDPfzE"
      },
      "source": [
        "## TÂCHE 1 – CLASSIFICATION DE QUESTIONS – RÉSEAU FEEDFORWARD (MLP) ET PLONGEMENTS DE MOTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Du3IyVQPfzK",
        "outputId": "aa7d186e-88c5-47be-8036-c555ad4bae0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting poutyne\n",
            "  Downloading Poutyne-1.13-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 25.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from poutyne) (1.12.1+cu113)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from poutyne) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->poutyne) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics->poutyne) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics->poutyne) (3.0.9)\n",
            "Installing collected packages: torchmetrics, poutyne\n",
            "Successfully installed poutyne-1.13 torchmetrics-0.10.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3160750 sha256=f2d7265a521e706289266ef1a76c428c842ef2423e4586bacd0ba8075fc8f281\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.1\n"
          ]
        }
      ],
      "source": [
        "%pip install poutyne\n",
        "%pip install gensim\n",
        "%pip install fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y16-agVPfzP"
      },
      "source": [
        "### Tâche 1.a – FastText vs. Word2Vec\n",
        "Comparez l’efficacité des modèles avec les plongements de mots préentraînés de FastText et ceux de\n",
        "Word2Vec (par l’entremise de Gensim).\n",
        "Présentez les résultats clairement à l’aide de figures. Analysez les résultats. Expliquez les fonctions\n",
        "particulières qui ont été implémentées pour pouvoir utiliser les plongements de FastText et de Word2Vec. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KOsomcRPfzV"
      },
      "source": [
        "#### FastText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "execfile(\"train.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWA-k6fKCkRu",
        "outputId": "e222d644-15a8-43db-c715-2ed1088445c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Taille de couche cachée égale à : 100, avec les embeddings préentraînés de fasttext aggrégés avec la méthode average :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Loading weights from model/fasttext/average_embedding_mlp_100/checkpoint.ckpt and starting at epoch 301.\n",
            "Loading optimizer state from model/fasttext/average_embedding_mlp_100/checkpoint.optim and starting at epoch 301.\n",
            "Loading random states from model/fasttext/average_embedding_mlp_100/checkpoint.randomstate and starting at epoch 301.\n",
            "Restoring data from model/fasttext/average_embedding_mlp_100/checkpoint_epoch_300.ckpt\n",
            "Found best checkpoint at epoch: 300\n",
            "lr: 0.01, loss: 0.687159, acc: 77.4245, fscore_macro: 0.698387, val_loss: 0.805563, val_acc: 73.1532, val_fscore_macro: 0.643931\n",
            "Loading checkpoint model/fasttext/average_embedding_mlp_100/checkpoint_epoch_300.ckpt\n",
            "Running test\n",
            "Test steps: 32 1.85s test_loss: 0.560922 test_acc: 82.600000 test_fscore_macro: 0.705120     \n",
            "\n",
            " Taille de couche cachée égale à : 100, avec les embeddings préentraînés de fasttext aggrégés avec la méthode maxpool :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Loading weights from model/fasttext/maxpool_embedding_mlp_100/checkpoint.ckpt and starting at epoch 301.\n",
            "Loading optimizer state from model/fasttext/maxpool_embedding_mlp_100/checkpoint.optim and starting at epoch 301.\n",
            "Loading random states from model/fasttext/maxpool_embedding_mlp_100/checkpoint.randomstate and starting at epoch 301.\n",
            "Restoring data from model/fasttext/maxpool_embedding_mlp_100/checkpoint_epoch_244.ckpt\n",
            "Found best checkpoint at epoch: 244\n",
            "lr: 0.01, loss: 0.841245, acc: 70.7858, fscore_macro: 0.660515, val_loss: 1.10259, val_acc: 63.4234, val_fscore_macro: 0.589411\n",
            "Loading checkpoint model/fasttext/maxpool_embedding_mlp_100/checkpoint_epoch_244.ckpt\n",
            "Running test\n",
            "Test steps: 32 1.23s test_loss: 0.687991 test_acc: 78.000000 test_fscore_macro: 0.701563    \n",
            "\n",
            " Taille de couche cachée égale à : 200, avec les embeddings préentraînés de fasttext aggrégés avec la méthode average :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Loading weights from model/fasttext/average_embedding_mlp_200/checkpoint.ckpt and starting at epoch 88.\n",
            "Loading optimizer state from model/fasttext/average_embedding_mlp_200/checkpoint.optim and starting at epoch 88.\n",
            "Loading random states from model/fasttext/average_embedding_mlp_200/checkpoint.randomstate and starting at epoch 88.\n",
            "Epoch:  88/300 Train steps: 313 Val steps: 35 37.67s loss: 1.137962 acc: 61.687662 fscore_macro: 0.441779 val_loss: 1.210522 val_acc: 57.297297 val_fscore_macro: 0.416362\n",
            "Epoch:  89/300 Train steps: 313 Val steps: 35 0.57s loss: 1.133290 acc: 62.067586 fscore_macro: 0.445163 val_loss: 1.203764 val_acc: 58.918919 val_fscore_macro: 0.432663\n",
            "Epoch 89: val_acc improved from 58.55856 to 58.91892, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_89.ckpt\n",
            "Epoch:  90/300 Train steps: 313 Val steps: 35 0.59s loss: 1.127097 acc: 61.827634 fscore_macro: 0.444009 val_loss: 1.202280 val_acc: 58.018018 val_fscore_macro: 0.417665\n",
            "Epoch:  91/300 Train steps: 313 Val steps: 35 0.56s loss: 1.121845 acc: 62.207558 fscore_macro: 0.447051 val_loss: 1.195830 val_acc: 58.018018 val_fscore_macro: 0.427626\n",
            "Epoch:  92/300 Train steps: 313 Val steps: 35 0.57s loss: 1.116705 acc: 62.407518 fscore_macro: 0.453476 val_loss: 1.190991 val_acc: 59.819820 val_fscore_macro: 0.441792\n",
            "Epoch 92: val_acc improved from 58.91892 to 59.81982, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_92.ckpt\n",
            "Epoch:  93/300 Train steps: 313 Val steps: 35 0.57s loss: 1.111275 acc: 62.467507 fscore_macro: 0.448873 val_loss: 1.185635 val_acc: 58.918919 val_fscore_macro: 0.433702\n",
            "Epoch:  94/300 Train steps: 313 Val steps: 35 0.55s loss: 1.106085 acc: 62.707459 fscore_macro: 0.457029 val_loss: 1.179509 val_acc: 58.378378 val_fscore_macro: 0.436889\n",
            "Epoch:  95/300 Train steps: 313 Val steps: 35 0.60s loss: 1.100809 acc: 62.787443 fscore_macro: 0.457278 val_loss: 1.176400 val_acc: 59.279279 val_fscore_macro: 0.441941\n",
            "Epoch:  96/300 Train steps: 313 Val steps: 35 0.58s loss: 1.097028 acc: 62.787443 fscore_macro: 0.458574 val_loss: 1.169621 val_acc: 59.639640 val_fscore_macro: 0.444351\n",
            "Epoch:  97/300 Train steps: 313 Val steps: 35 0.56s loss: 1.091264 acc: 63.147371 fscore_macro: 0.461691 val_loss: 1.168456 val_acc: 58.738739 val_fscore_macro: 0.438392\n",
            "Epoch:  98/300 Train steps: 313 Val steps: 35 0.57s loss: 1.086702 acc: 63.047391 fscore_macro: 0.462792 val_loss: 1.162561 val_acc: 60.000000 val_fscore_macro: 0.447608\n",
            "Epoch 98: val_acc improved from 59.81982 to 60.00000, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_98.ckpt\n",
            "Epoch:  99/300 Train steps: 313 Val steps: 35 0.55s loss: 1.081650 acc: 63.307339 fscore_macro: 0.467795 val_loss: 1.154530 val_acc: 60.720721 val_fscore_macro: 0.450689\n",
            "Epoch 99: val_acc improved from 60.00000 to 60.72072, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_99.ckpt\n",
            "Epoch: 100/300 Train steps: 313 Val steps: 35 0.59s loss: 1.077013 acc: 63.367327 fscore_macro: 0.468784 val_loss: 1.152463 val_acc: 61.081081 val_fscore_macro: 0.459068\n",
            "Epoch 100: val_acc improved from 60.72072 to 61.08108, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_100.ckpt\n",
            "Epoch: 101/300 Train steps: 313 Val steps: 35 0.59s loss: 1.071182 acc: 63.747251 fscore_macro: 0.476677 val_loss: 1.155765 val_acc: 60.180180 val_fscore_macro: 0.454129\n",
            "Epoch: 102/300 Train steps: 313 Val steps: 35 0.57s loss: 1.068025 acc: 64.087183 fscore_macro: 0.476086 val_loss: 1.145350 val_acc: 60.720721 val_fscore_macro: 0.450211\n",
            "Epoch: 103/300 Train steps: 313 Val steps: 35 0.56s loss: 1.062633 acc: 63.747251 fscore_macro: 0.474719 val_loss: 1.141711 val_acc: 60.720721 val_fscore_macro: 0.456690\n",
            "Epoch: 104/300 Train steps: 313 Val steps: 35 0.56s loss: 1.059550 acc: 64.187163 fscore_macro: 0.482548 val_loss: 1.133779 val_acc: 61.441441 val_fscore_macro: 0.461690\n",
            "Epoch 104: val_acc improved from 61.08108 to 61.44144, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_104.ckpt\n",
            "Epoch: 105/300 Train steps: 313 Val steps: 35 0.60s loss: 1.054128 acc: 64.247151 fscore_macro: 0.484543 val_loss: 1.130586 val_acc: 61.081081 val_fscore_macro: 0.458173\n",
            "Epoch: 106/300 Train steps: 313 Val steps: 35 0.59s loss: 1.050837 acc: 63.967207 fscore_macro: 0.479611 val_loss: 1.127709 val_acc: 60.720721 val_fscore_macro: 0.456801\n",
            "Epoch: 107/300 Train steps: 313 Val steps: 35 0.74s loss: 1.045902 acc: 64.987003 fscore_macro: 0.491691 val_loss: 1.124192 val_acc: 60.900901 val_fscore_macro: 0.457933\n",
            "Epoch: 108/300 Train steps: 313 Val steps: 35 0.77s loss: 1.040568 acc: 64.647071 fscore_macro: 0.487621 val_loss: 1.120343 val_acc: 62.342342 val_fscore_macro: 0.471614\n",
            "Epoch 108: val_acc improved from 61.44144 to 62.34234, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_108.ckpt\n",
            "Epoch: 109/300 Train steps: 313 Val steps: 35 0.78s loss: 1.037441 acc: 64.927015 fscore_macro: 0.491293 val_loss: 1.114599 val_acc: 61.981982 val_fscore_macro: 0.473742\n",
            "Epoch: 110/300 Train steps: 313 Val steps: 35 0.76s loss: 1.033504 acc: 65.186963 fscore_macro: 0.496748 val_loss: 1.110441 val_acc: 61.981982 val_fscore_macro: 0.468824\n",
            "Epoch: 111/300 Train steps: 313 Val steps: 35 0.80s loss: 1.029924 acc: 64.947011 fscore_macro: 0.496451 val_loss: 1.108252 val_acc: 62.162162 val_fscore_macro: 0.468917\n",
            "Epoch: 112/300 Train steps: 313 Val steps: 35 0.63s loss: 1.025767 acc: 65.486903 fscore_macro: 0.498553 val_loss: 1.104428 val_acc: 62.162162 val_fscore_macro: 0.468536\n",
            "Epoch: 113/300 Train steps: 313 Val steps: 35 0.57s loss: 1.021473 acc: 65.466907 fscore_macro: 0.499188 val_loss: 1.103070 val_acc: 62.522523 val_fscore_macro: 0.478216\n",
            "Epoch 113: val_acc improved from 62.34234 to 62.52252, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_113.ckpt\n",
            "Epoch: 114/300 Train steps: 313 Val steps: 35 0.63s loss: 1.017843 acc: 65.826835 fscore_macro: 0.502111 val_loss: 1.095923 val_acc: 62.702703 val_fscore_macro: 0.471868\n",
            "Epoch 114: val_acc improved from 62.52252 to 62.70270, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_114.ckpt\n",
            "Epoch: 115/300 Train steps: 313 Val steps: 35 0.56s loss: 1.014032 acc: 65.946811 fscore_macro: 0.503190 val_loss: 1.095903 val_acc: 62.342342 val_fscore_macro: 0.476736\n",
            "Epoch: 116/300 Train steps: 313 Val steps: 35 0.56s loss: 1.010594 acc: 66.026795 fscore_macro: 0.506138 val_loss: 1.088055 val_acc: 62.522523 val_fscore_macro: 0.471713\n",
            "Epoch: 117/300 Train steps: 313 Val steps: 35 0.58s loss: 1.006378 acc: 66.126775 fscore_macro: 0.508766 val_loss: 1.093603 val_acc: 62.702703 val_fscore_macro: 0.478373\n",
            "Epoch 117: val_acc improved from 62.70270 to 62.70270, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_117.ckpt\n",
            "Epoch: 118/300 Train steps: 313 Val steps: 35 0.57s loss: 1.003097 acc: 65.826835 fscore_macro: 0.505992 val_loss: 1.084277 val_acc: 62.882883 val_fscore_macro: 0.476322\n",
            "Epoch 118: val_acc improved from 62.70270 to 62.88288, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_118.ckpt\n",
            "Epoch: 119/300 Train steps: 313 Val steps: 35 0.58s loss: 0.999930 acc: 66.446711 fscore_macro: 0.512833 val_loss: 1.079273 val_acc: 62.522523 val_fscore_macro: 0.477030\n",
            "Epoch: 120/300 Train steps: 313 Val steps: 35 0.58s loss: 0.996117 acc: 66.446711 fscore_macro: 0.514126 val_loss: 1.079972 val_acc: 63.423423 val_fscore_macro: 0.488718\n",
            "Epoch 120: val_acc improved from 62.88288 to 63.42342, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_120.ckpt\n",
            "Epoch: 121/300 Train steps: 313 Val steps: 35 0.57s loss: 0.992952 acc: 66.566687 fscore_macro: 0.513818 val_loss: 1.075230 val_acc: 63.243243 val_fscore_macro: 0.491778\n",
            "Epoch: 122/300 Train steps: 313 Val steps: 35 0.58s loss: 0.989373 acc: 67.066587 fscore_macro: 0.519938 val_loss: 1.070089 val_acc: 62.702703 val_fscore_macro: 0.476260\n",
            "Epoch: 123/300 Train steps: 313 Val steps: 35 0.58s loss: 0.985389 acc: 67.206559 fscore_macro: 0.521690 val_loss: 1.066977 val_acc: 62.522523 val_fscore_macro: 0.475383\n",
            "Epoch: 124/300 Train steps: 313 Val steps: 35 0.65s loss: 0.981677 acc: 66.766647 fscore_macro: 0.516399 val_loss: 1.066247 val_acc: 63.423423 val_fscore_macro: 0.491806\n",
            "Epoch 124: val_acc improved from 63.42342 to 63.42342, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_124.ckpt\n",
            "Epoch: 125/300 Train steps: 313 Val steps: 35 0.62s loss: 0.979653 acc: 67.306539 fscore_macro: 0.525387 val_loss: 1.058980 val_acc: 63.603604 val_fscore_macro: 0.485126\n",
            "Epoch 125: val_acc improved from 63.42342 to 63.60360, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_125.ckpt\n",
            "Epoch: 126/300 Train steps: 313 Val steps: 35 0.60s loss: 0.976124 acc: 67.206559 fscore_macro: 0.521965 val_loss: 1.058714 val_acc: 64.864865 val_fscore_macro: 0.508605\n",
            "Epoch 126: val_acc improved from 63.60360 to 64.86486, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_126.ckpt\n",
            "Epoch: 127/300 Train steps: 313 Val steps: 35 0.58s loss: 0.972581 acc: 67.266547 fscore_macro: 0.524702 val_loss: 1.052641 val_acc: 64.684685 val_fscore_macro: 0.504742\n",
            "Epoch: 128/300 Train steps: 313 Val steps: 35 0.58s loss: 0.969532 acc: 67.366527 fscore_macro: 0.526738 val_loss: 1.053545 val_acc: 64.144144 val_fscore_macro: 0.503289\n",
            "Epoch: 129/300 Train steps: 313 Val steps: 35 0.58s loss: 0.966278 acc: 67.566487 fscore_macro: 0.528012 val_loss: 1.048057 val_acc: 63.603604 val_fscore_macro: 0.493549\n",
            "Epoch: 130/300 Train steps: 313 Val steps: 35 0.61s loss: 0.963062 acc: 67.926415 fscore_macro: 0.530757 val_loss: 1.043713 val_acc: 64.324324 val_fscore_macro: 0.507059\n",
            "Epoch: 131/300 Train steps: 313 Val steps: 35 0.59s loss: 0.960501 acc: 67.706459 fscore_macro: 0.528039 val_loss: 1.039895 val_acc: 64.324324 val_fscore_macro: 0.506203\n",
            "Epoch: 132/300 Train steps: 313 Val steps: 35 0.62s loss: 0.956184 acc: 68.046391 fscore_macro: 0.534524 val_loss: 1.039281 val_acc: 65.045045 val_fscore_macro: 0.507863\n",
            "Epoch 132: val_acc improved from 64.86486 to 65.04505, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_132.ckpt\n",
            "Epoch: 133/300 Train steps: 313 Val steps: 35 0.60s loss: 0.954481 acc: 67.886423 fscore_macro: 0.532342 val_loss: 1.035990 val_acc: 64.504505 val_fscore_macro: 0.506049\n",
            "Epoch: 134/300 Train steps: 313 Val steps: 35 0.59s loss: 0.950626 acc: 68.506299 fscore_macro: 0.537719 val_loss: 1.032513 val_acc: 64.864865 val_fscore_macro: 0.508869\n",
            "Epoch: 135/300 Train steps: 313 Val steps: 35 0.57s loss: 0.947704 acc: 68.526295 fscore_macro: 0.535655 val_loss: 1.034991 val_acc: 65.045045 val_fscore_macro: 0.513953\n",
            "Epoch 135: val_acc improved from 65.04505 to 65.04505, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_135.ckpt\n",
            "Epoch: 136/300 Train steps: 313 Val steps: 35 0.57s loss: 0.944057 acc: 68.386323 fscore_macro: 0.535562 val_loss: 1.027831 val_acc: 65.225225 val_fscore_macro: 0.513922\n",
            "Epoch 136: val_acc improved from 65.04505 to 65.22523, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_136.ckpt\n",
            "Epoch: 137/300 Train steps: 313 Val steps: 35 0.58s loss: 0.941886 acc: 68.446311 fscore_macro: 0.537551 val_loss: 1.025494 val_acc: 65.405405 val_fscore_macro: 0.512820\n",
            "Epoch 137: val_acc improved from 65.22523 to 65.40541, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_137.ckpt\n",
            "Epoch: 138/300 Train steps: 313 Val steps: 35 0.57s loss: 0.939359 acc: 68.726255 fscore_macro: 0.541432 val_loss: 1.024304 val_acc: 63.963964 val_fscore_macro: 0.501347\n",
            "Epoch: 139/300 Train steps: 313 Val steps: 35 0.57s loss: 0.936387 acc: 68.646271 fscore_macro: 0.539474 val_loss: 1.024045 val_acc: 65.225225 val_fscore_macro: 0.512055\n",
            "Epoch: 140/300 Train steps: 313 Val steps: 35 0.57s loss: 0.934216 acc: 68.526295 fscore_macro: 0.539335 val_loss: 1.022597 val_acc: 65.945946 val_fscore_macro: 0.519344\n",
            "Epoch 140: val_acc improved from 65.40541 to 65.94595, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_140.ckpt\n",
            "Epoch: 141/300 Train steps: 313 Val steps: 35 0.59s loss: 0.931237 acc: 69.046191 fscore_macro: 0.543821 val_loss: 1.012783 val_acc: 66.666667 val_fscore_macro: 0.523357\n",
            "Epoch 141: val_acc improved from 65.94595 to 66.66667, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_141.ckpt\n",
            "Epoch: 142/300 Train steps: 313 Val steps: 35 0.59s loss: 0.928817 acc: 69.006199 fscore_macro: 0.542974 val_loss: 1.011551 val_acc: 64.504505 val_fscore_macro: 0.508309\n",
            "Epoch: 143/300 Train steps: 313 Val steps: 35 0.58s loss: 0.924944 acc: 69.006199 fscore_macro: 0.544269 val_loss: 1.015172 val_acc: 66.126126 val_fscore_macro: 0.521530\n",
            "Epoch: 144/300 Train steps: 313 Val steps: 35 0.62s loss: 0.922579 acc: 69.146171 fscore_macro: 0.544993 val_loss: 1.009072 val_acc: 65.945946 val_fscore_macro: 0.515279\n",
            "Epoch: 145/300 Train steps: 313 Val steps: 35 0.58s loss: 0.920201 acc: 69.286143 fscore_macro: 0.548475 val_loss: 1.006289 val_acc: 64.684685 val_fscore_macro: 0.507160\n",
            "Epoch: 146/300 Train steps: 313 Val steps: 35 0.57s loss: 0.917869 acc: 69.306139 fscore_macro: 0.547220 val_loss: 1.005680 val_acc: 64.504505 val_fscore_macro: 0.507471\n",
            "Epoch: 147/300 Train steps: 313 Val steps: 35 0.60s loss: 0.914877 acc: 69.366127 fscore_macro: 0.548316 val_loss: 1.005110 val_acc: 65.765766 val_fscore_macro: 0.515425\n",
            "Epoch: 148/300 Train steps: 313 Val steps: 35 0.60s loss: 0.912199 acc: 69.606079 fscore_macro: 0.550913 val_loss: 0.998840 val_acc: 65.585586 val_fscore_macro: 0.516377\n",
            "Epoch: 149/300 Train steps: 313 Val steps: 35 0.58s loss: 0.909626 acc: 69.506099 fscore_macro: 0.549947 val_loss: 0.997654 val_acc: 64.864865 val_fscore_macro: 0.511031\n",
            "Epoch: 150/300 Train steps: 313 Val steps: 35 0.59s loss: 0.906764 acc: 69.526095 fscore_macro: 0.548665 val_loss: 0.996354 val_acc: 64.864865 val_fscore_macro: 0.511887\n",
            "Epoch: 151/300 Train steps: 313 Val steps: 35 0.56s loss: 0.905448 acc: 69.866027 fscore_macro: 0.552882 val_loss: 0.992171 val_acc: 66.666667 val_fscore_macro: 0.522871\n",
            "Epoch: 152/300 Train steps: 313 Val steps: 35 0.58s loss: 0.902648 acc: 69.486103 fscore_macro: 0.550016 val_loss: 0.991739 val_acc: 66.306306 val_fscore_macro: 0.520623\n",
            "Epoch: 153/300 Train steps: 313 Val steps: 35 0.59s loss: 0.899628 acc: 69.846031 fscore_macro: 0.552722 val_loss: 0.990554 val_acc: 66.306306 val_fscore_macro: 0.520716\n",
            "Epoch: 154/300 Train steps: 313 Val steps: 35 0.58s loss: 0.898106 acc: 69.866027 fscore_macro: 0.552269 val_loss: 0.985804 val_acc: 67.027027 val_fscore_macro: 0.525027\n",
            "Epoch 154: val_acc improved from 66.66667 to 67.02703, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_154.ckpt\n",
            "Epoch: 155/300 Train steps: 313 Val steps: 35 0.58s loss: 0.894383 acc: 70.325935 fscore_macro: 0.556600 val_loss: 0.984836 val_acc: 67.027027 val_fscore_macro: 0.527222\n",
            "Epoch 155: val_acc improved from 67.02703 to 67.02703, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_155.ckpt\n",
            "Epoch: 156/300 Train steps: 313 Val steps: 35 0.58s loss: 0.893352 acc: 70.045991 fscore_macro: 0.554873 val_loss: 0.981865 val_acc: 64.684685 val_fscore_macro: 0.508911\n",
            "Epoch: 157/300 Train steps: 313 Val steps: 35 0.57s loss: 0.891137 acc: 70.205959 fscore_macro: 0.556830 val_loss: 0.980775 val_acc: 67.747748 val_fscore_macro: 0.530828\n",
            "Epoch 157: val_acc improved from 67.02703 to 67.74775, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_157.ckpt\n",
            "Epoch: 158/300 Train steps: 313 Val steps: 35 0.57s loss: 0.887655 acc: 70.305939 fscore_macro: 0.555965 val_loss: 0.975208 val_acc: 67.027027 val_fscore_macro: 0.526628\n",
            "Epoch: 159/300 Train steps: 313 Val steps: 35 0.58s loss: 0.885873 acc: 70.225955 fscore_macro: 0.555723 val_loss: 0.976290 val_acc: 66.306306 val_fscore_macro: 0.522809\n",
            "Epoch: 160/300 Train steps: 313 Val steps: 35 0.57s loss: 0.883827 acc: 70.445911 fscore_macro: 0.557364 val_loss: 0.976798 val_acc: 67.027027 val_fscore_macro: 0.527049\n",
            "Epoch: 161/300 Train steps: 313 Val steps: 35 0.57s loss: 0.881123 acc: 70.645871 fscore_macro: 0.558878 val_loss: 0.970440 val_acc: 66.846847 val_fscore_macro: 0.522735\n",
            "Epoch: 162/300 Train steps: 313 Val steps: 35 0.57s loss: 0.878259 acc: 70.545891 fscore_macro: 0.558008 val_loss: 0.967807 val_acc: 67.567568 val_fscore_macro: 0.529239\n",
            "Epoch: 163/300 Train steps: 313 Val steps: 35 0.56s loss: 0.876710 acc: 70.665867 fscore_macro: 0.562467 val_loss: 0.965102 val_acc: 66.666667 val_fscore_macro: 0.521386\n",
            "Epoch: 164/300 Train steps: 313 Val steps: 35 0.56s loss: 0.875298 acc: 70.345931 fscore_macro: 0.561346 val_loss: 0.964496 val_acc: 67.567568 val_fscore_macro: 0.526038\n",
            "Epoch: 165/300 Train steps: 313 Val steps: 35 0.57s loss: 0.872611 acc: 70.845831 fscore_macro: 0.561907 val_loss: 0.959593 val_acc: 67.387387 val_fscore_macro: 0.524394\n",
            "Epoch: 166/300 Train steps: 313 Val steps: 35 0.58s loss: 0.870395 acc: 70.785843 fscore_macro: 0.560740 val_loss: 0.959196 val_acc: 66.846847 val_fscore_macro: 0.523931\n",
            "Epoch: 167/300 Train steps: 313 Val steps: 35 0.56s loss: 0.867373 acc: 71.225755 fscore_macro: 0.567647 val_loss: 0.957737 val_acc: 66.126126 val_fscore_macro: 0.520019\n",
            "Epoch: 168/300 Train steps: 313 Val steps: 35 0.59s loss: 0.865820 acc: 71.025795 fscore_macro: 0.565895 val_loss: 0.955242 val_acc: 67.027027 val_fscore_macro: 0.527565\n",
            "Epoch: 169/300 Train steps: 313 Val steps: 35 0.59s loss: 0.863834 acc: 70.965807 fscore_macro: 0.562219 val_loss: 0.955908 val_acc: 67.567568 val_fscore_macro: 0.529795\n",
            "Epoch: 170/300 Train steps: 313 Val steps: 35 0.62s loss: 0.862140 acc: 71.105779 fscore_macro: 0.567248 val_loss: 0.954284 val_acc: 67.747748 val_fscore_macro: 0.530563\n",
            "Epoch 170: val_acc improved from 67.74775 to 67.74775, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_170.ckpt\n",
            "Epoch: 171/300 Train steps: 313 Val steps: 35 0.58s loss: 0.859467 acc: 71.305739 fscore_macro: 0.567731 val_loss: 0.958364 val_acc: 66.846847 val_fscore_macro: 0.528696\n",
            "Epoch: 172/300 Train steps: 313 Val steps: 35 0.60s loss: 0.857342 acc: 71.245751 fscore_macro: 0.568143 val_loss: 0.946621 val_acc: 68.288288 val_fscore_macro: 0.539249\n",
            "Epoch 172: val_acc improved from 67.74775 to 68.28829, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_172.ckpt\n",
            "Epoch: 173/300 Train steps: 313 Val steps: 35 0.57s loss: 0.855871 acc: 71.205759 fscore_macro: 0.570866 val_loss: 0.946080 val_acc: 68.108108 val_fscore_macro: 0.533715\n",
            "Epoch: 174/300 Train steps: 313 Val steps: 35 0.58s loss: 0.852994 acc: 71.005799 fscore_macro: 0.569435 val_loss: 0.946886 val_acc: 68.648649 val_fscore_macro: 0.537377\n",
            "Epoch 174: val_acc improved from 68.28829 to 68.64865, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_174.ckpt\n",
            "Epoch: 175/300 Train steps: 313 Val steps: 35 0.56s loss: 0.850961 acc: 71.405719 fscore_macro: 0.572424 val_loss: 0.940188 val_acc: 67.927928 val_fscore_macro: 0.535017\n",
            "Epoch: 176/300 Train steps: 313 Val steps: 35 0.57s loss: 0.850308 acc: 71.385723 fscore_macro: 0.572286 val_loss: 0.937498 val_acc: 67.927928 val_fscore_macro: 0.534709\n",
            "Epoch: 177/300 Train steps: 313 Val steps: 35 0.58s loss: 0.848402 acc: 71.385723 fscore_macro: 0.573034 val_loss: 0.942835 val_acc: 67.207207 val_fscore_macro: 0.527826\n",
            "Epoch: 178/300 Train steps: 313 Val steps: 35 0.57s loss: 0.845391 acc: 71.725655 fscore_macro: 0.575702 val_loss: 0.935885 val_acc: 68.468468 val_fscore_macro: 0.537079\n",
            "Epoch: 179/300 Train steps: 313 Val steps: 35 0.58s loss: 0.844153 acc: 71.725655 fscore_macro: 0.577129 val_loss: 0.935661 val_acc: 68.828829 val_fscore_macro: 0.542534\n",
            "Epoch 179: val_acc improved from 68.64865 to 68.82883, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_179.ckpt\n",
            "Epoch: 180/300 Train steps: 313 Val steps: 35 0.59s loss: 0.842002 acc: 71.785643 fscore_macro: 0.575829 val_loss: 0.933493 val_acc: 68.108108 val_fscore_macro: 0.533781\n",
            "Epoch: 181/300 Train steps: 313 Val steps: 35 0.57s loss: 0.840924 acc: 71.805639 fscore_macro: 0.576295 val_loss: 0.936641 val_acc: 67.927928 val_fscore_macro: 0.534834\n",
            "Epoch: 182/300 Train steps: 313 Val steps: 35 0.58s loss: 0.837779 acc: 72.205559 fscore_macro: 0.578646 val_loss: 0.928901 val_acc: 69.189189 val_fscore_macro: 0.544868\n",
            "Epoch 182: val_acc improved from 68.82883 to 69.18919, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_182.ckpt\n",
            "Epoch: 183/300 Train steps: 313 Val steps: 35 0.58s loss: 0.835784 acc: 71.965607 fscore_macro: 0.576263 val_loss: 0.933283 val_acc: 67.927928 val_fscore_macro: 0.537994\n",
            "Epoch: 184/300 Train steps: 313 Val steps: 35 0.59s loss: 0.834847 acc: 72.045591 fscore_macro: 0.578222 val_loss: 0.926757 val_acc: 69.009009 val_fscore_macro: 0.542703\n",
            "Epoch: 185/300 Train steps: 313 Val steps: 35 0.56s loss: 0.832198 acc: 72.065587 fscore_macro: 0.580732 val_loss: 0.921042 val_acc: 68.468468 val_fscore_macro: 0.536702\n",
            "Epoch: 186/300 Train steps: 313 Val steps: 35 0.57s loss: 0.830134 acc: 71.845631 fscore_macro: 0.578776 val_loss: 0.926407 val_acc: 69.009009 val_fscore_macro: 0.543656\n",
            "Epoch: 187/300 Train steps: 313 Val steps: 35 0.57s loss: 0.829138 acc: 72.425515 fscore_macro: 0.586387 val_loss: 0.921752 val_acc: 69.009009 val_fscore_macro: 0.544950\n",
            "Epoch: 188/300 Train steps: 313 Val steps: 35 0.57s loss: 0.826732 acc: 71.965607 fscore_macro: 0.580894 val_loss: 0.923112 val_acc: 69.369369 val_fscore_macro: 0.549062\n",
            "Epoch 188: val_acc improved from 69.18919 to 69.36937, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_188.ckpt\n",
            "Epoch: 189/300 Train steps: 313 Val steps: 35 0.57s loss: 0.825168 acc: 72.365527 fscore_macro: 0.585921 val_loss: 0.919267 val_acc: 69.009009 val_fscore_macro: 0.544357\n",
            "Epoch: 190/300 Train steps: 313 Val steps: 35 0.58s loss: 0.821651 acc: 72.485503 fscore_macro: 0.589203 val_loss: 0.924782 val_acc: 69.549550 val_fscore_macro: 0.568833\n",
            "Epoch 190: val_acc improved from 69.36937 to 69.54955, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_190.ckpt\n",
            "Epoch: 191/300 Train steps: 313 Val steps: 35 0.56s loss: 0.820917 acc: 72.565487 fscore_macro: 0.587150 val_loss: 0.914388 val_acc: 68.828829 val_fscore_macro: 0.539535\n",
            "Epoch: 192/300 Train steps: 313 Val steps: 35 0.57s loss: 0.820530 acc: 72.085583 fscore_macro: 0.586180 val_loss: 0.910919 val_acc: 69.549550 val_fscore_macro: 0.548778\n",
            "Epoch: 193/300 Train steps: 313 Val steps: 35 0.57s loss: 0.818027 acc: 72.125575 fscore_macro: 0.587400 val_loss: 0.911285 val_acc: 69.909910 val_fscore_macro: 0.584337\n",
            "Epoch 193: val_acc improved from 69.54955 to 69.90991, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_193.ckpt\n",
            "Epoch: 194/300 Train steps: 313 Val steps: 35 0.61s loss: 0.815906 acc: 72.485503 fscore_macro: 0.593719 val_loss: 0.912371 val_acc: 68.828829 val_fscore_macro: 0.545245\n",
            "Epoch: 195/300 Train steps: 313 Val steps: 35 0.57s loss: 0.813605 acc: 72.605479 fscore_macro: 0.588180 val_loss: 0.908732 val_acc: 70.090090 val_fscore_macro: 0.550526\n",
            "Epoch 195: val_acc improved from 69.90991 to 70.09009, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_195.ckpt\n",
            "Epoch: 196/300 Train steps: 313 Val steps: 35 0.63s loss: 0.812755 acc: 72.725455 fscore_macro: 0.593527 val_loss: 0.908027 val_acc: 69.549550 val_fscore_macro: 0.569556\n",
            "Epoch: 197/300 Train steps: 313 Val steps: 35 0.58s loss: 0.810736 acc: 72.945411 fscore_macro: 0.600403 val_loss: 0.907409 val_acc: 69.729730 val_fscore_macro: 0.585984\n",
            "Epoch: 198/300 Train steps: 313 Val steps: 35 0.60s loss: 0.809244 acc: 72.905419 fscore_macro: 0.595554 val_loss: 0.904341 val_acc: 69.189189 val_fscore_macro: 0.564855\n",
            "Epoch: 199/300 Train steps: 313 Val steps: 35 0.58s loss: 0.808074 acc: 73.025395 fscore_macro: 0.599338 val_loss: 0.902624 val_acc: 69.189189 val_fscore_macro: 0.546603\n",
            "Epoch: 200/300 Train steps: 313 Val steps: 35 0.60s loss: 0.806176 acc: 73.085383 fscore_macro: 0.598862 val_loss: 0.907831 val_acc: 69.909910 val_fscore_macro: 0.584459\n",
            "Epoch: 201/300 Train steps: 313 Val steps: 35 0.57s loss: 0.804372 acc: 73.005399 fscore_macro: 0.597218 val_loss: 0.899640 val_acc: 70.090090 val_fscore_macro: 0.586917\n",
            "Epoch: 202/300 Train steps: 313 Val steps: 35 0.58s loss: 0.801449 acc: 73.345331 fscore_macro: 0.607674 val_loss: 0.903750 val_acc: 69.729730 val_fscore_macro: 0.591512\n",
            "Epoch: 203/300 Train steps: 313 Val steps: 35 0.62s loss: 0.800525 acc: 73.325335 fscore_macro: 0.605142 val_loss: 0.902633 val_acc: 69.549550 val_fscore_macro: 0.582730\n",
            "Epoch: 204/300 Train steps: 313 Val steps: 35 0.68s loss: 0.799148 acc: 73.185363 fscore_macro: 0.605316 val_loss: 0.895041 val_acc: 70.810811 val_fscore_macro: 0.592003\n",
            "Epoch 204: val_acc improved from 70.09009 to 70.81081, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_204.ckpt\n",
            "Epoch: 205/300 Train steps: 313 Val steps: 35 0.78s loss: 0.797726 acc: 73.245351 fscore_macro: 0.604560 val_loss: 0.896121 val_acc: 69.549550 val_fscore_macro: 0.584894\n",
            "Epoch: 206/300 Train steps: 313 Val steps: 35 0.80s loss: 0.795584 acc: 73.525295 fscore_macro: 0.614521 val_loss: 0.899815 val_acc: 68.648649 val_fscore_macro: 0.563122\n",
            "Epoch: 207/300 Train steps: 313 Val steps: 35 0.78s loss: 0.794324 acc: 73.185363 fscore_macro: 0.610551 val_loss: 0.889882 val_acc: 70.270270 val_fscore_macro: 0.588925\n",
            "Epoch: 208/300 Train steps: 313 Val steps: 35 0.79s loss: 0.792596 acc: 73.565287 fscore_macro: 0.616807 val_loss: 0.887626 val_acc: 71.171171 val_fscore_macro: 0.602110\n",
            "Epoch 208: val_acc improved from 70.81081 to 71.17117, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_208.ckpt\n",
            "Epoch: 209/300 Train steps: 313 Val steps: 35 0.69s loss: 0.791267 acc: 73.505299 fscore_macro: 0.617711 val_loss: 0.889180 val_acc: 70.450450 val_fscore_macro: 0.600728\n",
            "Epoch: 210/300 Train steps: 313 Val steps: 35 0.58s loss: 0.789633 acc: 73.885223 fscore_macro: 0.621004 val_loss: 0.886118 val_acc: 70.450450 val_fscore_macro: 0.597266\n",
            "Epoch: 211/300 Train steps: 313 Val steps: 35 0.58s loss: 0.787226 acc: 73.705259 fscore_macro: 0.616621 val_loss: 0.886017 val_acc: 70.810811 val_fscore_macro: 0.601839\n",
            "Epoch: 212/300 Train steps: 313 Val steps: 35 0.57s loss: 0.787182 acc: 73.645271 fscore_macro: 0.618332 val_loss: 0.887318 val_acc: 70.630631 val_fscore_macro: 0.597639\n",
            "Epoch: 213/300 Train steps: 313 Val steps: 35 0.58s loss: 0.784409 acc: 73.565287 fscore_macro: 0.615006 val_loss: 0.883733 val_acc: 70.450450 val_fscore_macro: 0.591757\n",
            "Epoch: 214/300 Train steps: 313 Val steps: 35 0.58s loss: 0.782738 acc: 73.665267 fscore_macro: 0.616425 val_loss: 0.883361 val_acc: 70.990991 val_fscore_macro: 0.615786\n",
            "Epoch: 215/300 Train steps: 313 Val steps: 35 0.59s loss: 0.782217 acc: 73.585283 fscore_macro: 0.622215 val_loss: 0.880325 val_acc: 70.450450 val_fscore_macro: 0.592979\n",
            "Epoch: 216/300 Train steps: 313 Val steps: 35 0.62s loss: 0.779901 acc: 73.985203 fscore_macro: 0.624405 val_loss: 0.888677 val_acc: 69.549550 val_fscore_macro: 0.579301\n",
            "Epoch: 217/300 Train steps: 313 Val steps: 35 0.59s loss: 0.778613 acc: 74.125175 fscore_macro: 0.629019 val_loss: 0.877494 val_acc: 70.990991 val_fscore_macro: 0.614500\n",
            "Epoch: 218/300 Train steps: 313 Val steps: 35 0.57s loss: 0.776023 acc: 74.005199 fscore_macro: 0.627605 val_loss: 0.875943 val_acc: 71.351351 val_fscore_macro: 0.605297\n",
            "Epoch 218: val_acc improved from 71.17117 to 71.35135, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_218.ckpt\n",
            "Epoch: 219/300 Train steps: 313 Val steps: 35 0.57s loss: 0.774307 acc: 74.025195 fscore_macro: 0.627146 val_loss: 0.883210 val_acc: 71.171171 val_fscore_macro: 0.621839\n",
            "Epoch: 220/300 Train steps: 313 Val steps: 35 0.60s loss: 0.774842 acc: 74.265147 fscore_macro: 0.630573 val_loss: 0.878053 val_acc: 71.891892 val_fscore_macro: 0.624686\n",
            "Epoch 220: val_acc improved from 71.35135 to 71.89189, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_220.ckpt\n",
            "Epoch: 221/300 Train steps: 313 Val steps: 35 0.60s loss: 0.773168 acc: 74.145171 fscore_macro: 0.630141 val_loss: 0.878353 val_acc: 71.171171 val_fscore_macro: 0.618065\n",
            "Epoch: 222/300 Train steps: 313 Val steps: 35 0.57s loss: 0.769388 acc: 74.265147 fscore_macro: 0.631375 val_loss: 0.874541 val_acc: 70.090090 val_fscore_macro: 0.596385\n",
            "Epoch: 223/300 Train steps: 313 Val steps: 35 0.60s loss: 0.769909 acc: 74.265147 fscore_macro: 0.635234 val_loss: 0.870971 val_acc: 71.171171 val_fscore_macro: 0.615424\n",
            "Epoch: 224/300 Train steps: 313 Val steps: 35 0.58s loss: 0.768479 acc: 74.485103 fscore_macro: 0.634294 val_loss: 0.871504 val_acc: 70.810811 val_fscore_macro: 0.617928\n",
            "Epoch: 225/300 Train steps: 313 Val steps: 35 0.59s loss: 0.765633 acc: 74.545091 fscore_macro: 0.636138 val_loss: 0.866716 val_acc: 72.072072 val_fscore_macro: 0.623516\n",
            "Epoch 225: val_acc improved from 71.89189 to 72.07207, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_225.ckpt\n",
            "Epoch: 226/300 Train steps: 313 Val steps: 35 0.58s loss: 0.765162 acc: 74.305139 fscore_macro: 0.633054 val_loss: 0.868853 val_acc: 70.990991 val_fscore_macro: 0.616899\n",
            "Epoch: 227/300 Train steps: 313 Val steps: 35 0.57s loss: 0.764030 acc: 74.545091 fscore_macro: 0.633373 val_loss: 0.866636 val_acc: 70.630631 val_fscore_macro: 0.613415\n",
            "Epoch: 228/300 Train steps: 313 Val steps: 35 0.59s loss: 0.761780 acc: 74.685063 fscore_macro: 0.638793 val_loss: 0.869851 val_acc: 70.630631 val_fscore_macro: 0.611534\n",
            "Epoch: 229/300 Train steps: 313 Val steps: 35 0.58s loss: 0.760865 acc: 74.305139 fscore_macro: 0.634573 val_loss: 0.872938 val_acc: 70.450450 val_fscore_macro: 0.612586\n",
            "Epoch: 230/300 Train steps: 313 Val steps: 35 0.59s loss: 0.759530 acc: 74.865027 fscore_macro: 0.638240 val_loss: 0.863677 val_acc: 71.711712 val_fscore_macro: 0.623727\n",
            "Epoch: 231/300 Train steps: 313 Val steps: 35 0.58s loss: 0.757677 acc: 74.705059 fscore_macro: 0.643844 val_loss: 0.866455 val_acc: 71.171171 val_fscore_macro: 0.618849\n",
            "Epoch: 232/300 Train steps: 313 Val steps: 35 0.58s loss: 0.756925 acc: 74.945011 fscore_macro: 0.642732 val_loss: 0.857689 val_acc: 71.531532 val_fscore_macro: 0.624831\n",
            "Epoch: 233/300 Train steps: 313 Val steps: 35 0.57s loss: 0.755707 acc: 74.525095 fscore_macro: 0.637724 val_loss: 0.858911 val_acc: 71.891892 val_fscore_macro: 0.622008\n",
            "Epoch: 234/300 Train steps: 313 Val steps: 35 0.58s loss: 0.753249 acc: 74.605079 fscore_macro: 0.640955 val_loss: 0.858725 val_acc: 71.351351 val_fscore_macro: 0.621056\n",
            "Epoch: 235/300 Train steps: 313 Val steps: 35 0.59s loss: 0.752945 acc: 75.284943 fscore_macro: 0.643026 val_loss: 0.856630 val_acc: 71.531532 val_fscore_macro: 0.621407\n",
            "Epoch: 236/300 Train steps: 313 Val steps: 35 0.57s loss: 0.751237 acc: 74.865027 fscore_macro: 0.642358 val_loss: 0.853144 val_acc: 71.711712 val_fscore_macro: 0.624235\n",
            "Epoch: 237/300 Train steps: 313 Val steps: 35 0.57s loss: 0.749728 acc: 75.064987 fscore_macro: 0.643037 val_loss: 0.850406 val_acc: 72.072072 val_fscore_macro: 0.626114\n",
            "Epoch: 238/300 Train steps: 313 Val steps: 35 0.58s loss: 0.747447 acc: 74.865027 fscore_macro: 0.643546 val_loss: 0.857414 val_acc: 71.891892 val_fscore_macro: 0.629465\n",
            "Epoch: 239/300 Train steps: 313 Val steps: 35 0.60s loss: 0.747111 acc: 75.004999 fscore_macro: 0.646852 val_loss: 0.851101 val_acc: 71.531532 val_fscore_macro: 0.622744\n",
            "Epoch: 240/300 Train steps: 313 Val steps: 35 0.58s loss: 0.744828 acc: 75.164967 fscore_macro: 0.650352 val_loss: 0.858833 val_acc: 70.810811 val_fscore_macro: 0.613672\n",
            "Epoch: 241/300 Train steps: 313 Val steps: 35 0.62s loss: 0.745319 acc: 75.224955 fscore_macro: 0.649994 val_loss: 0.854452 val_acc: 70.990991 val_fscore_macro: 0.618460\n",
            "Epoch: 242/300 Train steps: 313 Val steps: 35 0.63s loss: 0.743354 acc: 75.104979 fscore_macro: 0.649972 val_loss: 0.848425 val_acc: 71.351351 val_fscore_macro: 0.623395\n",
            "Epoch: 243/300 Train steps: 313 Val steps: 35 0.58s loss: 0.742376 acc: 75.064987 fscore_macro: 0.652431 val_loss: 0.853300 val_acc: 70.810811 val_fscore_macro: 0.624525\n",
            "Epoch: 244/300 Train steps: 313 Val steps: 35 0.57s loss: 0.739101 acc: 75.524895 fscore_macro: 0.652199 val_loss: 0.851295 val_acc: 70.990991 val_fscore_macro: 0.623169\n",
            "Epoch: 245/300 Train steps: 313 Val steps: 35 0.60s loss: 0.739641 acc: 75.164967 fscore_macro: 0.650860 val_loss: 0.843994 val_acc: 72.252252 val_fscore_macro: 0.632750\n",
            "Epoch 245: val_acc improved from 72.07207 to 72.25225, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_245.ckpt\n",
            "Epoch: 246/300 Train steps: 313 Val steps: 35 0.57s loss: 0.737914 acc: 75.804839 fscore_macro: 0.659942 val_loss: 0.846555 val_acc: 72.432432 val_fscore_macro: 0.631102\n",
            "Epoch 246: val_acc improved from 72.25225 to 72.43243, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_246.ckpt\n",
            "Epoch: 247/300 Train steps: 313 Val steps: 35 0.57s loss: 0.737491 acc: 75.384923 fscore_macro: 0.655125 val_loss: 0.841085 val_acc: 72.072072 val_fscore_macro: 0.626060\n",
            "Epoch: 248/300 Train steps: 313 Val steps: 35 0.57s loss: 0.735630 acc: 75.724855 fscore_macro: 0.663804 val_loss: 0.845321 val_acc: 71.891892 val_fscore_macro: 0.630992\n",
            "Epoch: 249/300 Train steps: 313 Val steps: 35 0.59s loss: 0.734260 acc: 75.684863 fscore_macro: 0.662735 val_loss: 0.846985 val_acc: 71.891892 val_fscore_macro: 0.629384\n",
            "Epoch: 250/300 Train steps: 313 Val steps: 35 0.59s loss: 0.731220 acc: 75.684863 fscore_macro: 0.662261 val_loss: 0.841471 val_acc: 71.531532 val_fscore_macro: 0.617386\n",
            "Epoch: 251/300 Train steps: 313 Val steps: 35 0.58s loss: 0.731146 acc: 75.324935 fscore_macro: 0.656820 val_loss: 0.835530 val_acc: 72.252252 val_fscore_macro: 0.629570\n",
            "Epoch: 252/300 Train steps: 313 Val steps: 35 0.57s loss: 0.729998 acc: 75.844831 fscore_macro: 0.667113 val_loss: 0.839005 val_acc: 71.891892 val_fscore_macro: 0.631211\n",
            "Epoch: 253/300 Train steps: 313 Val steps: 35 0.58s loss: 0.728215 acc: 75.624875 fscore_macro: 0.665989 val_loss: 0.845140 val_acc: 71.351351 val_fscore_macro: 0.627566\n",
            "Epoch: 254/300 Train steps: 313 Val steps: 35 0.58s loss: 0.726522 acc: 76.044791 fscore_macro: 0.667646 val_loss: 0.833727 val_acc: 72.612613 val_fscore_macro: 0.634869\n",
            "Epoch 254: val_acc improved from 72.43243 to 72.61261, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_254.ckpt\n",
            "Epoch: 255/300 Train steps: 313 Val steps: 35 0.59s loss: 0.726380 acc: 76.104779 fscore_macro: 0.669428 val_loss: 0.835124 val_acc: 71.531532 val_fscore_macro: 0.624368\n",
            "Epoch: 256/300 Train steps: 313 Val steps: 35 0.57s loss: 0.724293 acc: 75.944811 fscore_macro: 0.665691 val_loss: 0.830285 val_acc: 72.432433 val_fscore_macro: 0.628673\n",
            "Epoch: 257/300 Train steps: 313 Val steps: 35 0.58s loss: 0.723907 acc: 75.944811 fscore_macro: 0.664939 val_loss: 0.835375 val_acc: 72.432432 val_fscore_macro: 0.631313\n",
            "Epoch: 258/300 Train steps: 313 Val steps: 35 0.60s loss: 0.722010 acc: 76.224755 fscore_macro: 0.673461 val_loss: 0.841675 val_acc: 71.531532 val_fscore_macro: 0.617411\n",
            "Epoch: 259/300 Train steps: 313 Val steps: 35 0.58s loss: 0.722065 acc: 76.024795 fscore_macro: 0.669912 val_loss: 0.833684 val_acc: 72.792793 val_fscore_macro: 0.636638\n",
            "Epoch 259: val_acc improved from 72.61261 to 72.79279, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_259.ckpt\n",
            "Epoch: 260/300 Train steps: 313 Val steps: 35 0.60s loss: 0.719996 acc: 76.024795 fscore_macro: 0.670226 val_loss: 0.846749 val_acc: 69.729730 val_fscore_macro: 0.612112\n",
            "Epoch: 261/300 Train steps: 313 Val steps: 35 0.60s loss: 0.718990 acc: 75.864827 fscore_macro: 0.669892 val_loss: 0.833931 val_acc: 72.432432 val_fscore_macro: 0.635861\n",
            "Epoch: 262/300 Train steps: 313 Val steps: 35 0.61s loss: 0.718225 acc: 76.264747 fscore_macro: 0.678431 val_loss: 0.826214 val_acc: 72.792793 val_fscore_macro: 0.628006\n",
            "Epoch 262: val_acc improved from 72.79279 to 72.79279, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_262.ckpt\n",
            "Epoch: 263/300 Train steps: 313 Val steps: 35 0.59s loss: 0.715434 acc: 76.284743 fscore_macro: 0.677070 val_loss: 0.829506 val_acc: 72.612613 val_fscore_macro: 0.637944\n",
            "Epoch: 264/300 Train steps: 313 Val steps: 35 0.58s loss: 0.715180 acc: 76.204759 fscore_macro: 0.672776 val_loss: 0.833041 val_acc: 71.351351 val_fscore_macro: 0.625766\n",
            "Epoch: 265/300 Train steps: 313 Val steps: 35 0.58s loss: 0.713180 acc: 76.264747 fscore_macro: 0.679636 val_loss: 0.828280 val_acc: 72.432432 val_fscore_macro: 0.630834\n",
            "Epoch: 266/300 Train steps: 313 Val steps: 35 0.60s loss: 0.714103 acc: 76.464707 fscore_macro: 0.677917 val_loss: 0.825184 val_acc: 72.072072 val_fscore_macro: 0.628830\n",
            "Epoch: 267/300 Train steps: 313 Val steps: 35 0.59s loss: 0.711611 acc: 76.444711 fscore_macro: 0.679585 val_loss: 0.820862 val_acc: 72.792793 val_fscore_macro: 0.627641\n",
            "Epoch: 268/300 Train steps: 313 Val steps: 35 0.57s loss: 0.710067 acc: 76.424715 fscore_macro: 0.673809 val_loss: 0.821341 val_acc: 72.972973 val_fscore_macro: 0.633225\n",
            "Epoch 268: val_acc improved from 72.79279 to 72.97297, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_268.ckpt\n",
            "Epoch: 269/300 Train steps: 313 Val steps: 35 0.59s loss: 0.709568 acc: 76.484703 fscore_macro: 0.678395 val_loss: 0.832008 val_acc: 71.351351 val_fscore_macro: 0.623148\n",
            "Epoch: 270/300 Train steps: 313 Val steps: 35 0.59s loss: 0.709413 acc: 76.524695 fscore_macro: 0.679156 val_loss: 0.819893 val_acc: 72.972973 val_fscore_macro: 0.629956\n",
            "Epoch 270: val_acc improved from 72.97297 to 72.97297, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_270.ckpt\n",
            "Epoch: 271/300 Train steps: 313 Val steps: 35 0.59s loss: 0.706776 acc: 76.364727 fscore_macro: 0.678277 val_loss: 0.826561 val_acc: 72.072072 val_fscore_macro: 0.626833\n",
            "Epoch: 272/300 Train steps: 313 Val steps: 35 0.61s loss: 0.706326 acc: 76.584683 fscore_macro: 0.681010 val_loss: 0.819950 val_acc: 72.792793 val_fscore_macro: 0.630636\n",
            "Epoch: 273/300 Train steps: 313 Val steps: 35 0.59s loss: 0.705623 acc: 76.964607 fscore_macro: 0.685337 val_loss: 0.824911 val_acc: 72.072072 val_fscore_macro: 0.629345\n",
            "Epoch: 274/300 Train steps: 313 Val steps: 35 0.59s loss: 0.703400 acc: 76.784643 fscore_macro: 0.680389 val_loss: 0.815565 val_acc: 72.612613 val_fscore_macro: 0.628331\n",
            "Epoch: 275/300 Train steps: 313 Val steps: 35 0.59s loss: 0.702124 acc: 76.784643 fscore_macro: 0.688849 val_loss: 0.810499 val_acc: 73.513514 val_fscore_macro: 0.638589\n",
            "Epoch 275: val_acc improved from 72.97297 to 73.51351, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_275.ckpt\n",
            "Epoch: 276/300 Train steps: 313 Val steps: 35 0.59s loss: 0.701020 acc: 76.644671 fscore_macro: 0.684558 val_loss: 0.812017 val_acc: 72.612613 val_fscore_macro: 0.634383\n",
            "Epoch: 277/300 Train steps: 313 Val steps: 35 0.59s loss: 0.700834 acc: 76.764647 fscore_macro: 0.688751 val_loss: 0.825407 val_acc: 71.351351 val_fscore_macro: 0.627041\n",
            "Epoch: 278/300 Train steps: 313 Val steps: 35 0.57s loss: 0.698826 acc: 77.104579 fscore_macro: 0.688874 val_loss: 0.812267 val_acc: 73.333333 val_fscore_macro: 0.635662\n",
            "Epoch: 279/300 Train steps: 313 Val steps: 35 0.57s loss: 0.698287 acc: 76.724655 fscore_macro: 0.686130 val_loss: 0.812724 val_acc: 72.072072 val_fscore_macro: 0.627641\n",
            "Epoch: 280/300 Train steps: 313 Val steps: 35 0.60s loss: 0.697910 acc: 76.944611 fscore_macro: 0.690328 val_loss: 0.811278 val_acc: 72.252252 val_fscore_macro: 0.624472\n",
            "Epoch: 281/300 Train steps: 313 Val steps: 35 0.58s loss: 0.696824 acc: 76.824635 fscore_macro: 0.688332 val_loss: 0.808115 val_acc: 73.333333 val_fscore_macro: 0.637574\n",
            "Epoch: 282/300 Train steps: 313 Val steps: 35 0.58s loss: 0.695136 acc: 76.964607 fscore_macro: 0.692795 val_loss: 0.819387 val_acc: 71.711712 val_fscore_macro: 0.618787\n",
            "Epoch: 283/300 Train steps: 313 Val steps: 35 0.59s loss: 0.694194 acc: 76.904619 fscore_macro: 0.688244 val_loss: 0.808592 val_acc: 73.333333 val_fscore_macro: 0.634194\n",
            "Epoch: 284/300 Train steps: 313 Val steps: 35 0.57s loss: 0.693580 acc: 77.124575 fscore_macro: 0.689835 val_loss: 0.811457 val_acc: 72.972973 val_fscore_macro: 0.632488\n",
            "Epoch: 285/300 Train steps: 313 Val steps: 35 0.58s loss: 0.691657 acc: 77.124575 fscore_macro: 0.690839 val_loss: 0.815724 val_acc: 72.072072 val_fscore_macro: 0.636547\n",
            "Epoch: 286/300 Train steps: 313 Val steps: 35 0.58s loss: 0.690291 acc: 77.124575 fscore_macro: 0.694251 val_loss: 0.811475 val_acc: 72.612613 val_fscore_macro: 0.629688\n",
            "Epoch: 287/300 Train steps: 313 Val steps: 35 0.62s loss: 0.688995 acc: 77.404519 fscore_macro: 0.694858 val_loss: 0.804872 val_acc: 72.792793 val_fscore_macro: 0.630646\n",
            "Epoch: 288/300 Train steps: 313 Val steps: 35 0.58s loss: 0.688289 acc: 76.864627 fscore_macro: 0.692968 val_loss: 0.810595 val_acc: 72.252252 val_fscore_macro: 0.632179\n",
            "Epoch: 289/300 Train steps: 313 Val steps: 35 0.59s loss: 0.687408 acc: 77.104579 fscore_macro: 0.694036 val_loss: 0.809319 val_acc: 72.612613 val_fscore_macro: 0.631421\n",
            "Epoch: 290/300 Train steps: 313 Val steps: 35 0.57s loss: 0.686942 acc: 76.984603 fscore_macro: 0.692901 val_loss: 0.819514 val_acc: 71.891892 val_fscore_macro: 0.630201\n",
            "Epoch: 291/300 Train steps: 313 Val steps: 35 0.65s loss: 0.686080 acc: 77.244551 fscore_macro: 0.695987 val_loss: 0.802594 val_acc: 72.792793 val_fscore_macro: 0.632666\n",
            "Epoch: 292/300 Train steps: 313 Val steps: 35 0.58s loss: 0.683428 acc: 77.404519 fscore_macro: 0.698552 val_loss: 0.802861 val_acc: 72.432432 val_fscore_macro: 0.633161\n",
            "Epoch: 293/300 Train steps: 313 Val steps: 35 0.57s loss: 0.683494 acc: 77.524495 fscore_macro: 0.697564 val_loss: 0.812370 val_acc: 72.432432 val_fscore_macro: 0.638703\n",
            "Epoch: 294/300 Train steps: 313 Val steps: 35 0.62s loss: 0.681911 acc: 77.264547 fscore_macro: 0.693142 val_loss: 0.799266 val_acc: 72.612613 val_fscore_macro: 0.638238\n",
            "Epoch: 295/300 Train steps: 313 Val steps: 35 0.59s loss: 0.680776 acc: 77.484503 fscore_macro: 0.696960 val_loss: 0.805906 val_acc: 72.612613 val_fscore_macro: 0.642071\n",
            "Epoch: 296/300 Train steps: 313 Val steps: 35 0.59s loss: 0.680344 acc: 77.344531 fscore_macro: 0.694661 val_loss: 0.808875 val_acc: 71.891892 val_fscore_macro: 0.630551\n",
            "Epoch: 297/300 Train steps: 313 Val steps: 35 0.59s loss: 0.679131 acc: 77.604479 fscore_macro: 0.693231 val_loss: 0.800617 val_acc: 72.792793 val_fscore_macro: 0.643679\n",
            "Epoch: 298/300 Train steps: 313 Val steps: 35 0.61s loss: 0.678030 acc: 77.324535 fscore_macro: 0.698753 val_loss: 0.803447 val_acc: 72.432432 val_fscore_macro: 0.642975\n",
            "Epoch: 299/300 Train steps: 313 Val steps: 35 0.59s loss: 0.676798 acc: 77.464507 fscore_macro: 0.697101 val_loss: 0.810385 val_acc: 72.072072 val_fscore_macro: 0.628624\n",
            "Epoch: 300/300 Train steps: 313 Val steps: 35 0.68s loss: 0.677666 acc: 77.624475 fscore_macro: 0.699411 val_loss: 0.793880 val_acc: 73.693694 val_fscore_macro: 0.651466\n",
            "Epoch 300: val_acc improved from 73.51351 to 73.69369, saving file to model/fasttext/average_embedding_mlp_200/checkpoint_epoch_300.ckpt\n",
            "Restoring data from model/fasttext/average_embedding_mlp_200/checkpoint_epoch_300.ckpt\n",
            "Found best checkpoint at epoch: 300\n",
            "lr: 0.01, loss: 0.677666, acc: 77.6245, fscore_macro: 0.699411, val_loss: 0.79388, val_acc: 73.6937, val_fscore_macro: 0.651466\n",
            "Loading checkpoint model/fasttext/average_embedding_mlp_200/checkpoint_epoch_300.ckpt\n",
            "Running test\n",
            "Test steps: 32 2.19s test_loss: 0.548998 test_acc: 84.200000 test_fscore_macro: 0.738479    \n",
            "\n",
            " Taille de couche cachée égale à : 200, avec les embeddings préentraînés de fasttext aggrégés avec la méthode maxpool :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Epoch:   1/300 Train steps: 313 Val steps: 35 37.92s loss: 2.043192 acc: 22.175565 fscore_macro: 0.056188 val_loss: 2.017096 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch 1: val_acc improved from -inf to 20.72072, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_1.ckpt\n",
            "Epoch:   2/300 Train steps: 313 Val steps: 35 0.62s loss: 1.969615 acc: 22.815437 fscore_macro: 0.042907 val_loss: 2.008487 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   3/300 Train steps: 313 Val steps: 35 0.64s loss: 1.954152 acc: 23.875225 fscore_macro: 0.062055 val_loss: 1.993098 val_acc: 21.801802 val_fscore_macro: 0.054306\n",
            "Epoch 3: val_acc improved from 20.72072 to 21.80180, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_3.ckpt\n",
            "Epoch:   4/300 Train steps: 313 Val steps: 35 0.61s loss: 1.941209 acc: 23.875225 fscore_macro: 0.056560 val_loss: 1.980360 val_acc: 20.900901 val_fscore_macro: 0.042938\n",
            "Epoch:   5/300 Train steps: 313 Val steps: 35 0.61s loss: 1.925227 acc: 23.675265 fscore_macro: 0.057060 val_loss: 1.966574 val_acc: 25.765766 val_fscore_macro: 0.108430\n",
            "Epoch 5: val_acc improved from 21.80180 to 25.76577, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_5.ckpt\n",
            "Epoch:   6/300 Train steps: 313 Val steps: 35 0.60s loss: 1.905935 acc: 27.714457 fscore_macro: 0.113014 val_loss: 1.952263 val_acc: 22.882883 val_fscore_macro: 0.082711\n",
            "Epoch:   7/300 Train steps: 313 Val steps: 35 0.61s loss: 1.886280 acc: 28.854229 fscore_macro: 0.136821 val_loss: 1.932237 val_acc: 30.270270 val_fscore_macro: 0.165438\n",
            "Epoch 7: val_acc improved from 25.76577 to 30.27027, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_7.ckpt\n",
            "Epoch:   8/300 Train steps: 313 Val steps: 35 0.58s loss: 1.864566 acc: 32.253549 fscore_macro: 0.167500 val_loss: 1.914057 val_acc: 30.270270 val_fscore_macro: 0.167804\n",
            "Epoch 8: val_acc improved from 30.27027 to 30.27027, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_8.ckpt\n",
            "Epoch:   9/300 Train steps: 313 Val steps: 35 0.57s loss: 1.842217 acc: 33.693261 fscore_macro: 0.182325 val_loss: 1.893575 val_acc: 30.810811 val_fscore_macro: 0.180502\n",
            "Epoch 9: val_acc improved from 30.27027 to 30.81081, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_9.ckpt\n",
            "Epoch:  10/300 Train steps: 313 Val steps: 35 0.59s loss: 1.819516 acc: 35.092981 fscore_macro: 0.199286 val_loss: 1.873588 val_acc: 30.630631 val_fscore_macro: 0.176846\n",
            "Epoch:  11/300 Train steps: 313 Val steps: 35 0.59s loss: 1.797885 acc: 35.592881 fscore_macro: 0.204764 val_loss: 1.854649 val_acc: 32.612613 val_fscore_macro: 0.203986\n",
            "Epoch 11: val_acc improved from 30.81081 to 32.61261, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_11.ckpt\n",
            "Epoch:  12/300 Train steps: 313 Val steps: 35 0.58s loss: 1.776426 acc: 36.732653 fscore_macro: 0.218725 val_loss: 1.839119 val_acc: 30.450450 val_fscore_macro: 0.176864\n",
            "Epoch:  13/300 Train steps: 313 Val steps: 35 0.59s loss: 1.755709 acc: 37.292542 fscore_macro: 0.224377 val_loss: 1.823781 val_acc: 31.711712 val_fscore_macro: 0.186747\n",
            "Epoch:  14/300 Train steps: 313 Val steps: 35 0.59s loss: 1.735893 acc: 38.592282 fscore_macro: 0.235029 val_loss: 1.802772 val_acc: 32.972973 val_fscore_macro: 0.209173\n",
            "Epoch 14: val_acc improved from 32.61261 to 32.97297, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_14.ckpt\n",
            "Epoch:  15/300 Train steps: 313 Val steps: 35 0.57s loss: 1.717628 acc: 38.992202 fscore_macro: 0.238935 val_loss: 1.787019 val_acc: 32.972973 val_fscore_macro: 0.208059\n",
            "Epoch 15: val_acc improved from 32.97297 to 32.97297, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_15.ckpt\n",
            "Epoch:  16/300 Train steps: 313 Val steps: 35 0.59s loss: 1.699460 acc: 39.572086 fscore_macro: 0.244168 val_loss: 1.772853 val_acc: 33.513514 val_fscore_macro: 0.216697\n",
            "Epoch 16: val_acc improved from 32.97297 to 33.51351, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_16.ckpt\n",
            "Epoch:  17/300 Train steps: 313 Val steps: 35 0.62s loss: 1.682901 acc: 40.171966 fscore_macro: 0.251807 val_loss: 1.757780 val_acc: 33.693694 val_fscore_macro: 0.215297\n",
            "Epoch 17: val_acc improved from 33.51351 to 33.69369, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_17.ckpt\n",
            "Epoch:  18/300 Train steps: 313 Val steps: 35 0.62s loss: 1.664528 acc: 40.751850 fscore_macro: 0.255256 val_loss: 1.742991 val_acc: 34.234234 val_fscore_macro: 0.214747\n",
            "Epoch 18: val_acc improved from 33.69369 to 34.23423, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_18.ckpt\n",
            "Epoch:  19/300 Train steps: 313 Val steps: 35 0.61s loss: 1.650431 acc: 40.891822 fscore_macro: 0.259063 val_loss: 1.728087 val_acc: 35.675676 val_fscore_macro: 0.224371\n",
            "Epoch 19: val_acc improved from 34.23423 to 35.67568, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_19.ckpt\n",
            "Epoch:  20/300 Train steps: 313 Val steps: 35 0.60s loss: 1.635249 acc: 41.391722 fscore_macro: 0.262914 val_loss: 1.711355 val_acc: 37.477477 val_fscore_macro: 0.244548\n",
            "Epoch 20: val_acc improved from 35.67568 to 37.47748, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_20.ckpt\n",
            "Epoch:  21/300 Train steps: 313 Val steps: 35 0.59s loss: 1.620004 acc: 42.511498 fscore_macro: 0.274942 val_loss: 1.696724 val_acc: 37.837838 val_fscore_macro: 0.245515\n",
            "Epoch 21: val_acc improved from 37.47748 to 37.83784, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_21.ckpt\n",
            "Epoch:  22/300 Train steps: 313 Val steps: 35 0.60s loss: 1.605852 acc: 42.491502 fscore_macro: 0.277509 val_loss: 1.688739 val_acc: 37.477478 val_fscore_macro: 0.242276\n",
            "Epoch:  23/300 Train steps: 313 Val steps: 35 0.58s loss: 1.590424 acc: 43.251350 fscore_macro: 0.284722 val_loss: 1.690114 val_acc: 37.657658 val_fscore_macro: 0.244851\n",
            "Epoch:  24/300 Train steps: 313 Val steps: 35 0.60s loss: 1.579960 acc: 43.091382 fscore_macro: 0.286440 val_loss: 1.665578 val_acc: 39.459459 val_fscore_macro: 0.254454\n",
            "Epoch 24: val_acc improved from 37.83784 to 39.45946, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_24.ckpt\n",
            "Epoch:  25/300 Train steps: 313 Val steps: 35 0.60s loss: 1.566193 acc: 44.031194 fscore_macro: 0.291564 val_loss: 1.656226 val_acc: 38.018018 val_fscore_macro: 0.239106\n",
            "Epoch:  26/300 Train steps: 313 Val steps: 35 0.59s loss: 1.554437 acc: 44.091182 fscore_macro: 0.296266 val_loss: 1.637043 val_acc: 40.180180 val_fscore_macro: 0.259628\n",
            "Epoch 26: val_acc improved from 39.45946 to 40.18018, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_26.ckpt\n",
            "Epoch:  27/300 Train steps: 313 Val steps: 35 0.61s loss: 1.543317 acc: 44.511098 fscore_macro: 0.298956 val_loss: 1.632216 val_acc: 42.162162 val_fscore_macro: 0.291162\n",
            "Epoch 27: val_acc improved from 40.18018 to 42.16216, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_27.ckpt\n",
            "Epoch:  28/300 Train steps: 313 Val steps: 35 0.61s loss: 1.531501 acc: 45.170966 fscore_macro: 0.306203 val_loss: 1.616458 val_acc: 42.162162 val_fscore_macro: 0.288479\n",
            "Epoch:  29/300 Train steps: 313 Val steps: 35 0.64s loss: 1.522161 acc: 45.510898 fscore_macro: 0.311341 val_loss: 1.604675 val_acc: 43.063063 val_fscore_macro: 0.298564\n",
            "Epoch 29: val_acc improved from 42.16216 to 43.06306, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_29.ckpt\n",
            "Epoch:  30/300 Train steps: 313 Val steps: 35 0.59s loss: 1.509335 acc: 46.270746 fscore_macro: 0.318325 val_loss: 1.604006 val_acc: 44.144144 val_fscore_macro: 0.307258\n",
            "Epoch 30: val_acc improved from 43.06306 to 44.14414, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_30.ckpt\n",
            "Epoch:  31/300 Train steps: 313 Val steps: 35 0.59s loss: 1.501296 acc: 46.790642 fscore_macro: 0.322901 val_loss: 1.591539 val_acc: 44.144144 val_fscore_macro: 0.308983\n",
            "Epoch:  32/300 Train steps: 313 Val steps: 35 0.61s loss: 1.488436 acc: 47.490502 fscore_macro: 0.328585 val_loss: 1.578667 val_acc: 45.585586 val_fscore_macro: 0.320382\n",
            "Epoch 32: val_acc improved from 44.14414 to 45.58559, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_32.ckpt\n",
            "Epoch:  33/300 Train steps: 313 Val steps: 35 0.60s loss: 1.480225 acc: 47.910418 fscore_macro: 0.332207 val_loss: 1.564419 val_acc: 47.027027 val_fscore_macro: 0.331834\n",
            "Epoch 33: val_acc improved from 45.58559 to 47.02703, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_33.ckpt\n",
            "Epoch:  34/300 Train steps: 313 Val steps: 35 0.70s loss: 1.470715 acc: 48.090382 fscore_macro: 0.336300 val_loss: 1.557143 val_acc: 47.027027 val_fscore_macro: 0.336984\n",
            "Epoch:  35/300 Train steps: 313 Val steps: 35 0.80s loss: 1.460086 acc: 48.670266 fscore_macro: 0.339409 val_loss: 1.544495 val_acc: 45.225225 val_fscore_macro: 0.317732\n",
            "Epoch:  36/300 Train steps: 313 Val steps: 35 0.80s loss: 1.449871 acc: 49.110178 fscore_macro: 0.343143 val_loss: 1.534940 val_acc: 48.828829 val_fscore_macro: 0.344246\n",
            "Epoch 36: val_acc improved from 47.02703 to 48.82883, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_36.ckpt\n",
            "Epoch:  37/300 Train steps: 313 Val steps: 35 0.85s loss: 1.439938 acc: 50.269946 fscore_macro: 0.352499 val_loss: 1.526249 val_acc: 49.909910 val_fscore_macro: 0.354573\n",
            "Epoch 37: val_acc improved from 48.82883 to 49.90991, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_37.ckpt\n",
            "Epoch:  38/300 Train steps: 313 Val steps: 35 0.86s loss: 1.431100 acc: 50.489902 fscore_macro: 0.357590 val_loss: 1.512055 val_acc: 49.729730 val_fscore_macro: 0.351160\n",
            "Epoch:  39/300 Train steps: 313 Val steps: 35 0.79s loss: 1.418656 acc: 51.129774 fscore_macro: 0.361940 val_loss: 1.500564 val_acc: 50.810811 val_fscore_macro: 0.359621\n",
            "Epoch 39: val_acc improved from 49.90991 to 50.81081, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_39.ckpt\n",
            "Epoch:  40/300 Train steps: 313 Val steps: 35 0.58s loss: 1.409037 acc: 51.829634 fscore_macro: 0.368051 val_loss: 1.490131 val_acc: 51.711712 val_fscore_macro: 0.367868\n",
            "Epoch 40: val_acc improved from 50.81081 to 51.71171, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_40.ckpt\n",
            "Epoch:  41/300 Train steps: 313 Val steps: 35 0.62s loss: 1.400037 acc: 51.969606 fscore_macro: 0.368907 val_loss: 1.488476 val_acc: 49.189189 val_fscore_macro: 0.356050\n",
            "Epoch:  42/300 Train steps: 313 Val steps: 35 0.63s loss: 1.390994 acc: 52.369526 fscore_macro: 0.373139 val_loss: 1.472088 val_acc: 53.513514 val_fscore_macro: 0.377905\n",
            "Epoch 42: val_acc improved from 51.71171 to 53.51351, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_42.ckpt\n",
            "Epoch:  43/300 Train steps: 313 Val steps: 35 0.57s loss: 1.382455 acc: 52.869426 fscore_macro: 0.380087 val_loss: 1.459100 val_acc: 53.333333 val_fscore_macro: 0.376651\n",
            "Epoch:  44/300 Train steps: 313 Val steps: 35 0.59s loss: 1.371172 acc: 53.769246 fscore_macro: 0.382563 val_loss: 1.479426 val_acc: 49.009009 val_fscore_macro: 0.355963\n",
            "Epoch:  45/300 Train steps: 313 Val steps: 35 0.59s loss: 1.362368 acc: 54.249150 fscore_macro: 0.391853 val_loss: 1.448306 val_acc: 50.450450 val_fscore_macro: 0.368324\n",
            "Epoch:  46/300 Train steps: 313 Val steps: 35 0.59s loss: 1.352048 acc: 54.289142 fscore_macro: 0.395577 val_loss: 1.429603 val_acc: 52.972973 val_fscore_macro: 0.380073\n",
            "Epoch:  47/300 Train steps: 313 Val steps: 35 0.59s loss: 1.344337 acc: 54.669066 fscore_macro: 0.403217 val_loss: 1.426060 val_acc: 53.873874 val_fscore_macro: 0.386602\n",
            "Epoch 47: val_acc improved from 53.51351 to 53.87387, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_47.ckpt\n",
            "Epoch:  48/300 Train steps: 313 Val steps: 35 0.58s loss: 1.337320 acc: 54.749050 fscore_macro: 0.398963 val_loss: 1.414955 val_acc: 54.054054 val_fscore_macro: 0.384191\n",
            "Epoch 48: val_acc improved from 53.87387 to 54.05405, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_48.ckpt\n",
            "Epoch:  49/300 Train steps: 313 Val steps: 35 0.59s loss: 1.327249 acc: 55.288942 fscore_macro: 0.408122 val_loss: 1.406373 val_acc: 55.135135 val_fscore_macro: 0.388849\n",
            "Epoch 49: val_acc improved from 54.05405 to 55.13514, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_49.ckpt\n",
            "Epoch:  50/300 Train steps: 313 Val steps: 35 0.58s loss: 1.317472 acc: 55.608878 fscore_macro: 0.411538 val_loss: 1.402331 val_acc: 54.234234 val_fscore_macro: 0.388737\n",
            "Epoch:  51/300 Train steps: 313 Val steps: 35 0.59s loss: 1.310417 acc: 55.788842 fscore_macro: 0.417570 val_loss: 1.398343 val_acc: 53.513514 val_fscore_macro: 0.387685\n",
            "Epoch:  52/300 Train steps: 313 Val steps: 35 0.62s loss: 1.303603 acc: 56.008798 fscore_macro: 0.419010 val_loss: 1.383146 val_acc: 54.054054 val_fscore_macro: 0.392516\n",
            "Epoch:  53/300 Train steps: 313 Val steps: 35 0.59s loss: 1.294468 acc: 56.248750 fscore_macro: 0.420047 val_loss: 1.380623 val_acc: 54.774775 val_fscore_macro: 0.398334\n",
            "Epoch:  54/300 Train steps: 313 Val steps: 35 0.57s loss: 1.286097 acc: 56.548690 fscore_macro: 0.421880 val_loss: 1.378597 val_acc: 54.594595 val_fscore_macro: 0.397866\n",
            "Epoch:  55/300 Train steps: 313 Val steps: 35 0.58s loss: 1.280632 acc: 56.828634 fscore_macro: 0.432417 val_loss: 1.360713 val_acc: 55.495495 val_fscore_macro: 0.401928\n",
            "Epoch 55: val_acc improved from 55.13514 to 55.49550, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_55.ckpt\n",
            "Epoch:  56/300 Train steps: 313 Val steps: 35 0.59s loss: 1.272041 acc: 57.388522 fscore_macro: 0.433724 val_loss: 1.356492 val_acc: 55.495496 val_fscore_macro: 0.403853\n",
            "Epoch 56: val_acc improved from 55.49550 to 55.49550, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_56.ckpt\n",
            "Epoch:  57/300 Train steps: 313 Val steps: 35 0.58s loss: 1.267242 acc: 56.868626 fscore_macro: 0.431518 val_loss: 1.354008 val_acc: 54.774775 val_fscore_macro: 0.396319\n",
            "Epoch:  58/300 Train steps: 313 Val steps: 35 0.59s loss: 1.261371 acc: 57.648470 fscore_macro: 0.436549 val_loss: 1.343038 val_acc: 56.036036 val_fscore_macro: 0.407441\n",
            "Epoch 58: val_acc improved from 55.49550 to 56.03604, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_58.ckpt\n",
            "Epoch:  59/300 Train steps: 313 Val steps: 35 0.62s loss: 1.252862 acc: 57.628474 fscore_macro: 0.437810 val_loss: 1.337543 val_acc: 55.495495 val_fscore_macro: 0.414051\n",
            "Epoch:  60/300 Train steps: 313 Val steps: 35 0.63s loss: 1.246295 acc: 58.128374 fscore_macro: 0.443584 val_loss: 1.334693 val_acc: 53.513514 val_fscore_macro: 0.384344\n",
            "Epoch:  61/300 Train steps: 313 Val steps: 35 0.61s loss: 1.242042 acc: 58.028394 fscore_macro: 0.444687 val_loss: 1.332875 val_acc: 56.396396 val_fscore_macro: 0.408219\n",
            "Epoch 61: val_acc improved from 56.03604 to 56.39640, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_61.ckpt\n",
            "Epoch:  62/300 Train steps: 313 Val steps: 35 0.61s loss: 1.233561 acc: 58.408318 fscore_macro: 0.446490 val_loss: 1.348676 val_acc: 56.036036 val_fscore_macro: 0.417230\n",
            "Epoch:  63/300 Train steps: 313 Val steps: 35 0.58s loss: 1.229561 acc: 58.248350 fscore_macro: 0.444951 val_loss: 1.309718 val_acc: 57.117117 val_fscore_macro: 0.423139\n",
            "Epoch 63: val_acc improved from 56.39640 to 57.11712, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_63.ckpt\n",
            "Epoch:  64/300 Train steps: 313 Val steps: 35 0.62s loss: 1.224732 acc: 58.148370 fscore_macro: 0.443527 val_loss: 1.312024 val_acc: 57.117117 val_fscore_macro: 0.416780\n",
            "Epoch:  65/300 Train steps: 313 Val steps: 35 0.58s loss: 1.219723 acc: 58.648270 fscore_macro: 0.448214 val_loss: 1.319046 val_acc: 56.396396 val_fscore_macro: 0.409598\n",
            "Epoch:  66/300 Train steps: 313 Val steps: 35 0.60s loss: 1.212510 acc: 58.768246 fscore_macro: 0.451284 val_loss: 1.297779 val_acc: 55.495496 val_fscore_macro: 0.417724\n",
            "Epoch:  67/300 Train steps: 313 Val steps: 35 0.59s loss: 1.208146 acc: 58.808238 fscore_macro: 0.451521 val_loss: 1.292374 val_acc: 56.576577 val_fscore_macro: 0.425120\n",
            "Epoch:  68/300 Train steps: 313 Val steps: 35 0.60s loss: 1.203214 acc: 59.128174 fscore_macro: 0.456668 val_loss: 1.295343 val_acc: 56.396396 val_fscore_macro: 0.425168\n",
            "Epoch:  69/300 Train steps: 313 Val steps: 35 0.58s loss: 1.197434 acc: 59.308138 fscore_macro: 0.457665 val_loss: 1.289649 val_acc: 55.675676 val_fscore_macro: 0.434634\n",
            "Epoch:  70/300 Train steps: 313 Val steps: 35 0.61s loss: 1.193854 acc: 59.648070 fscore_macro: 0.459791 val_loss: 1.276197 val_acc: 56.756757 val_fscore_macro: 0.432488\n",
            "Epoch:  71/300 Train steps: 313 Val steps: 35 0.59s loss: 1.186158 acc: 59.048190 fscore_macro: 0.455804 val_loss: 1.281294 val_acc: 57.477478 val_fscore_macro: 0.449486\n",
            "Epoch 71: val_acc improved from 57.11712 to 57.47748, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_71.ckpt\n",
            "Epoch:  72/300 Train steps: 313 Val steps: 35 0.59s loss: 1.181547 acc: 59.888022 fscore_macro: 0.464626 val_loss: 1.282411 val_acc: 55.855856 val_fscore_macro: 0.415162\n",
            "Epoch:  73/300 Train steps: 313 Val steps: 35 0.57s loss: 1.178492 acc: 59.788042 fscore_macro: 0.465026 val_loss: 1.272929 val_acc: 56.396396 val_fscore_macro: 0.434684\n",
            "Epoch:  74/300 Train steps: 313 Val steps: 35 0.58s loss: 1.174379 acc: 60.327934 fscore_macro: 0.469234 val_loss: 1.286436 val_acc: 53.693694 val_fscore_macro: 0.389539\n",
            "Epoch:  75/300 Train steps: 313 Val steps: 35 0.59s loss: 1.171777 acc: 60.007998 fscore_macro: 0.464183 val_loss: 1.261655 val_acc: 56.576577 val_fscore_macro: 0.436546\n",
            "Epoch:  76/300 Train steps: 313 Val steps: 35 0.59s loss: 1.165130 acc: 60.007998 fscore_macro: 0.465723 val_loss: 1.273926 val_acc: 55.675676 val_fscore_macro: 0.418393\n",
            "Epoch:  77/300 Train steps: 313 Val steps: 35 0.59s loss: 1.162304 acc: 60.007998 fscore_macro: 0.466027 val_loss: 1.275837 val_acc: 56.756757 val_fscore_macro: 0.429170\n",
            "Epoch:  78/300 Train steps: 313 Val steps: 35 0.59s loss: 1.156495 acc: 60.627874 fscore_macro: 0.471135 val_loss: 1.247128 val_acc: 57.477478 val_fscore_macro: 0.448362\n",
            "Epoch 78: val_acc improved from 57.47748 to 57.47748, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_78.ckpt\n",
            "Epoch:  79/300 Train steps: 313 Val steps: 35 0.58s loss: 1.154019 acc: 60.807838 fscore_macro: 0.475653 val_loss: 1.263320 val_acc: 56.936937 val_fscore_macro: 0.453569\n",
            "Epoch:  80/300 Train steps: 313 Val steps: 35 0.58s loss: 1.149973 acc: 60.707858 fscore_macro: 0.472577 val_loss: 1.260125 val_acc: 56.576577 val_fscore_macro: 0.453848\n",
            "Epoch:  81/300 Train steps: 313 Val steps: 35 0.59s loss: 1.146571 acc: 60.567886 fscore_macro: 0.471969 val_loss: 1.244923 val_acc: 57.117117 val_fscore_macro: 0.461884\n",
            "Epoch:  82/300 Train steps: 313 Val steps: 35 0.59s loss: 1.142008 acc: 61.207758 fscore_macro: 0.481098 val_loss: 1.246915 val_acc: 56.396396 val_fscore_macro: 0.455065\n",
            "Epoch:  83/300 Train steps: 313 Val steps: 35 0.63s loss: 1.138399 acc: 60.667866 fscore_macro: 0.481937 val_loss: 1.244256 val_acc: 56.396396 val_fscore_macro: 0.451586\n",
            "Epoch:  84/300 Train steps: 313 Val steps: 35 0.64s loss: 1.135117 acc: 61.147770 fscore_macro: 0.484083 val_loss: 1.251217 val_acc: 58.198198 val_fscore_macro: 0.486222\n",
            "Epoch 84: val_acc improved from 57.47748 to 58.19820, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_84.ckpt\n",
            "Epoch:  85/300 Train steps: 313 Val steps: 35 0.60s loss: 1.131136 acc: 61.487702 fscore_macro: 0.492475 val_loss: 1.222771 val_acc: 58.378378 val_fscore_macro: 0.470500\n",
            "Epoch 85: val_acc improved from 58.19820 to 58.37838, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_85.ckpt\n",
            "Epoch:  86/300 Train steps: 313 Val steps: 35 0.61s loss: 1.124431 acc: 61.927614 fscore_macro: 0.493064 val_loss: 1.245056 val_acc: 57.297297 val_fscore_macro: 0.456228\n",
            "Epoch:  87/300 Train steps: 313 Val steps: 35 0.58s loss: 1.123079 acc: 61.647670 fscore_macro: 0.498366 val_loss: 1.233432 val_acc: 59.099099 val_fscore_macro: 0.488429\n",
            "Epoch 87: val_acc improved from 58.37838 to 59.09910, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_87.ckpt\n",
            "Epoch:  88/300 Train steps: 313 Val steps: 35 0.60s loss: 1.116697 acc: 61.767646 fscore_macro: 0.504469 val_loss: 1.226354 val_acc: 58.918919 val_fscore_macro: 0.471230\n",
            "Epoch:  89/300 Train steps: 313 Val steps: 35 0.59s loss: 1.116365 acc: 61.807638 fscore_macro: 0.497242 val_loss: 1.237265 val_acc: 55.135135 val_fscore_macro: 0.445871\n",
            "Epoch:  90/300 Train steps: 313 Val steps: 35 0.58s loss: 1.111349 acc: 62.427515 fscore_macro: 0.508230 val_loss: 1.230255 val_acc: 57.117117 val_fscore_macro: 0.466678\n",
            "Epoch:  91/300 Train steps: 313 Val steps: 35 0.62s loss: 1.107519 acc: 61.767646 fscore_macro: 0.506295 val_loss: 1.226036 val_acc: 58.378378 val_fscore_macro: 0.491901\n",
            "Epoch:  92/300 Train steps: 313 Val steps: 35 0.59s loss: 1.105244 acc: 62.627475 fscore_macro: 0.513314 val_loss: 1.226776 val_acc: 57.657658 val_fscore_macro: 0.457867\n",
            "Epoch:  93/300 Train steps: 313 Val steps: 35 0.58s loss: 1.101956 acc: 61.987602 fscore_macro: 0.509713 val_loss: 1.217983 val_acc: 56.936937 val_fscore_macro: 0.468372\n",
            "Epoch:  94/300 Train steps: 313 Val steps: 35 0.61s loss: 1.097134 acc: 62.987403 fscore_macro: 0.523445 val_loss: 1.221019 val_acc: 57.837838 val_fscore_macro: 0.468377\n",
            "Epoch:  95/300 Train steps: 313 Val steps: 35 0.58s loss: 1.093824 acc: 62.907419 fscore_macro: 0.517078 val_loss: 1.212441 val_acc: 59.459459 val_fscore_macro: 0.508237\n",
            "Epoch 95: val_acc improved from 59.09910 to 59.45946, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_95.ckpt\n",
            "Epoch:  96/300 Train steps: 313 Val steps: 35 0.59s loss: 1.094258 acc: 62.447511 fscore_macro: 0.516706 val_loss: 1.224035 val_acc: 58.018018 val_fscore_macro: 0.472890\n",
            "Epoch:  97/300 Train steps: 313 Val steps: 35 0.58s loss: 1.088418 acc: 63.067387 fscore_macro: 0.522762 val_loss: 1.253765 val_acc: 55.135135 val_fscore_macro: 0.463605\n",
            "Epoch:  98/300 Train steps: 313 Val steps: 35 0.58s loss: 1.086045 acc: 62.787443 fscore_macro: 0.523202 val_loss: 1.209977 val_acc: 58.558559 val_fscore_macro: 0.499254\n",
            "Epoch:  99/300 Train steps: 313 Val steps: 35 0.59s loss: 1.082779 acc: 62.887423 fscore_macro: 0.526334 val_loss: 1.192166 val_acc: 58.198198 val_fscore_macro: 0.490355\n",
            "Epoch: 100/300 Train steps: 313 Val steps: 35 0.60s loss: 1.079009 acc: 63.147371 fscore_macro: 0.530315 val_loss: 1.194037 val_acc: 58.918919 val_fscore_macro: 0.496975\n",
            "Epoch: 101/300 Train steps: 313 Val steps: 35 0.58s loss: 1.076054 acc: 63.167367 fscore_macro: 0.526318 val_loss: 1.195697 val_acc: 58.918919 val_fscore_macro: 0.501841\n",
            "Epoch: 102/300 Train steps: 313 Val steps: 35 0.60s loss: 1.073306 acc: 63.567287 fscore_macro: 0.537155 val_loss: 1.190624 val_acc: 59.819820 val_fscore_macro: 0.497446\n",
            "Epoch 102: val_acc improved from 59.45946 to 59.81982, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_102.ckpt\n",
            "Epoch: 103/300 Train steps: 313 Val steps: 35 0.60s loss: 1.067990 acc: 64.167167 fscore_macro: 0.545403 val_loss: 1.187924 val_acc: 57.657658 val_fscore_macro: 0.482260\n",
            "Epoch: 104/300 Train steps: 313 Val steps: 35 0.58s loss: 1.068612 acc: 63.507299 fscore_macro: 0.538242 val_loss: 1.202158 val_acc: 58.198198 val_fscore_macro: 0.492867\n",
            "Epoch: 105/300 Train steps: 313 Val steps: 35 0.60s loss: 1.063978 acc: 63.747251 fscore_macro: 0.538224 val_loss: 1.192460 val_acc: 58.918919 val_fscore_macro: 0.490756\n",
            "Epoch: 106/300 Train steps: 313 Val steps: 35 0.62s loss: 1.061531 acc: 63.987203 fscore_macro: 0.541645 val_loss: 1.185287 val_acc: 59.279279 val_fscore_macro: 0.506113\n",
            "Epoch: 107/300 Train steps: 313 Val steps: 35 0.57s loss: 1.059256 acc: 63.967207 fscore_macro: 0.545211 val_loss: 1.186294 val_acc: 57.837838 val_fscore_macro: 0.494055\n",
            "Epoch: 108/300 Train steps: 313 Val steps: 35 0.61s loss: 1.054683 acc: 64.167167 fscore_macro: 0.545114 val_loss: 1.210180 val_acc: 58.558559 val_fscore_macro: 0.494265\n",
            "Epoch: 109/300 Train steps: 313 Val steps: 35 0.62s loss: 1.053837 acc: 64.247151 fscore_macro: 0.548736 val_loss: 1.174381 val_acc: 59.279279 val_fscore_macro: 0.508193\n",
            "Epoch: 110/300 Train steps: 313 Val steps: 35 0.59s loss: 1.050735 acc: 64.627075 fscore_macro: 0.555736 val_loss: 1.187053 val_acc: 58.018018 val_fscore_macro: 0.489493\n",
            "Epoch: 111/300 Train steps: 313 Val steps: 35 0.60s loss: 1.050148 acc: 64.227155 fscore_macro: 0.547088 val_loss: 1.183745 val_acc: 58.018018 val_fscore_macro: 0.495362\n",
            "Epoch: 112/300 Train steps: 313 Val steps: 35 0.60s loss: 1.046326 acc: 63.947211 fscore_macro: 0.552363 val_loss: 1.174211 val_acc: 59.279279 val_fscore_macro: 0.499284\n",
            "Epoch: 113/300 Train steps: 313 Val steps: 35 0.58s loss: 1.044705 acc: 64.567087 fscore_macro: 0.550024 val_loss: 1.188391 val_acc: 58.558559 val_fscore_macro: 0.499456\n",
            "Epoch: 114/300 Train steps: 313 Val steps: 35 0.60s loss: 1.039312 acc: 64.967007 fscore_macro: 0.555423 val_loss: 1.162635 val_acc: 59.639640 val_fscore_macro: 0.512778\n",
            "Epoch: 115/300 Train steps: 313 Val steps: 35 0.60s loss: 1.036761 acc: 65.066987 fscore_macro: 0.563009 val_loss: 1.217124 val_acc: 58.378378 val_fscore_macro: 0.514247\n",
            "Epoch: 116/300 Train steps: 313 Val steps: 35 0.60s loss: 1.036870 acc: 65.066987 fscore_macro: 0.560368 val_loss: 1.169448 val_acc: 60.180180 val_fscore_macro: 0.501957\n",
            "Epoch 116: val_acc improved from 59.81982 to 60.18018, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_116.ckpt\n",
            "Epoch: 117/300 Train steps: 313 Val steps: 35 0.60s loss: 1.032649 acc: 65.126975 fscore_macro: 0.562071 val_loss: 1.191029 val_acc: 58.378378 val_fscore_macro: 0.511440\n",
            "Epoch: 118/300 Train steps: 313 Val steps: 35 0.58s loss: 1.029781 acc: 65.186963 fscore_macro: 0.562834 val_loss: 1.159031 val_acc: 59.459459 val_fscore_macro: 0.512724\n",
            "Epoch: 119/300 Train steps: 313 Val steps: 35 0.59s loss: 1.029878 acc: 65.046991 fscore_macro: 0.568627 val_loss: 1.168858 val_acc: 59.639640 val_fscore_macro: 0.497768\n",
            "Epoch: 120/300 Train steps: 313 Val steps: 35 0.59s loss: 1.025534 acc: 64.967007 fscore_macro: 0.563372 val_loss: 1.194427 val_acc: 56.936937 val_fscore_macro: 0.490664\n",
            "Epoch: 121/300 Train steps: 313 Val steps: 35 0.59s loss: 1.026627 acc: 65.146971 fscore_macro: 0.569454 val_loss: 1.177533 val_acc: 57.657658 val_fscore_macro: 0.497984\n",
            "Epoch: 122/300 Train steps: 313 Val steps: 35 0.60s loss: 1.020991 acc: 65.726855 fscore_macro: 0.569949 val_loss: 1.158216 val_acc: 59.099099 val_fscore_macro: 0.514236\n",
            "Epoch: 123/300 Train steps: 313 Val steps: 35 0.61s loss: 1.015607 acc: 65.086983 fscore_macro: 0.567587 val_loss: 1.179278 val_acc: 58.018018 val_fscore_macro: 0.500968\n",
            "Epoch: 124/300 Train steps: 313 Val steps: 35 0.58s loss: 1.015821 acc: 65.666867 fscore_macro: 0.573931 val_loss: 1.156120 val_acc: 60.180180 val_fscore_macro: 0.517727\n",
            "Epoch 124: val_acc improved from 60.18018 to 60.18018, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_124.ckpt\n",
            "Epoch: 125/300 Train steps: 313 Val steps: 35 0.58s loss: 1.016564 acc: 65.486903 fscore_macro: 0.569059 val_loss: 1.152775 val_acc: 60.360360 val_fscore_macro: 0.514474\n",
            "Epoch 125: val_acc improved from 60.18018 to 60.36036, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_125.ckpt\n",
            "Epoch: 126/300 Train steps: 313 Val steps: 35 0.59s loss: 1.013532 acc: 65.406919 fscore_macro: 0.573746 val_loss: 1.142024 val_acc: 60.180180 val_fscore_macro: 0.508843\n",
            "Epoch: 127/300 Train steps: 313 Val steps: 35 0.59s loss: 1.010483 acc: 65.826835 fscore_macro: 0.576910 val_loss: 1.147007 val_acc: 59.639640 val_fscore_macro: 0.502483\n",
            "Epoch: 128/300 Train steps: 313 Val steps: 35 0.60s loss: 1.008437 acc: 65.726855 fscore_macro: 0.575247 val_loss: 1.196704 val_acc: 57.837838 val_fscore_macro: 0.491236\n",
            "Epoch: 129/300 Train steps: 313 Val steps: 35 0.61s loss: 1.006972 acc: 65.906819 fscore_macro: 0.580467 val_loss: 1.177100 val_acc: 57.657658 val_fscore_macro: 0.491558\n",
            "Epoch: 130/300 Train steps: 313 Val steps: 35 0.66s loss: 1.002679 acc: 65.806839 fscore_macro: 0.577354 val_loss: 1.146209 val_acc: 60.180180 val_fscore_macro: 0.512013\n",
            "Epoch: 131/300 Train steps: 313 Val steps: 35 0.78s loss: 1.002768 acc: 66.046791 fscore_macro: 0.584631 val_loss: 1.142890 val_acc: 58.918919 val_fscore_macro: 0.504905\n",
            "Epoch: 132/300 Train steps: 313 Val steps: 35 0.81s loss: 1.000659 acc: 65.346931 fscore_macro: 0.574094 val_loss: 1.155880 val_acc: 60.900901 val_fscore_macro: 0.519497\n",
            "Epoch 132: val_acc improved from 60.36036 to 60.90090, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_132.ckpt\n",
            "Epoch: 133/300 Train steps: 313 Val steps: 35 0.83s loss: 0.999279 acc: 66.406719 fscore_macro: 0.582337 val_loss: 1.150505 val_acc: 59.279279 val_fscore_macro: 0.513630\n",
            "Epoch: 134/300 Train steps: 313 Val steps: 35 0.80s loss: 0.994512 acc: 65.766847 fscore_macro: 0.580097 val_loss: 1.165696 val_acc: 60.000000 val_fscore_macro: 0.510464\n",
            "Epoch: 135/300 Train steps: 313 Val steps: 35 0.80s loss: 0.996210 acc: 65.626875 fscore_macro: 0.581755 val_loss: 1.148378 val_acc: 61.261261 val_fscore_macro: 0.521600\n",
            "Epoch 135: val_acc improved from 60.90090 to 61.26126, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_135.ckpt\n",
            "Epoch: 136/300 Train steps: 313 Val steps: 35 0.74s loss: 0.991041 acc: 66.446711 fscore_macro: 0.584244 val_loss: 1.154564 val_acc: 58.198198 val_fscore_macro: 0.506457\n",
            "Epoch: 137/300 Train steps: 313 Val steps: 35 0.58s loss: 0.989872 acc: 66.386723 fscore_macro: 0.586790 val_loss: 1.136951 val_acc: 60.540541 val_fscore_macro: 0.513142\n",
            "Epoch: 138/300 Train steps: 313 Val steps: 35 0.57s loss: 0.988043 acc: 66.506699 fscore_macro: 0.587106 val_loss: 1.163795 val_acc: 59.639640 val_fscore_macro: 0.516302\n",
            "Epoch: 139/300 Train steps: 313 Val steps: 35 0.60s loss: 0.986394 acc: 66.146771 fscore_macro: 0.583197 val_loss: 1.159565 val_acc: 60.360360 val_fscore_macro: 0.518280\n",
            "Epoch: 140/300 Train steps: 313 Val steps: 35 0.58s loss: 0.985094 acc: 66.666667 fscore_macro: 0.590301 val_loss: 1.176531 val_acc: 57.837838 val_fscore_macro: 0.517525\n",
            "Epoch: 141/300 Train steps: 313 Val steps: 35 0.60s loss: 0.982465 acc: 66.186763 fscore_macro: 0.586256 val_loss: 1.180695 val_acc: 58.558559 val_fscore_macro: 0.505630\n",
            "Epoch: 142/300 Train steps: 313 Val steps: 35 0.59s loss: 0.981971 acc: 66.446711 fscore_macro: 0.589708 val_loss: 1.156907 val_acc: 58.918919 val_fscore_macro: 0.511415\n",
            "Epoch: 143/300 Train steps: 313 Val steps: 35 0.60s loss: 0.978807 acc: 66.326735 fscore_macro: 0.591055 val_loss: 1.140461 val_acc: 59.639640 val_fscore_macro: 0.513726\n",
            "Epoch: 144/300 Train steps: 313 Val steps: 35 0.60s loss: 0.977900 acc: 66.666667 fscore_macro: 0.587997 val_loss: 1.167968 val_acc: 59.819820 val_fscore_macro: 0.519325\n",
            "Epoch: 145/300 Train steps: 313 Val steps: 35 0.61s loss: 0.975371 acc: 66.966607 fscore_macro: 0.592497 val_loss: 1.138206 val_acc: 60.180180 val_fscore_macro: 0.519136\n",
            "Epoch: 146/300 Train steps: 313 Val steps: 35 0.59s loss: 0.971556 acc: 67.006599 fscore_macro: 0.594676 val_loss: 1.139277 val_acc: 59.819820 val_fscore_macro: 0.513920\n",
            "Epoch: 147/300 Train steps: 313 Val steps: 35 0.59s loss: 0.970266 acc: 66.846631 fscore_macro: 0.596637 val_loss: 1.188098 val_acc: 57.297297 val_fscore_macro: 0.503096\n",
            "Epoch: 148/300 Train steps: 313 Val steps: 35 0.61s loss: 0.967677 acc: 66.866627 fscore_macro: 0.594320 val_loss: 1.140877 val_acc: 59.099099 val_fscore_macro: 0.508673\n",
            "Epoch: 149/300 Train steps: 313 Val steps: 35 0.60s loss: 0.968039 acc: 66.626675 fscore_macro: 0.596237 val_loss: 1.151481 val_acc: 57.837838 val_fscore_macro: 0.502574\n",
            "Epoch: 150/300 Train steps: 313 Val steps: 35 0.58s loss: 0.966141 acc: 66.646671 fscore_macro: 0.590475 val_loss: 1.127807 val_acc: 59.459459 val_fscore_macro: 0.519424\n",
            "Epoch: 151/300 Train steps: 313 Val steps: 35 0.60s loss: 0.964577 acc: 66.666667 fscore_macro: 0.600097 val_loss: 1.180070 val_acc: 58.378378 val_fscore_macro: 0.508879\n",
            "Epoch: 152/300 Train steps: 313 Val steps: 35 0.59s loss: 0.962664 acc: 66.866627 fscore_macro: 0.598261 val_loss: 1.144238 val_acc: 58.738739 val_fscore_macro: 0.500283\n",
            "Epoch: 153/300 Train steps: 313 Val steps: 35 0.67s loss: 0.959295 acc: 67.006599 fscore_macro: 0.601187 val_loss: 1.122935 val_acc: 59.819820 val_fscore_macro: 0.514076\n",
            "Epoch: 154/300 Train steps: 313 Val steps: 35 0.58s loss: 0.958174 acc: 66.806639 fscore_macro: 0.594980 val_loss: 1.150659 val_acc: 58.018018 val_fscore_macro: 0.499531\n",
            "Epoch: 155/300 Train steps: 313 Val steps: 35 0.59s loss: 0.955042 acc: 67.326535 fscore_macro: 0.601385 val_loss: 1.131854 val_acc: 58.378378 val_fscore_macro: 0.499502\n",
            "Epoch: 156/300 Train steps: 313 Val steps: 35 0.57s loss: 0.957621 acc: 67.206559 fscore_macro: 0.599941 val_loss: 1.162167 val_acc: 58.738739 val_fscore_macro: 0.512093\n",
            "Epoch: 157/300 Train steps: 313 Val steps: 35 0.59s loss: 0.956531 acc: 66.786643 fscore_macro: 0.603003 val_loss: 1.122239 val_acc: 59.819820 val_fscore_macro: 0.516891\n",
            "Epoch: 158/300 Train steps: 313 Val steps: 35 0.62s loss: 0.950414 acc: 67.286543 fscore_macro: 0.607846 val_loss: 1.119001 val_acc: 60.900901 val_fscore_macro: 0.514543\n",
            "Epoch: 159/300 Train steps: 313 Val steps: 35 0.60s loss: 0.951634 acc: 67.546491 fscore_macro: 0.606502 val_loss: 1.131129 val_acc: 60.900901 val_fscore_macro: 0.527102\n",
            "Epoch: 160/300 Train steps: 313 Val steps: 35 0.60s loss: 0.949832 acc: 67.026595 fscore_macro: 0.601960 val_loss: 1.138896 val_acc: 59.459459 val_fscore_macro: 0.503866\n",
            "Epoch: 161/300 Train steps: 313 Val steps: 35 0.58s loss: 0.947821 acc: 67.486503 fscore_macro: 0.604500 val_loss: 1.148432 val_acc: 60.000000 val_fscore_macro: 0.531796\n",
            "Epoch: 162/300 Train steps: 313 Val steps: 35 0.60s loss: 0.941453 acc: 67.846431 fscore_macro: 0.613130 val_loss: 1.115595 val_acc: 60.180180 val_fscore_macro: 0.511637\n",
            "Epoch: 163/300 Train steps: 313 Val steps: 35 0.58s loss: 0.941646 acc: 67.866427 fscore_macro: 0.613297 val_loss: 1.136998 val_acc: 60.900901 val_fscore_macro: 0.524223\n",
            "Epoch: 164/300 Train steps: 313 Val steps: 35 0.58s loss: 0.946345 acc: 67.406519 fscore_macro: 0.608148 val_loss: 1.148409 val_acc: 59.639640 val_fscore_macro: 0.504701\n",
            "Epoch: 165/300 Train steps: 313 Val steps: 35 0.57s loss: 0.941503 acc: 67.706459 fscore_macro: 0.611740 val_loss: 1.132892 val_acc: 61.621622 val_fscore_macro: 0.527453\n",
            "Epoch 165: val_acc improved from 61.26126 to 61.62162, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_165.ckpt\n",
            "Epoch: 166/300 Train steps: 313 Val steps: 35 0.59s loss: 0.941117 acc: 67.826435 fscore_macro: 0.614678 val_loss: 1.130402 val_acc: 61.621622 val_fscore_macro: 0.537569\n",
            "Epoch: 167/300 Train steps: 313 Val steps: 35 0.58s loss: 0.937093 acc: 67.866427 fscore_macro: 0.612631 val_loss: 1.148370 val_acc: 60.360360 val_fscore_macro: 0.537822\n",
            "Epoch: 168/300 Train steps: 313 Val steps: 35 0.59s loss: 0.933583 acc: 68.286343 fscore_macro: 0.616969 val_loss: 1.127966 val_acc: 62.162162 val_fscore_macro: 0.542077\n",
            "Epoch 168: val_acc improved from 61.62162 to 62.16216, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_168.ckpt\n",
            "Epoch: 169/300 Train steps: 313 Val steps: 35 0.60s loss: 0.935247 acc: 67.786443 fscore_macro: 0.616909 val_loss: 1.119878 val_acc: 60.540541 val_fscore_macro: 0.530042\n",
            "Epoch: 170/300 Train steps: 313 Val steps: 35 0.62s loss: 0.933487 acc: 67.986403 fscore_macro: 0.616684 val_loss: 1.170137 val_acc: 57.117117 val_fscore_macro: 0.491817\n",
            "Epoch: 171/300 Train steps: 313 Val steps: 35 0.63s loss: 0.931540 acc: 67.946411 fscore_macro: 0.618421 val_loss: 1.148129 val_acc: 59.639640 val_fscore_macro: 0.525158\n",
            "Epoch: 172/300 Train steps: 313 Val steps: 35 0.58s loss: 0.931415 acc: 68.186363 fscore_macro: 0.619139 val_loss: 1.112572 val_acc: 60.360360 val_fscore_macro: 0.522217\n",
            "Epoch: 173/300 Train steps: 313 Val steps: 35 0.61s loss: 0.929106 acc: 68.226355 fscore_macro: 0.621711 val_loss: 1.132337 val_acc: 58.738739 val_fscore_macro: 0.513151\n",
            "Epoch: 174/300 Train steps: 313 Val steps: 35 0.66s loss: 0.925264 acc: 68.186363 fscore_macro: 0.621875 val_loss: 1.150579 val_acc: 59.459459 val_fscore_macro: 0.508525\n",
            "Epoch: 175/300 Train steps: 313 Val steps: 35 0.62s loss: 0.926514 acc: 68.526295 fscore_macro: 0.621646 val_loss: 1.124357 val_acc: 60.900901 val_fscore_macro: 0.523181\n",
            "Epoch: 176/300 Train steps: 313 Val steps: 35 0.59s loss: 0.928391 acc: 68.046391 fscore_macro: 0.615037 val_loss: 1.115592 val_acc: 59.639640 val_fscore_macro: 0.540682\n",
            "Epoch: 177/300 Train steps: 313 Val steps: 35 0.61s loss: 0.922661 acc: 68.226355 fscore_macro: 0.625212 val_loss: 1.130943 val_acc: 58.558559 val_fscore_macro: 0.502918\n",
            "Epoch: 178/300 Train steps: 313 Val steps: 35 0.63s loss: 0.921582 acc: 68.306339 fscore_macro: 0.620507 val_loss: 1.125874 val_acc: 61.801802 val_fscore_macro: 0.536386\n",
            "Epoch: 179/300 Train steps: 313 Val steps: 35 0.62s loss: 0.920993 acc: 68.306339 fscore_macro: 0.624442 val_loss: 1.105721 val_acc: 61.441441 val_fscore_macro: 0.523772\n",
            "Epoch: 180/300 Train steps: 313 Val steps: 35 0.61s loss: 0.917832 acc: 68.326335 fscore_macro: 0.618989 val_loss: 1.139511 val_acc: 59.099099 val_fscore_macro: 0.534405\n",
            "Epoch: 181/300 Train steps: 313 Val steps: 35 0.62s loss: 0.919120 acc: 68.246351 fscore_macro: 0.617777 val_loss: 1.123957 val_acc: 59.279279 val_fscore_macro: 0.510309\n",
            "Epoch: 182/300 Train steps: 313 Val steps: 35 0.60s loss: 0.917445 acc: 68.466307 fscore_macro: 0.626868 val_loss: 1.108106 val_acc: 60.360360 val_fscore_macro: 0.520145\n",
            "Epoch: 183/300 Train steps: 313 Val steps: 35 0.58s loss: 0.913623 acc: 68.506299 fscore_macro: 0.633350 val_loss: 1.187943 val_acc: 58.018018 val_fscore_macro: 0.503006\n",
            "Epoch: 184/300 Train steps: 313 Val steps: 35 0.60s loss: 0.914375 acc: 68.066387 fscore_macro: 0.621819 val_loss: 1.119644 val_acc: 59.459460 val_fscore_macro: 0.523118\n",
            "Epoch: 185/300 Train steps: 313 Val steps: 35 0.60s loss: 0.910044 acc: 68.746251 fscore_macro: 0.627259 val_loss: 1.111009 val_acc: 61.261261 val_fscore_macro: 0.543918\n",
            "Epoch: 186/300 Train steps: 313 Val steps: 35 0.61s loss: 0.909209 acc: 68.586283 fscore_macro: 0.631242 val_loss: 1.128183 val_acc: 58.918919 val_fscore_macro: 0.529471\n",
            "Epoch: 187/300 Train steps: 313 Val steps: 35 0.60s loss: 0.909830 acc: 68.906219 fscore_macro: 0.629689 val_loss: 1.130129 val_acc: 58.198198 val_fscore_macro: 0.506674\n",
            "Epoch: 188/300 Train steps: 313 Val steps: 35 0.59s loss: 0.905177 acc: 69.086183 fscore_macro: 0.633956 val_loss: 1.134848 val_acc: 59.459460 val_fscore_macro: 0.528288\n",
            "Epoch: 189/300 Train steps: 313 Val steps: 35 0.60s loss: 0.905073 acc: 68.726255 fscore_macro: 0.634604 val_loss: 1.118680 val_acc: 61.081081 val_fscore_macro: 0.539308\n",
            "Epoch: 190/300 Train steps: 313 Val steps: 35 0.60s loss: 0.898701 acc: 69.466107 fscore_macro: 0.640253 val_loss: 1.143724 val_acc: 56.216216 val_fscore_macro: 0.501045\n",
            "Epoch: 191/300 Train steps: 313 Val steps: 35 0.61s loss: 0.902372 acc: 68.966207 fscore_macro: 0.634422 val_loss: 1.118079 val_acc: 60.540541 val_fscore_macro: 0.534088\n",
            "Epoch: 192/300 Train steps: 313 Val steps: 35 0.59s loss: 0.902896 acc: 68.666267 fscore_macro: 0.637132 val_loss: 1.117354 val_acc: 58.198198 val_fscore_macro: 0.512588\n",
            "Epoch: 193/300 Train steps: 313 Val steps: 35 0.59s loss: 0.899694 acc: 68.926215 fscore_macro: 0.636815 val_loss: 1.096558 val_acc: 60.360360 val_fscore_macro: 0.525284\n",
            "Epoch: 194/300 Train steps: 313 Val steps: 35 0.65s loss: 0.896424 acc: 69.126175 fscore_macro: 0.639639 val_loss: 1.108996 val_acc: 60.180180 val_fscore_macro: 0.524026\n",
            "Epoch: 195/300 Train steps: 313 Val steps: 35 0.59s loss: 0.896545 acc: 69.046191 fscore_macro: 0.639555 val_loss: 1.107571 val_acc: 62.522523 val_fscore_macro: 0.543997\n",
            "Epoch 195: val_acc improved from 62.16216 to 62.52252, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_195.ckpt\n",
            "Epoch: 196/300 Train steps: 313 Val steps: 35 0.57s loss: 0.892840 acc: 69.106179 fscore_macro: 0.639003 val_loss: 1.141259 val_acc: 58.378378 val_fscore_macro: 0.509682\n",
            "Epoch: 197/300 Train steps: 313 Val steps: 35 0.59s loss: 0.894740 acc: 68.906219 fscore_macro: 0.639246 val_loss: 1.124545 val_acc: 58.558559 val_fscore_macro: 0.523263\n",
            "Epoch: 198/300 Train steps: 313 Val steps: 35 0.60s loss: 0.892318 acc: 69.046191 fscore_macro: 0.637901 val_loss: 1.098722 val_acc: 62.162162 val_fscore_macro: 0.559900\n",
            "Epoch: 199/300 Train steps: 313 Val steps: 35 0.59s loss: 0.892096 acc: 68.826235 fscore_macro: 0.636343 val_loss: 1.134509 val_acc: 60.360360 val_fscore_macro: 0.531304\n",
            "Epoch: 200/300 Train steps: 313 Val steps: 35 0.59s loss: 0.892057 acc: 69.246151 fscore_macro: 0.642122 val_loss: 1.130848 val_acc: 59.279279 val_fscore_macro: 0.512720\n",
            "Epoch: 201/300 Train steps: 313 Val steps: 35 0.59s loss: 0.887197 acc: 69.706059 fscore_macro: 0.647629 val_loss: 1.102827 val_acc: 60.900901 val_fscore_macro: 0.525053\n",
            "Epoch: 202/300 Train steps: 313 Val steps: 35 0.60s loss: 0.886532 acc: 69.926015 fscore_macro: 0.651230 val_loss: 1.104845 val_acc: 60.900901 val_fscore_macro: 0.535267\n",
            "Epoch: 203/300 Train steps: 313 Val steps: 35 0.63s loss: 0.886253 acc: 69.146171 fscore_macro: 0.642059 val_loss: 1.100270 val_acc: 61.261261 val_fscore_macro: 0.542444\n",
            "Epoch: 204/300 Train steps: 313 Val steps: 35 0.60s loss: 0.882720 acc: 69.446111 fscore_macro: 0.646576 val_loss: 1.105962 val_acc: 60.360360 val_fscore_macro: 0.523636\n",
            "Epoch: 205/300 Train steps: 313 Val steps: 35 0.59s loss: 0.882394 acc: 69.326135 fscore_macro: 0.644233 val_loss: 1.113130 val_acc: 62.522523 val_fscore_macro: 0.549210\n",
            "Epoch 205: val_acc improved from 62.52252 to 62.52252, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_205.ckpt\n",
            "Epoch: 206/300 Train steps: 313 Val steps: 35 0.59s loss: 0.881448 acc: 69.706059 fscore_macro: 0.649033 val_loss: 1.121136 val_acc: 62.702703 val_fscore_macro: 0.562525\n",
            "Epoch 206: val_acc improved from 62.52252 to 62.70270, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_206.ckpt\n",
            "Epoch: 207/300 Train steps: 313 Val steps: 35 0.60s loss: 0.880642 acc: 69.906019 fscore_macro: 0.656629 val_loss: 1.105318 val_acc: 58.738739 val_fscore_macro: 0.517996\n",
            "Epoch: 208/300 Train steps: 313 Val steps: 35 0.60s loss: 0.881033 acc: 68.966207 fscore_macro: 0.644935 val_loss: 1.103417 val_acc: 60.000000 val_fscore_macro: 0.520880\n",
            "Epoch: 209/300 Train steps: 313 Val steps: 35 0.62s loss: 0.875759 acc: 69.726055 fscore_macro: 0.654330 val_loss: 1.133660 val_acc: 60.360360 val_fscore_macro: 0.535694\n",
            "Epoch: 210/300 Train steps: 313 Val steps: 35 0.60s loss: 0.879533 acc: 69.486103 fscore_macro: 0.651626 val_loss: 1.100912 val_acc: 62.342342 val_fscore_macro: 0.541309\n",
            "Epoch: 211/300 Train steps: 313 Val steps: 35 0.60s loss: 0.874095 acc: 70.165967 fscore_macro: 0.648315 val_loss: 1.121907 val_acc: 60.900901 val_fscore_macro: 0.574063\n",
            "Epoch: 212/300 Train steps: 313 Val steps: 35 0.59s loss: 0.876544 acc: 69.946011 fscore_macro: 0.655562 val_loss: 1.109474 val_acc: 60.180180 val_fscore_macro: 0.530199\n",
            "Epoch: 213/300 Train steps: 313 Val steps: 35 0.60s loss: 0.871293 acc: 69.946011 fscore_macro: 0.649350 val_loss: 1.108565 val_acc: 60.900901 val_fscore_macro: 0.546646\n",
            "Epoch: 214/300 Train steps: 313 Val steps: 35 0.59s loss: 0.872112 acc: 70.065987 fscore_macro: 0.652023 val_loss: 1.115089 val_acc: 60.720721 val_fscore_macro: 0.565620\n",
            "Epoch: 215/300 Train steps: 313 Val steps: 35 0.66s loss: 0.869770 acc: 70.105979 fscore_macro: 0.652054 val_loss: 1.133431 val_acc: 59.279279 val_fscore_macro: 0.520492\n",
            "Epoch: 216/300 Train steps: 313 Val steps: 35 0.65s loss: 0.871363 acc: 69.326135 fscore_macro: 0.645979 val_loss: 1.122658 val_acc: 60.180180 val_fscore_macro: 0.529960\n",
            "Epoch: 217/300 Train steps: 313 Val steps: 35 0.62s loss: 0.866901 acc: 69.866027 fscore_macro: 0.654094 val_loss: 1.094543 val_acc: 60.900901 val_fscore_macro: 0.533403\n",
            "Epoch: 218/300 Train steps: 313 Val steps: 35 0.64s loss: 0.864637 acc: 70.225955 fscore_macro: 0.662072 val_loss: 1.105248 val_acc: 59.459460 val_fscore_macro: 0.522926\n",
            "Epoch: 219/300 Train steps: 313 Val steps: 35 0.69s loss: 0.862537 acc: 70.445911 fscore_macro: 0.658054 val_loss: 1.121681 val_acc: 60.360360 val_fscore_macro: 0.516952\n",
            "Epoch: 220/300 Train steps: 313 Val steps: 35 0.64s loss: 0.865690 acc: 69.586083 fscore_macro: 0.659996 val_loss: 1.129190 val_acc: 61.801802 val_fscore_macro: 0.548851\n",
            "Epoch: 221/300 Train steps: 313 Val steps: 35 0.67s loss: 0.863303 acc: 70.385923 fscore_macro: 0.661911 val_loss: 1.158883 val_acc: 58.018018 val_fscore_macro: 0.518403\n",
            "Epoch: 222/300 Train steps: 313 Val steps: 35 0.60s loss: 0.858088 acc: 70.445911 fscore_macro: 0.663124 val_loss: 1.155112 val_acc: 58.918919 val_fscore_macro: 0.509957\n",
            "Epoch: 223/300 Train steps: 313 Val steps: 35 0.63s loss: 0.860604 acc: 69.926015 fscore_macro: 0.656585 val_loss: 1.092239 val_acc: 62.882883 val_fscore_macro: 0.560067\n",
            "Epoch 223: val_acc improved from 62.70270 to 62.88288, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_223.ckpt\n",
            "Epoch: 224/300 Train steps: 313 Val steps: 35 0.59s loss: 0.856573 acc: 70.025995 fscore_macro: 0.652217 val_loss: 1.104859 val_acc: 61.801802 val_fscore_macro: 0.553099\n",
            "Epoch: 225/300 Train steps: 313 Val steps: 35 0.60s loss: 0.857274 acc: 70.445911 fscore_macro: 0.666114 val_loss: 1.105788 val_acc: 60.180180 val_fscore_macro: 0.531005\n",
            "Epoch: 226/300 Train steps: 313 Val steps: 35 0.78s loss: 0.852808 acc: 70.825835 fscore_macro: 0.666226 val_loss: 1.111132 val_acc: 61.801802 val_fscore_macro: 0.541926\n",
            "Epoch: 227/300 Train steps: 313 Val steps: 35 0.79s loss: 0.855620 acc: 70.105979 fscore_macro: 0.660423 val_loss: 1.109957 val_acc: 61.981982 val_fscore_macro: 0.561180\n",
            "Epoch: 228/300 Train steps: 313 Val steps: 35 0.83s loss: 0.852315 acc: 70.725855 fscore_macro: 0.667981 val_loss: 1.105070 val_acc: 61.441441 val_fscore_macro: 0.543147\n",
            "Epoch: 229/300 Train steps: 313 Val steps: 35 0.83s loss: 0.852211 acc: 70.325935 fscore_macro: 0.660472 val_loss: 1.126897 val_acc: 60.000000 val_fscore_macro: 0.549203\n",
            "Epoch: 230/300 Train steps: 313 Val steps: 35 0.86s loss: 0.849945 acc: 70.765847 fscore_macro: 0.664176 val_loss: 1.145298 val_acc: 61.441441 val_fscore_macro: 0.540415\n",
            "Epoch: 231/300 Train steps: 313 Val steps: 35 0.72s loss: 0.846825 acc: 70.445911 fscore_macro: 0.668513 val_loss: 1.146469 val_acc: 59.819820 val_fscore_macro: 0.545961\n",
            "Epoch: 232/300 Train steps: 313 Val steps: 35 0.61s loss: 0.849156 acc: 70.645871 fscore_macro: 0.664490 val_loss: 1.101528 val_acc: 60.900901 val_fscore_macro: 0.551225\n",
            "Epoch: 233/300 Train steps: 313 Val steps: 35 0.63s loss: 0.849056 acc: 70.365927 fscore_macro: 0.661526 val_loss: 1.095490 val_acc: 61.441441 val_fscore_macro: 0.565994\n",
            "Epoch: 234/300 Train steps: 313 Val steps: 35 0.61s loss: 0.845558 acc: 71.165767 fscore_macro: 0.669107 val_loss: 1.116362 val_acc: 59.099099 val_fscore_macro: 0.530184\n",
            "Epoch: 235/300 Train steps: 313 Val steps: 35 0.58s loss: 0.844552 acc: 70.765847 fscore_macro: 0.663036 val_loss: 1.101043 val_acc: 63.063063 val_fscore_macro: 0.553455\n",
            "Epoch 235: val_acc improved from 62.88288 to 63.06306, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_235.ckpt\n",
            "Epoch: 236/300 Train steps: 313 Val steps: 35 0.60s loss: 0.842872 acc: 70.785843 fscore_macro: 0.666697 val_loss: 1.116942 val_acc: 61.261261 val_fscore_macro: 0.531914\n",
            "Epoch: 237/300 Train steps: 313 Val steps: 35 0.61s loss: 0.840845 acc: 71.245751 fscore_macro: 0.670907 val_loss: 1.094330 val_acc: 63.063063 val_fscore_macro: 0.575816\n",
            "Epoch 237: val_acc improved from 63.06306 to 63.06306, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_237.ckpt\n",
            "Epoch: 238/300 Train steps: 313 Val steps: 35 0.59s loss: 0.842205 acc: 71.405719 fscore_macro: 0.679889 val_loss: 1.144928 val_acc: 60.720721 val_fscore_macro: 0.558031\n",
            "Epoch: 239/300 Train steps: 313 Val steps: 35 0.60s loss: 0.840365 acc: 70.705859 fscore_macro: 0.673216 val_loss: 1.096798 val_acc: 62.162162 val_fscore_macro: 0.559651\n",
            "Epoch: 240/300 Train steps: 313 Val steps: 35 0.70s loss: 0.838425 acc: 70.925815 fscore_macro: 0.673896 val_loss: 1.128092 val_acc: 61.801802 val_fscore_macro: 0.555532\n",
            "Epoch: 241/300 Train steps: 313 Val steps: 35 0.58s loss: 0.839024 acc: 71.205759 fscore_macro: 0.664618 val_loss: 1.115611 val_acc: 61.981982 val_fscore_macro: 0.560034\n",
            "Epoch: 242/300 Train steps: 313 Val steps: 35 0.62s loss: 0.836976 acc: 71.125775 fscore_macro: 0.676533 val_loss: 1.106443 val_acc: 60.360360 val_fscore_macro: 0.542210\n",
            "Epoch: 243/300 Train steps: 313 Val steps: 35 0.58s loss: 0.835526 acc: 70.865827 fscore_macro: 0.669063 val_loss: 1.126790 val_acc: 61.081081 val_fscore_macro: 0.542874\n",
            "Epoch: 244/300 Train steps: 313 Val steps: 35 0.60s loss: 0.830894 acc: 71.445711 fscore_macro: 0.673743 val_loss: 1.097665 val_acc: 61.441441 val_fscore_macro: 0.573681\n",
            "Epoch: 245/300 Train steps: 313 Val steps: 35 0.59s loss: 0.830034 acc: 71.385723 fscore_macro: 0.676914 val_loss: 1.092502 val_acc: 60.900901 val_fscore_macro: 0.555412\n",
            "Epoch: 246/300 Train steps: 313 Val steps: 35 0.61s loss: 0.833804 acc: 71.305739 fscore_macro: 0.672179 val_loss: 1.088504 val_acc: 63.963964 val_fscore_macro: 0.591149\n",
            "Epoch 246: val_acc improved from 63.06306 to 63.96396, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_246.ckpt\n",
            "Epoch: 247/300 Train steps: 313 Val steps: 35 0.59s loss: 0.832077 acc: 71.045791 fscore_macro: 0.673450 val_loss: 1.106151 val_acc: 61.981982 val_fscore_macro: 0.558618\n",
            "Epoch: 248/300 Train steps: 313 Val steps: 35 0.62s loss: 0.829417 acc: 71.185763 fscore_macro: 0.672053 val_loss: 1.123079 val_acc: 62.162162 val_fscore_macro: 0.583724\n",
            "Epoch: 249/300 Train steps: 313 Val steps: 35 0.58s loss: 0.826050 acc: 71.565687 fscore_macro: 0.681102 val_loss: 1.107458 val_acc: 59.459459 val_fscore_macro: 0.523010\n",
            "Epoch: 250/300 Train steps: 313 Val steps: 35 0.61s loss: 0.821613 acc: 71.265747 fscore_macro: 0.675560 val_loss: 1.132605 val_acc: 61.621622 val_fscore_macro: 0.543870\n",
            "Epoch: 251/300 Train steps: 313 Val steps: 35 0.60s loss: 0.825368 acc: 71.665667 fscore_macro: 0.678247 val_loss: 1.100842 val_acc: 64.144144 val_fscore_macro: 0.590519\n",
            "Epoch 251: val_acc improved from 63.96396 to 64.14414, saving file to model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_251.ckpt\n",
            "Epoch: 252/300 Train steps: 313 Val steps: 35 0.59s loss: 0.822661 acc: 71.845631 fscore_macro: 0.678352 val_loss: 1.088928 val_acc: 62.882883 val_fscore_macro: 0.589823\n",
            "Epoch: 253/300 Train steps: 313 Val steps: 35 0.64s loss: 0.821771 acc: 71.685663 fscore_macro: 0.681121 val_loss: 1.106244 val_acc: 60.180180 val_fscore_macro: 0.532459\n",
            "Epoch: 254/300 Train steps: 313 Val steps: 35 0.61s loss: 0.820118 acc: 71.945611 fscore_macro: 0.681897 val_loss: 1.102703 val_acc: 60.360360 val_fscore_macro: 0.538885\n",
            "Epoch: 255/300 Train steps: 313 Val steps: 35 0.58s loss: 0.819959 acc: 71.565687 fscore_macro: 0.682009 val_loss: 1.095037 val_acc: 62.342342 val_fscore_macro: 0.571476\n",
            "Epoch: 256/300 Train steps: 313 Val steps: 35 0.61s loss: 0.817985 acc: 71.385723 fscore_macro: 0.677461 val_loss: 1.115806 val_acc: 60.720721 val_fscore_macro: 0.574506\n",
            "Epoch: 257/300 Train steps: 313 Val steps: 35 0.62s loss: 0.817310 acc: 71.485703 fscore_macro: 0.681417 val_loss: 1.093725 val_acc: 61.981982 val_fscore_macro: 0.581399\n",
            "Epoch: 258/300 Train steps: 313 Val steps: 35 0.60s loss: 0.816156 acc: 71.665667 fscore_macro: 0.682839 val_loss: 1.134646 val_acc: 61.441441 val_fscore_macro: 0.536716\n",
            "Epoch: 259/300 Train steps: 313 Val steps: 35 0.63s loss: 0.817862 acc: 71.145771 fscore_macro: 0.679337 val_loss: 1.123496 val_acc: 61.081081 val_fscore_macro: 0.553097\n",
            "Epoch: 260/300 Train steps: 313 Val steps: 35 0.63s loss: 0.814333 acc: 72.345531 fscore_macro: 0.687681 val_loss: 1.137048 val_acc: 59.279279 val_fscore_macro: 0.510798\n",
            "Epoch: 261/300 Train steps: 313 Val steps: 35 0.63s loss: 0.812814 acc: 71.945611 fscore_macro: 0.687212 val_loss: 1.105082 val_acc: 61.441441 val_fscore_macro: 0.557750\n",
            "Epoch: 262/300 Train steps: 313 Val steps: 35 0.61s loss: 0.815573 acc: 71.725655 fscore_macro: 0.683994 val_loss: 1.102318 val_acc: 63.063063 val_fscore_macro: 0.589620\n",
            "Epoch: 263/300 Train steps: 313 Val steps: 35 0.64s loss: 0.810539 acc: 71.705659 fscore_macro: 0.682798 val_loss: 1.099182 val_acc: 61.801802 val_fscore_macro: 0.560657\n",
            "Epoch: 264/300 Train steps: 313 Val steps: 35 0.60s loss: 0.809116 acc: 71.745651 fscore_macro: 0.687348 val_loss: 1.136314 val_acc: 59.819820 val_fscore_macro: 0.522558\n",
            "Epoch: 265/300 Train steps: 313 Val steps: 35 0.59s loss: 0.806085 acc: 72.045591 fscore_macro: 0.685413 val_loss: 1.114288 val_acc: 63.423423 val_fscore_macro: 0.585290\n",
            "Epoch: 266/300 Train steps: 313 Val steps: 35 0.60s loss: 0.807759 acc: 72.285543 fscore_macro: 0.687127 val_loss: 1.103677 val_acc: 60.900901 val_fscore_macro: 0.539839\n",
            "Epoch: 267/300 Train steps: 313 Val steps: 35 0.60s loss: 0.807041 acc: 71.865627 fscore_macro: 0.683902 val_loss: 1.111979 val_acc: 61.981982 val_fscore_macro: 0.577919\n",
            "Epoch: 268/300 Train steps: 313 Val steps: 35 0.62s loss: 0.802121 acc: 72.125575 fscore_macro: 0.691417 val_loss: 1.110959 val_acc: 62.162162 val_fscore_macro: 0.580593\n",
            "Epoch: 269/300 Train steps: 313 Val steps: 35 0.61s loss: 0.804169 acc: 72.025595 fscore_macro: 0.690695 val_loss: 1.111964 val_acc: 61.261261 val_fscore_macro: 0.549649\n",
            "Epoch: 270/300 Train steps: 313 Val steps: 35 0.64s loss: 0.803069 acc: 72.145571 fscore_macro: 0.691461 val_loss: 1.108473 val_acc: 61.261261 val_fscore_macro: 0.556128\n",
            "Epoch: 271/300 Train steps: 313 Val steps: 35 0.59s loss: 0.801158 acc: 72.445511 fscore_macro: 0.691372 val_loss: 1.094395 val_acc: 60.000000 val_fscore_macro: 0.534166\n",
            "Epoch: 272/300 Train steps: 313 Val steps: 35 0.62s loss: 0.799174 acc: 72.465507 fscore_macro: 0.692362 val_loss: 1.129550 val_acc: 61.621622 val_fscore_macro: 0.568908\n",
            "Epoch: 273/300 Train steps: 313 Val steps: 35 0.60s loss: 0.801395 acc: 72.485503 fscore_macro: 0.699002 val_loss: 1.108611 val_acc: 58.918919 val_fscore_macro: 0.511030\n",
            "Epoch: 274/300 Train steps: 313 Val steps: 35 0.62s loss: 0.796194 acc: 72.505499 fscore_macro: 0.692410 val_loss: 1.122835 val_acc: 59.639640 val_fscore_macro: 0.556820\n",
            "Epoch: 275/300 Train steps: 313 Val steps: 35 0.62s loss: 0.797354 acc: 72.665467 fscore_macro: 0.693515 val_loss: 1.085258 val_acc: 61.261261 val_fscore_macro: 0.567593\n",
            "Epoch: 276/300 Train steps: 313 Val steps: 35 0.59s loss: 0.795426 acc: 72.525495 fscore_macro: 0.694855 val_loss: 1.132396 val_acc: 62.522523 val_fscore_macro: 0.583535\n",
            "Epoch: 277/300 Train steps: 313 Val steps: 35 0.60s loss: 0.795375 acc: 72.345531 fscore_macro: 0.691166 val_loss: 1.110386 val_acc: 61.261261 val_fscore_macro: 0.534582\n",
            "Epoch: 278/300 Train steps: 313 Val steps: 35 0.61s loss: 0.791158 acc: 72.725455 fscore_macro: 0.695465 val_loss: 1.170138 val_acc: 60.000000 val_fscore_macro: 0.562851\n",
            "Epoch: 279/300 Train steps: 313 Val steps: 35 0.59s loss: 0.791705 acc: 72.265547 fscore_macro: 0.694256 val_loss: 1.120356 val_acc: 60.180180 val_fscore_macro: 0.564713\n",
            "Epoch: 280/300 Train steps: 313 Val steps: 35 0.58s loss: 0.791666 acc: 72.345531 fscore_macro: 0.697054 val_loss: 1.123640 val_acc: 60.360360 val_fscore_macro: 0.557341\n",
            "Epoch: 281/300 Train steps: 313 Val steps: 35 0.64s loss: 0.789400 acc: 72.405519 fscore_macro: 0.694980 val_loss: 1.082742 val_acc: 62.702703 val_fscore_macro: 0.585207\n",
            "Epoch: 282/300 Train steps: 313 Val steps: 35 0.59s loss: 0.787970 acc: 72.665467 fscore_macro: 0.697810 val_loss: 1.161832 val_acc: 58.558559 val_fscore_macro: 0.552465\n",
            "Epoch: 283/300 Train steps: 313 Val steps: 35 0.59s loss: 0.789486 acc: 72.885423 fscore_macro: 0.698320 val_loss: 1.094225 val_acc: 61.261261 val_fscore_macro: 0.579338\n",
            "Epoch: 284/300 Train steps: 313 Val steps: 35 0.64s loss: 0.787126 acc: 72.805439 fscore_macro: 0.696769 val_loss: 1.113390 val_acc: 62.162162 val_fscore_macro: 0.585047\n",
            "Epoch: 285/300 Train steps: 313 Val steps: 35 0.59s loss: 0.785054 acc: 72.705459 fscore_macro: 0.703191 val_loss: 1.123474 val_acc: 60.360360 val_fscore_macro: 0.562240\n",
            "Epoch: 286/300 Train steps: 313 Val steps: 35 0.59s loss: 0.782374 acc: 72.805439 fscore_macro: 0.700781 val_loss: 1.109387 val_acc: 60.900901 val_fscore_macro: 0.534015\n",
            "Epoch: 287/300 Train steps: 313 Val steps: 35 0.61s loss: 0.776423 acc: 73.205359 fscore_macro: 0.703767 val_loss: 1.089615 val_acc: 62.342342 val_fscore_macro: 0.593085\n",
            "Epoch: 288/300 Train steps: 313 Val steps: 35 0.60s loss: 0.781424 acc: 72.865427 fscore_macro: 0.697363 val_loss: 1.119042 val_acc: 61.621622 val_fscore_macro: 0.577336\n",
            "Epoch: 289/300 Train steps: 313 Val steps: 35 0.63s loss: 0.779845 acc: 72.845431 fscore_macro: 0.702360 val_loss: 1.115869 val_acc: 62.882883 val_fscore_macro: 0.562795\n",
            "Epoch: 290/300 Train steps: 313 Val steps: 35 0.61s loss: 0.779364 acc: 72.685463 fscore_macro: 0.705610 val_loss: 1.126617 val_acc: 60.900901 val_fscore_macro: 0.576314\n",
            "Epoch: 291/300 Train steps: 313 Val steps: 35 0.59s loss: 0.779817 acc: 73.025395 fscore_macro: 0.702423 val_loss: 1.096769 val_acc: 63.603604 val_fscore_macro: 0.593135\n",
            "Epoch: 292/300 Train steps: 313 Val steps: 35 0.62s loss: 0.776130 acc: 72.785443 fscore_macro: 0.702502 val_loss: 1.112992 val_acc: 62.342342 val_fscore_macro: 0.593432\n",
            "Epoch: 293/300 Train steps: 313 Val steps: 35 0.60s loss: 0.774726 acc: 73.185363 fscore_macro: 0.707556 val_loss: 1.144857 val_acc: 60.540541 val_fscore_macro: 0.552118\n",
            "Epoch: 294/300 Train steps: 313 Val steps: 35 0.62s loss: 0.771603 acc: 73.305339 fscore_macro: 0.704880 val_loss: 1.118687 val_acc: 61.981982 val_fscore_macro: 0.573190\n",
            "Epoch: 295/300 Train steps: 313 Val steps: 35 0.62s loss: 0.770573 acc: 73.345331 fscore_macro: 0.704698 val_loss: 1.106586 val_acc: 63.063063 val_fscore_macro: 0.584948\n",
            "Epoch: 296/300 Train steps: 313 Val steps: 35 0.63s loss: 0.775872 acc: 72.945411 fscore_macro: 0.708973 val_loss: 1.121051 val_acc: 60.540541 val_fscore_macro: 0.545569\n",
            "Epoch: 297/300 Train steps: 313 Val steps: 35 0.60s loss: 0.769431 acc: 73.425315 fscore_macro: 0.704592 val_loss: 1.108414 val_acc: 60.540541 val_fscore_macro: 0.580999\n",
            "Epoch: 298/300 Train steps: 313 Val steps: 35 0.61s loss: 0.768415 acc: 73.265347 fscore_macro: 0.704463 val_loss: 1.114053 val_acc: 62.162162 val_fscore_macro: 0.568833\n",
            "Epoch: 299/300 Train steps: 313 Val steps: 35 0.61s loss: 0.767245 acc: 73.645271 fscore_macro: 0.711804 val_loss: 1.128461 val_acc: 59.099099 val_fscore_macro: 0.526263\n",
            "Epoch: 300/300 Train steps: 313 Val steps: 35 0.58s loss: 0.768601 acc: 73.445311 fscore_macro: 0.707044 val_loss: 1.149965 val_acc: 60.000000 val_fscore_macro: 0.536055\n",
            "Restoring data from model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_251.ckpt\n",
            "Found best checkpoint at epoch: 251\n",
            "lr: 0.01, loss: 0.825368, acc: 71.6657, fscore_macro: 0.678247, val_loss: 1.10084, val_acc: 64.1441, val_fscore_macro: 0.590519\n",
            "Loading checkpoint model/fasttext/maxpool_embedding_mlp_200/checkpoint_epoch_251.ckpt\n",
            "Running test\n",
            "Test steps: 32 1.32s test_loss: 0.747717 test_acc: 75.200000 test_fscore_macro: 0.672974    \n",
            "\n",
            " Taille de couche cachée égale à : 300, avec les embeddings préentraînés de fasttext aggrégés avec la méthode average :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Epoch:   1/300 Train steps: 313 Val steps: 35 40.74s loss: 2.122749 acc: 22.275545 fscore_macro: 0.049345 val_loss: 2.083845 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch 1: val_acc improved from -inf to 20.72072, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_1.ckpt\n",
            "Epoch:   2/300 Train steps: 313 Val steps: 35 0.66s loss: 2.035578 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.040701 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   3/300 Train steps: 313 Val steps: 35 0.66s loss: 2.000831 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.024387 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   4/300 Train steps: 313 Val steps: 35 0.65s loss: 1.985524 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.016339 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   5/300 Train steps: 313 Val steps: 35 0.64s loss: 1.976166 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.010713 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   6/300 Train steps: 313 Val steps: 35 0.65s loss: 1.968347 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.005271 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   7/300 Train steps: 313 Val steps: 35 0.67s loss: 1.961112 acc: 22.695461 fscore_macro: 0.041105 val_loss: 1.998445 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   8/300 Train steps: 313 Val steps: 35 0.65s loss: 1.953058 acc: 22.695461 fscore_macro: 0.041105 val_loss: 1.991733 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   9/300 Train steps: 313 Val steps: 35 0.65s loss: 1.943921 acc: 22.695461 fscore_macro: 0.041105 val_loss: 1.983124 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:  10/300 Train steps: 313 Val steps: 35 0.67s loss: 1.933421 acc: 22.775445 fscore_macro: 0.042216 val_loss: 1.972942 val_acc: 20.900901 val_fscore_macro: 0.040539\n",
            "Epoch 10: val_acc improved from 20.72072 to 20.90090, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_10.ckpt\n",
            "Epoch:  11/300 Train steps: 313 Val steps: 35 0.64s loss: 1.921622 acc: 23.115377 fscore_macro: 0.047875 val_loss: 1.961379 val_acc: 22.702703 val_fscore_macro: 0.074723\n",
            "Epoch 11: val_acc improved from 20.90090 to 22.70270, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_11.ckpt\n",
            "Epoch:  12/300 Train steps: 313 Val steps: 35 0.67s loss: 1.908512 acc: 25.534893 fscore_macro: 0.089185 val_loss: 1.950115 val_acc: 23.243243 val_fscore_macro: 0.081435\n",
            "Epoch 12: val_acc improved from 22.70270 to 23.24324, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_12.ckpt\n",
            "Epoch:  13/300 Train steps: 313 Val steps: 35 0.65s loss: 1.894071 acc: 28.114377 fscore_macro: 0.124112 val_loss: 1.936752 val_acc: 25.585586 val_fscore_macro: 0.116013\n",
            "Epoch 13: val_acc improved from 23.24324 to 25.58559, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_13.ckpt\n",
            "Epoch:  14/300 Train steps: 313 Val steps: 35 0.66s loss: 1.878274 acc: 29.614077 fscore_macro: 0.144177 val_loss: 1.921178 val_acc: 30.630631 val_fscore_macro: 0.168934\n",
            "Epoch 14: val_acc improved from 25.58559 to 30.63063, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_14.ckpt\n",
            "Epoch:  15/300 Train steps: 313 Val steps: 35 0.65s loss: 1.862512 acc: 33.433313 fscore_macro: 0.184452 val_loss: 1.907156 val_acc: 31.711712 val_fscore_macro: 0.182198\n",
            "Epoch 15: val_acc improved from 30.63063 to 31.71171, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_15.ckpt\n",
            "Epoch:  16/300 Train steps: 313 Val steps: 35 0.68s loss: 1.845595 acc: 35.912817 fscore_macro: 0.202667 val_loss: 1.892120 val_acc: 31.171171 val_fscore_macro: 0.178627\n",
            "Epoch:  17/300 Train steps: 313 Val steps: 35 0.66s loss: 1.828567 acc: 36.372725 fscore_macro: 0.206620 val_loss: 1.876780 val_acc: 31.711712 val_fscore_macro: 0.181873\n",
            "Epoch 17: val_acc improved from 31.71171 to 31.71171, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_17.ckpt\n",
            "Epoch:  18/300 Train steps: 313 Val steps: 35 0.67s loss: 1.811121 acc: 36.792641 fscore_macro: 0.210690 val_loss: 1.860858 val_acc: 35.135135 val_fscore_macro: 0.204690\n",
            "Epoch 18: val_acc improved from 31.71171 to 35.13514, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_18.ckpt\n",
            "Epoch:  19/300 Train steps: 313 Val steps: 35 0.65s loss: 1.794482 acc: 37.352529 fscore_macro: 0.214157 val_loss: 1.847424 val_acc: 35.855856 val_fscore_macro: 0.204128\n",
            "Epoch 19: val_acc improved from 35.13514 to 35.85586, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_19.ckpt\n",
            "Epoch:  20/300 Train steps: 313 Val steps: 35 0.65s loss: 1.778410 acc: 37.892422 fscore_macro: 0.216075 val_loss: 1.830935 val_acc: 35.855856 val_fscore_macro: 0.209108\n",
            "Epoch:  21/300 Train steps: 313 Val steps: 35 0.67s loss: 1.762169 acc: 38.032394 fscore_macro: 0.217844 val_loss: 1.816010 val_acc: 36.756757 val_fscore_macro: 0.209963\n",
            "Epoch 21: val_acc improved from 35.85586 to 36.75676, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_21.ckpt\n",
            "Epoch:  22/300 Train steps: 313 Val steps: 35 0.66s loss: 1.746769 acc: 38.392322 fscore_macro: 0.218518 val_loss: 1.801551 val_acc: 36.036036 val_fscore_macro: 0.208415\n",
            "Epoch:  23/300 Train steps: 313 Val steps: 35 0.66s loss: 1.731119 acc: 38.272346 fscore_macro: 0.218896 val_loss: 1.789895 val_acc: 36.216216 val_fscore_macro: 0.205631\n",
            "Epoch:  24/300 Train steps: 313 Val steps: 35 0.65s loss: 1.716843 acc: 38.392322 fscore_macro: 0.219516 val_loss: 1.774639 val_acc: 36.576577 val_fscore_macro: 0.212365\n",
            "Epoch:  25/300 Train steps: 313 Val steps: 35 0.67s loss: 1.702330 acc: 38.892222 fscore_macro: 0.222180 val_loss: 1.763688 val_acc: 37.297297 val_fscore_macro: 0.212733\n",
            "Epoch 25: val_acc improved from 36.75676 to 37.29730, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_25.ckpt\n",
            "Epoch:  26/300 Train steps: 313 Val steps: 35 0.69s loss: 1.688573 acc: 39.072186 fscore_macro: 0.223189 val_loss: 1.748204 val_acc: 36.036036 val_fscore_macro: 0.207592\n",
            "Epoch:  27/300 Train steps: 313 Val steps: 35 0.66s loss: 1.675473 acc: 39.112178 fscore_macro: 0.225375 val_loss: 1.735361 val_acc: 37.477477 val_fscore_macro: 0.214449\n",
            "Epoch 27: val_acc improved from 37.29730 to 37.47748, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_27.ckpt\n",
            "Epoch:  28/300 Train steps: 313 Val steps: 35 0.66s loss: 1.662370 acc: 39.592082 fscore_macro: 0.231322 val_loss: 1.723185 val_acc: 38.018018 val_fscore_macro: 0.220761\n",
            "Epoch 28: val_acc improved from 37.47748 to 38.01802, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_28.ckpt\n",
            "Epoch:  29/300 Train steps: 313 Val steps: 35 0.67s loss: 1.650042 acc: 39.892022 fscore_macro: 0.233688 val_loss: 1.710860 val_acc: 38.558559 val_fscore_macro: 0.234362\n",
            "Epoch 29: val_acc improved from 38.01802 to 38.55856, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_29.ckpt\n",
            "Epoch:  30/300 Train steps: 313 Val steps: 35 0.64s loss: 1.637078 acc: 40.791842 fscore_macro: 0.248615 val_loss: 1.700837 val_acc: 38.198198 val_fscore_macro: 0.225536\n",
            "Epoch:  31/300 Train steps: 313 Val steps: 35 0.65s loss: 1.626079 acc: 40.871826 fscore_macro: 0.248038 val_loss: 1.687809 val_acc: 40.000000 val_fscore_macro: 0.247729\n",
            "Epoch 31: val_acc improved from 38.55856 to 40.00000, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_31.ckpt\n",
            "Epoch:  32/300 Train steps: 313 Val steps: 35 0.66s loss: 1.614260 acc: 42.411518 fscore_macro: 0.266740 val_loss: 1.675857 val_acc: 39.459459 val_fscore_macro: 0.249911\n",
            "Epoch:  33/300 Train steps: 313 Val steps: 35 0.66s loss: 1.603407 acc: 42.491502 fscore_macro: 0.271036 val_loss: 1.664532 val_acc: 40.900901 val_fscore_macro: 0.269189\n",
            "Epoch 33: val_acc improved from 40.00000 to 40.90090, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_33.ckpt\n",
            "Epoch:  34/300 Train steps: 313 Val steps: 35 0.66s loss: 1.592595 acc: 43.151370 fscore_macro: 0.279970 val_loss: 1.653881 val_acc: 42.702703 val_fscore_macro: 0.286530\n",
            "Epoch 34: val_acc improved from 40.90090 to 42.70270, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_34.ckpt\n",
            "Epoch:  35/300 Train steps: 313 Val steps: 35 0.66s loss: 1.581500 acc: 44.251150 fscore_macro: 0.294834 val_loss: 1.643138 val_acc: 42.702703 val_fscore_macro: 0.284757\n",
            "Epoch:  36/300 Train steps: 313 Val steps: 35 0.74s loss: 1.571078 acc: 44.851030 fscore_macro: 0.296743 val_loss: 1.633475 val_acc: 42.522523 val_fscore_macro: 0.289129\n",
            "Epoch:  37/300 Train steps: 313 Val steps: 35 0.66s loss: 1.560646 acc: 45.530894 fscore_macro: 0.306509 val_loss: 1.622733 val_acc: 44.684685 val_fscore_macro: 0.303919\n",
            "Epoch 37: val_acc improved from 42.70270 to 44.68468, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_37.ckpt\n",
            "Epoch:  38/300 Train steps: 313 Val steps: 35 0.64s loss: 1.550329 acc: 46.370726 fscore_macro: 0.316357 val_loss: 1.611910 val_acc: 46.126126 val_fscore_macro: 0.318541\n",
            "Epoch 38: val_acc improved from 44.68468 to 46.12613, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_38.ckpt\n",
            "Epoch:  39/300 Train steps: 313 Val steps: 35 0.72s loss: 1.539455 acc: 47.590482 fscore_macro: 0.328093 val_loss: 1.602869 val_acc: 45.045045 val_fscore_macro: 0.311128\n",
            "Epoch:  40/300 Train steps: 313 Val steps: 35 0.67s loss: 1.529406 acc: 48.050390 fscore_macro: 0.332534 val_loss: 1.593188 val_acc: 47.027027 val_fscore_macro: 0.325802\n",
            "Epoch 40: val_acc improved from 46.12613 to 47.02703, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_40.ckpt\n",
            "Epoch:  41/300 Train steps: 313 Val steps: 35 0.64s loss: 1.519680 acc: 48.990202 fscore_macro: 0.341781 val_loss: 1.583801 val_acc: 47.207207 val_fscore_macro: 0.326976\n",
            "Epoch 41: val_acc improved from 47.02703 to 47.20721, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_41.ckpt\n",
            "Epoch:  42/300 Train steps: 313 Val steps: 35 0.67s loss: 1.509476 acc: 49.630074 fscore_macro: 0.346008 val_loss: 1.572291 val_acc: 48.108108 val_fscore_macro: 0.339842\n",
            "Epoch 42: val_acc improved from 47.20721 to 48.10811, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_42.ckpt\n",
            "Epoch:  43/300 Train steps: 313 Val steps: 35 0.70s loss: 1.499439 acc: 50.289942 fscore_macro: 0.352092 val_loss: 1.562074 val_acc: 48.108108 val_fscore_macro: 0.341186\n",
            "Epoch:  44/300 Train steps: 313 Val steps: 35 0.87s loss: 1.489089 acc: 50.849830 fscore_macro: 0.357745 val_loss: 1.555699 val_acc: 47.927928 val_fscore_macro: 0.334198\n",
            "Epoch:  45/300 Train steps: 313 Val steps: 35 0.84s loss: 1.478481 acc: 51.549690 fscore_macro: 0.362779 val_loss: 1.542397 val_acc: 49.189189 val_fscore_macro: 0.344064\n",
            "Epoch 45: val_acc improved from 48.10811 to 49.18919, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_45.ckpt\n",
            "Epoch:  46/300 Train steps: 313 Val steps: 35 0.88s loss: 1.468744 acc: 51.869626 fscore_macro: 0.366376 val_loss: 1.536129 val_acc: 49.729730 val_fscore_macro: 0.349898\n",
            "Epoch 46: val_acc improved from 49.18919 to 49.72973, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_46.ckpt\n",
            "Epoch:  47/300 Train steps: 313 Val steps: 35 0.85s loss: 1.459169 acc: 52.449510 fscore_macro: 0.370382 val_loss: 1.521704 val_acc: 50.810811 val_fscore_macro: 0.362486\n",
            "Epoch 47: val_acc improved from 49.72973 to 50.81081, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_47.ckpt\n",
            "Epoch:  48/300 Train steps: 313 Val steps: 35 0.81s loss: 1.449511 acc: 53.029394 fscore_macro: 0.374563 val_loss: 1.510492 val_acc: 51.351351 val_fscore_macro: 0.366990\n",
            "Epoch 48: val_acc improved from 50.81081 to 51.35135, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_48.ckpt\n",
            "Epoch:  49/300 Train steps: 313 Val steps: 35 0.66s loss: 1.439031 acc: 53.049390 fscore_macro: 0.375261 val_loss: 1.501504 val_acc: 52.072072 val_fscore_macro: 0.372300\n",
            "Epoch 49: val_acc improved from 51.35135 to 52.07207, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_49.ckpt\n",
            "Epoch:  50/300 Train steps: 313 Val steps: 35 0.68s loss: 1.428864 acc: 53.429314 fscore_macro: 0.378238 val_loss: 1.491196 val_acc: 51.891892 val_fscore_macro: 0.371879\n",
            "Epoch:  51/300 Train steps: 313 Val steps: 35 0.66s loss: 1.419629 acc: 53.969206 fscore_macro: 0.382379 val_loss: 1.481626 val_acc: 52.972973 val_fscore_macro: 0.377922\n",
            "Epoch 51: val_acc improved from 52.07207 to 52.97297, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_51.ckpt\n",
            "Epoch:  52/300 Train steps: 313 Val steps: 35 0.67s loss: 1.409928 acc: 54.389122 fscore_macro: 0.384729 val_loss: 1.472560 val_acc: 52.432432 val_fscore_macro: 0.375384\n",
            "Epoch:  53/300 Train steps: 313 Val steps: 35 0.65s loss: 1.399772 acc: 54.909018 fscore_macro: 0.389295 val_loss: 1.462702 val_acc: 52.432432 val_fscore_macro: 0.374379\n",
            "Epoch:  54/300 Train steps: 313 Val steps: 35 0.71s loss: 1.389075 acc: 55.028994 fscore_macro: 0.389727 val_loss: 1.456384 val_acc: 51.531532 val_fscore_macro: 0.370547\n",
            "Epoch:  55/300 Train steps: 313 Val steps: 35 0.67s loss: 1.381057 acc: 55.268946 fscore_macro: 0.391629 val_loss: 1.442125 val_acc: 54.234234 val_fscore_macro: 0.386938\n",
            "Epoch 55: val_acc improved from 52.97297 to 54.23423, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_55.ckpt\n",
            "Epoch:  56/300 Train steps: 313 Val steps: 35 0.67s loss: 1.370766 acc: 55.948810 fscore_macro: 0.395849 val_loss: 1.437007 val_acc: 52.792793 val_fscore_macro: 0.381530\n",
            "Epoch:  57/300 Train steps: 313 Val steps: 35 0.66s loss: 1.361851 acc: 56.068786 fscore_macro: 0.397431 val_loss: 1.424155 val_acc: 54.774775 val_fscore_macro: 0.393445\n",
            "Epoch 57: val_acc improved from 54.23423 to 54.77477, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_57.ckpt\n",
            "Epoch:  58/300 Train steps: 313 Val steps: 35 0.65s loss: 1.352615 acc: 56.688662 fscore_macro: 0.401318 val_loss: 1.417403 val_acc: 55.315315 val_fscore_macro: 0.397244\n",
            "Epoch 58: val_acc improved from 54.77477 to 55.31532, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_58.ckpt\n",
            "Epoch:  59/300 Train steps: 313 Val steps: 35 0.67s loss: 1.342758 acc: 56.388722 fscore_macro: 0.399807 val_loss: 1.407632 val_acc: 55.495496 val_fscore_macro: 0.396114\n",
            "Epoch 59: val_acc improved from 55.31532 to 55.49550, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_59.ckpt\n",
            "Epoch:  60/300 Train steps: 313 Val steps: 35 0.66s loss: 1.334025 acc: 56.848630 fscore_macro: 0.402264 val_loss: 1.396698 val_acc: 56.216216 val_fscore_macro: 0.398449\n",
            "Epoch 60: val_acc improved from 55.49550 to 56.21622, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_60.ckpt\n",
            "Epoch:  61/300 Train steps: 313 Val steps: 35 0.67s loss: 1.325519 acc: 57.408518 fscore_macro: 0.405668 val_loss: 1.389254 val_acc: 54.954955 val_fscore_macro: 0.395691\n",
            "Epoch:  62/300 Train steps: 313 Val steps: 35 0.69s loss: 1.316409 acc: 57.428514 fscore_macro: 0.406242 val_loss: 1.382134 val_acc: 54.594595 val_fscore_macro: 0.393254\n",
            "Epoch:  63/300 Train steps: 313 Val steps: 35 0.68s loss: 1.307663 acc: 57.788442 fscore_macro: 0.408804 val_loss: 1.371685 val_acc: 55.855856 val_fscore_macro: 0.402993\n",
            "Epoch:  64/300 Train steps: 313 Val steps: 35 0.64s loss: 1.298721 acc: 58.588282 fscore_macro: 0.413997 val_loss: 1.365704 val_acc: 55.495496 val_fscore_macro: 0.393269\n",
            "Epoch:  65/300 Train steps: 313 Val steps: 35 0.64s loss: 1.291026 acc: 58.308338 fscore_macro: 0.411221 val_loss: 1.357511 val_acc: 55.315315 val_fscore_macro: 0.400222\n",
            "Epoch:  66/300 Train steps: 313 Val steps: 35 0.65s loss: 1.282543 acc: 58.268346 fscore_macro: 0.412285 val_loss: 1.348864 val_acc: 56.216216 val_fscore_macro: 0.402430\n",
            "Epoch 66: val_acc improved from 56.21622 to 56.21622, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_66.ckpt\n",
            "Epoch:  67/300 Train steps: 313 Val steps: 35 0.67s loss: 1.274319 acc: 58.468306 fscore_macro: 0.412363 val_loss: 1.339722 val_acc: 56.396396 val_fscore_macro: 0.405832\n",
            "Epoch 67: val_acc improved from 56.21622 to 56.39640, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_67.ckpt\n",
            "Epoch:  68/300 Train steps: 313 Val steps: 35 0.64s loss: 1.266370 acc: 58.728254 fscore_macro: 0.414107 val_loss: 1.331387 val_acc: 56.216216 val_fscore_macro: 0.403780\n",
            "Epoch:  69/300 Train steps: 313 Val steps: 35 0.70s loss: 1.258748 acc: 58.888222 fscore_macro: 0.415000 val_loss: 1.323589 val_acc: 56.036036 val_fscore_macro: 0.401362\n",
            "Epoch:  70/300 Train steps: 313 Val steps: 35 0.65s loss: 1.250959 acc: 58.908218 fscore_macro: 0.415710 val_loss: 1.315223 val_acc: 56.576577 val_fscore_macro: 0.403994\n",
            "Epoch 70: val_acc improved from 56.39640 to 56.57658, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_70.ckpt\n",
            "Epoch:  71/300 Train steps: 313 Val steps: 35 0.66s loss: 1.242697 acc: 59.168166 fscore_macro: 0.416490 val_loss: 1.311746 val_acc: 56.756757 val_fscore_macro: 0.406288\n",
            "Epoch 71: val_acc improved from 56.57658 to 56.75676, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_71.ckpt\n",
            "Epoch:  72/300 Train steps: 313 Val steps: 35 0.66s loss: 1.235632 acc: 59.588082 fscore_macro: 0.419270 val_loss: 1.302116 val_acc: 56.576577 val_fscore_macro: 0.402939\n",
            "Epoch:  73/300 Train steps: 313 Val steps: 35 0.68s loss: 1.228362 acc: 59.508098 fscore_macro: 0.418987 val_loss: 1.294583 val_acc: 56.576577 val_fscore_macro: 0.405749\n",
            "Epoch:  74/300 Train steps: 313 Val steps: 35 0.64s loss: 1.221171 acc: 59.908018 fscore_macro: 0.422854 val_loss: 1.288392 val_acc: 55.495496 val_fscore_macro: 0.390786\n",
            "Epoch:  75/300 Train steps: 313 Val steps: 35 0.66s loss: 1.214240 acc: 59.948010 fscore_macro: 0.421836 val_loss: 1.280877 val_acc: 56.756757 val_fscore_macro: 0.407569\n",
            "Epoch:  76/300 Train steps: 313 Val steps: 35 0.74s loss: 1.206746 acc: 60.187962 fscore_macro: 0.424115 val_loss: 1.274209 val_acc: 57.117117 val_fscore_macro: 0.408454\n",
            "Epoch 76: val_acc improved from 56.75676 to 57.11712, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_76.ckpt\n",
            "Epoch:  77/300 Train steps: 313 Val steps: 35 0.67s loss: 1.200573 acc: 60.347930 fscore_macro: 0.424780 val_loss: 1.269737 val_acc: 57.117117 val_fscore_macro: 0.409432\n",
            "Epoch:  78/300 Train steps: 313 Val steps: 35 0.65s loss: 1.193216 acc: 60.147970 fscore_macro: 0.423025 val_loss: 1.263625 val_acc: 56.756757 val_fscore_macro: 0.407890\n",
            "Epoch:  79/300 Train steps: 313 Val steps: 35 0.65s loss: 1.187345 acc: 60.627874 fscore_macro: 0.427598 val_loss: 1.259951 val_acc: 56.936937 val_fscore_macro: 0.409355\n",
            "Epoch:  80/300 Train steps: 313 Val steps: 35 0.63s loss: 1.180633 acc: 60.527894 fscore_macro: 0.427278 val_loss: 1.253533 val_acc: 58.198198 val_fscore_macro: 0.414999\n",
            "Epoch 80: val_acc improved from 57.11712 to 58.19820, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_80.ckpt\n",
            "Epoch:  81/300 Train steps: 313 Val steps: 35 0.64s loss: 1.174408 acc: 60.767846 fscore_macro: 0.429049 val_loss: 1.244546 val_acc: 56.936937 val_fscore_macro: 0.405623\n",
            "Epoch:  82/300 Train steps: 313 Val steps: 35 0.65s loss: 1.168163 acc: 60.847830 fscore_macro: 0.430006 val_loss: 1.240562 val_acc: 57.657658 val_fscore_macro: 0.411613\n",
            "Epoch:  83/300 Train steps: 313 Val steps: 35 0.65s loss: 1.162120 acc: 61.067786 fscore_macro: 0.432881 val_loss: 1.232299 val_acc: 58.378378 val_fscore_macro: 0.415304\n",
            "Epoch 83: val_acc improved from 58.19820 to 58.37838, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_83.ckpt\n",
            "Epoch:  84/300 Train steps: 313 Val steps: 35 0.65s loss: 1.156086 acc: 61.087782 fscore_macro: 0.434436 val_loss: 1.227307 val_acc: 57.297297 val_fscore_macro: 0.410830\n",
            "Epoch:  85/300 Train steps: 313 Val steps: 35 0.65s loss: 1.149774 acc: 61.567686 fscore_macro: 0.438519 val_loss: 1.223210 val_acc: 57.657658 val_fscore_macro: 0.421838\n",
            "Epoch:  86/300 Train steps: 313 Val steps: 35 0.63s loss: 1.143614 acc: 61.367726 fscore_macro: 0.436601 val_loss: 1.215985 val_acc: 58.018018 val_fscore_macro: 0.412673\n",
            "Epoch:  87/300 Train steps: 313 Val steps: 35 0.64s loss: 1.138364 acc: 61.847630 fscore_macro: 0.440735 val_loss: 1.212067 val_acc: 58.378378 val_fscore_macro: 0.415495\n",
            "Epoch 87: val_acc improved from 58.37838 to 58.37838, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_87.ckpt\n",
            "Epoch:  88/300 Train steps: 313 Val steps: 35 0.69s loss: 1.132170 acc: 61.687662 fscore_macro: 0.440683 val_loss: 1.206706 val_acc: 57.837838 val_fscore_macro: 0.420243\n",
            "Epoch:  89/300 Train steps: 313 Val steps: 35 0.65s loss: 1.127579 acc: 62.327534 fscore_macro: 0.446240 val_loss: 1.199773 val_acc: 59.279279 val_fscore_macro: 0.429738\n",
            "Epoch 89: val_acc improved from 58.37838 to 59.27928, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_89.ckpt\n",
            "Epoch:  90/300 Train steps: 313 Val steps: 35 0.66s loss: 1.121457 acc: 61.787642 fscore_macro: 0.443312 val_loss: 1.198348 val_acc: 58.558559 val_fscore_macro: 0.422755\n",
            "Epoch:  91/300 Train steps: 313 Val steps: 35 0.68s loss: 1.116301 acc: 62.047590 fscore_macro: 0.443898 val_loss: 1.192365 val_acc: 58.198198 val_fscore_macro: 0.423015\n",
            "Epoch:  92/300 Train steps: 313 Val steps: 35 0.62s loss: 1.111222 acc: 62.687463 fscore_macro: 0.453578 val_loss: 1.187346 val_acc: 60.360360 val_fscore_macro: 0.437090\n",
            "Epoch 92: val_acc improved from 59.27928 to 60.36036, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_92.ckpt\n",
            "Epoch:  93/300 Train steps: 313 Val steps: 35 0.66s loss: 1.105908 acc: 62.587483 fscore_macro: 0.450397 val_loss: 1.181981 val_acc: 59.819820 val_fscore_macro: 0.433180\n",
            "Epoch:  94/300 Train steps: 313 Val steps: 35 0.66s loss: 1.100795 acc: 62.707459 fscore_macro: 0.453233 val_loss: 1.175700 val_acc: 59.279279 val_fscore_macro: 0.436147\n",
            "Epoch:  95/300 Train steps: 313 Val steps: 35 0.65s loss: 1.095558 acc: 62.927415 fscore_macro: 0.454429 val_loss: 1.172905 val_acc: 58.738739 val_fscore_macro: 0.431466\n",
            "Epoch:  96/300 Train steps: 313 Val steps: 35 0.65s loss: 1.091819 acc: 62.887423 fscore_macro: 0.456257 val_loss: 1.166485 val_acc: 59.639640 val_fscore_macro: 0.444245\n",
            "Epoch:  97/300 Train steps: 313 Val steps: 35 0.69s loss: 1.086171 acc: 63.247351 fscore_macro: 0.459429 val_loss: 1.165053 val_acc: 58.918919 val_fscore_macro: 0.433691\n",
            "Epoch:  98/300 Train steps: 313 Val steps: 35 0.67s loss: 1.081672 acc: 63.167367 fscore_macro: 0.460144 val_loss: 1.159088 val_acc: 59.819820 val_fscore_macro: 0.445697\n",
            "Epoch:  99/300 Train steps: 313 Val steps: 35 0.64s loss: 1.076673 acc: 63.487303 fscore_macro: 0.464633 val_loss: 1.151188 val_acc: 60.360360 val_fscore_macro: 0.449502\n",
            "Epoch: 100/300 Train steps: 313 Val steps: 35 0.63s loss: 1.072117 acc: 63.647271 fscore_macro: 0.469543 val_loss: 1.148902 val_acc: 60.900901 val_fscore_macro: 0.458632\n",
            "Epoch 100: val_acc improved from 60.36036 to 60.90090, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_100.ckpt\n",
            "Epoch: 101/300 Train steps: 313 Val steps: 35 0.64s loss: 1.066362 acc: 63.707259 fscore_macro: 0.473611 val_loss: 1.152740 val_acc: 61.081081 val_fscore_macro: 0.460236\n",
            "Epoch 101: val_acc improved from 60.90090 to 61.08108, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_101.ckpt\n",
            "Epoch: 102/300 Train steps: 313 Val steps: 35 0.64s loss: 1.063283 acc: 64.127175 fscore_macro: 0.474691 val_loss: 1.141981 val_acc: 61.081081 val_fscore_macro: 0.452199\n",
            "Epoch 102: val_acc improved from 61.08108 to 61.08108, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_102.ckpt\n",
            "Epoch: 103/300 Train steps: 313 Val steps: 35 0.65s loss: 1.057906 acc: 63.987203 fscore_macro: 0.473674 val_loss: 1.138159 val_acc: 61.441441 val_fscore_macro: 0.460865\n",
            "Epoch 103: val_acc improved from 61.08108 to 61.44144, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_103.ckpt\n",
            "Epoch: 104/300 Train steps: 313 Val steps: 35 0.68s loss: 1.054854 acc: 64.107179 fscore_macro: 0.477618 val_loss: 1.130642 val_acc: 61.081081 val_fscore_macro: 0.459628\n",
            "Epoch: 105/300 Train steps: 313 Val steps: 35 0.67s loss: 1.049566 acc: 64.347131 fscore_macro: 0.481612 val_loss: 1.127534 val_acc: 61.261261 val_fscore_macro: 0.461022\n",
            "Epoch: 106/300 Train steps: 313 Val steps: 35 0.67s loss: 1.046259 acc: 64.227155 fscore_macro: 0.478419 val_loss: 1.124832 val_acc: 61.261261 val_fscore_macro: 0.460448\n",
            "Epoch: 107/300 Train steps: 313 Val steps: 35 0.65s loss: 1.041384 acc: 64.987003 fscore_macro: 0.490485 val_loss: 1.121312 val_acc: 60.180180 val_fscore_macro: 0.453555\n",
            "Epoch: 108/300 Train steps: 313 Val steps: 35 0.66s loss: 1.036109 acc: 64.807039 fscore_macro: 0.488788 val_loss: 1.117606 val_acc: 62.162162 val_fscore_macro: 0.465015\n",
            "Epoch 108: val_acc improved from 61.44144 to 62.16216, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_108.ckpt\n",
            "Epoch: 109/300 Train steps: 313 Val steps: 35 0.66s loss: 1.032971 acc: 64.927015 fscore_macro: 0.488328 val_loss: 1.111730 val_acc: 61.981982 val_fscore_macro: 0.472674\n",
            "Epoch: 110/300 Train steps: 313 Val steps: 35 0.66s loss: 1.029118 acc: 65.386923 fscore_macro: 0.498332 val_loss: 1.107618 val_acc: 62.162162 val_fscore_macro: 0.470237\n",
            "Epoch 110: val_acc improved from 62.16216 to 62.16216, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_110.ckpt\n",
            "Epoch: 111/300 Train steps: 313 Val steps: 35 0.67s loss: 1.025567 acc: 65.326935 fscore_macro: 0.498061 val_loss: 1.105242 val_acc: 62.522523 val_fscore_macro: 0.472492\n",
            "Epoch 111: val_acc improved from 62.16216 to 62.52252, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_111.ckpt\n",
            "Epoch: 112/300 Train steps: 313 Val steps: 35 0.66s loss: 1.021390 acc: 65.626875 fscore_macro: 0.499796 val_loss: 1.101590 val_acc: 62.342342 val_fscore_macro: 0.470960\n",
            "Epoch: 113/300 Train steps: 313 Val steps: 35 0.69s loss: 1.017130 acc: 65.646871 fscore_macro: 0.499431 val_loss: 1.100142 val_acc: 61.801802 val_fscore_macro: 0.468473\n",
            "Epoch: 114/300 Train steps: 313 Val steps: 35 0.67s loss: 1.013535 acc: 65.966807 fscore_macro: 0.504075 val_loss: 1.092866 val_acc: 62.342342 val_fscore_macro: 0.470243\n",
            "Epoch: 115/300 Train steps: 313 Val steps: 35 0.64s loss: 1.009797 acc: 66.046791 fscore_macro: 0.505887 val_loss: 1.093022 val_acc: 61.801802 val_fscore_macro: 0.472977\n",
            "Epoch: 116/300 Train steps: 313 Val steps: 35 0.64s loss: 1.006339 acc: 66.006799 fscore_macro: 0.505747 val_loss: 1.085224 val_acc: 62.522523 val_fscore_macro: 0.471379\n",
            "Epoch: 117/300 Train steps: 313 Val steps: 35 0.65s loss: 1.002176 acc: 66.106779 fscore_macro: 0.507942 val_loss: 1.090560 val_acc: 62.342342 val_fscore_macro: 0.475555\n",
            "Epoch: 118/300 Train steps: 313 Val steps: 35 0.70s loss: 0.998919 acc: 65.846831 fscore_macro: 0.503867 val_loss: 1.081557 val_acc: 62.882883 val_fscore_macro: 0.479427\n",
            "Epoch 118: val_acc improved from 62.52252 to 62.88288, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_118.ckpt\n",
            "Epoch: 119/300 Train steps: 313 Val steps: 35 0.70s loss: 0.995785 acc: 66.506699 fscore_macro: 0.511563 val_loss: 1.076664 val_acc: 62.882883 val_fscore_macro: 0.475940\n",
            "Epoch: 120/300 Train steps: 313 Val steps: 35 0.66s loss: 0.991990 acc: 66.606679 fscore_macro: 0.514943 val_loss: 1.076970 val_acc: 63.603604 val_fscore_macro: 0.488374\n",
            "Epoch 120: val_acc improved from 62.88288 to 63.60360, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_120.ckpt\n",
            "Epoch: 121/300 Train steps: 313 Val steps: 35 0.67s loss: 0.988797 acc: 66.586683 fscore_macro: 0.514557 val_loss: 1.072547 val_acc: 62.702703 val_fscore_macro: 0.483884\n",
            "Epoch: 122/300 Train steps: 313 Val steps: 35 0.67s loss: 0.985306 acc: 66.966607 fscore_macro: 0.517707 val_loss: 1.067508 val_acc: 62.882883 val_fscore_macro: 0.477373\n",
            "Epoch: 123/300 Train steps: 313 Val steps: 35 0.66s loss: 0.981377 acc: 67.246551 fscore_macro: 0.520541 val_loss: 1.064326 val_acc: 63.063063 val_fscore_macro: 0.477956\n",
            "Epoch: 124/300 Train steps: 313 Val steps: 35 0.69s loss: 0.977627 acc: 66.926615 fscore_macro: 0.517378 val_loss: 1.063332 val_acc: 63.783784 val_fscore_macro: 0.492629\n",
            "Epoch 124: val_acc improved from 63.60360 to 63.78378, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_124.ckpt\n",
            "Epoch: 125/300 Train steps: 313 Val steps: 35 0.67s loss: 0.975691 acc: 67.506499 fscore_macro: 0.525546 val_loss: 1.056287 val_acc: 63.783784 val_fscore_macro: 0.486773\n",
            "Epoch 125: val_acc improved from 63.78378 to 63.78378, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_125.ckpt\n",
            "Epoch: 126/300 Train steps: 313 Val steps: 35 0.69s loss: 0.972154 acc: 67.426515 fscore_macro: 0.523926 val_loss: 1.056001 val_acc: 64.684685 val_fscore_macro: 0.506705\n",
            "Epoch 126: val_acc improved from 63.78378 to 64.68468, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_126.ckpt\n",
            "Epoch: 127/300 Train steps: 313 Val steps: 35 0.67s loss: 0.968681 acc: 67.446511 fscore_macro: 0.526248 val_loss: 1.049657 val_acc: 64.324324 val_fscore_macro: 0.500418\n",
            "Epoch: 128/300 Train steps: 313 Val steps: 35 0.67s loss: 0.965570 acc: 67.686463 fscore_macro: 0.528075 val_loss: 1.051090 val_acc: 64.144144 val_fscore_macro: 0.499537\n",
            "Epoch: 129/300 Train steps: 313 Val steps: 35 0.67s loss: 0.962373 acc: 67.566487 fscore_macro: 0.527264 val_loss: 1.045581 val_acc: 63.783784 val_fscore_macro: 0.491030\n",
            "Epoch: 130/300 Train steps: 313 Val steps: 35 0.67s loss: 0.959166 acc: 67.906419 fscore_macro: 0.529971 val_loss: 1.041247 val_acc: 64.504505 val_fscore_macro: 0.508422\n",
            "Epoch: 131/300 Train steps: 313 Val steps: 35 0.73s loss: 0.956619 acc: 67.806439 fscore_macro: 0.528418 val_loss: 1.037201 val_acc: 64.324324 val_fscore_macro: 0.507471\n",
            "Epoch: 132/300 Train steps: 313 Val steps: 35 0.94s loss: 0.952371 acc: 68.046391 fscore_macro: 0.532735 val_loss: 1.036819 val_acc: 64.504504 val_fscore_macro: 0.500903\n",
            "Epoch: 133/300 Train steps: 313 Val steps: 35 0.93s loss: 0.950617 acc: 67.926415 fscore_macro: 0.533372 val_loss: 1.032949 val_acc: 64.144144 val_fscore_macro: 0.500222\n",
            "Epoch: 134/300 Train steps: 313 Val steps: 35 0.81s loss: 0.946807 acc: 68.626275 fscore_macro: 0.538777 val_loss: 1.029753 val_acc: 64.504505 val_fscore_macro: 0.507024\n",
            "Epoch: 135/300 Train steps: 313 Val steps: 35 0.89s loss: 0.943813 acc: 68.386323 fscore_macro: 0.535309 val_loss: 1.032730 val_acc: 66.306306 val_fscore_macro: 0.525520\n",
            "Epoch 135: val_acc improved from 64.68468 to 66.30631, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_135.ckpt\n",
            "Epoch: 136/300 Train steps: 313 Val steps: 35 0.71s loss: 0.940326 acc: 68.726255 fscore_macro: 0.539511 val_loss: 1.025256 val_acc: 65.585586 val_fscore_macro: 0.519077\n",
            "Epoch: 137/300 Train steps: 313 Val steps: 35 0.67s loss: 0.938145 acc: 68.526295 fscore_macro: 0.539144 val_loss: 1.022334 val_acc: 65.405405 val_fscore_macro: 0.513242\n",
            "Epoch: 138/300 Train steps: 313 Val steps: 35 0.67s loss: 0.935582 acc: 68.846231 fscore_macro: 0.542020 val_loss: 1.021201 val_acc: 63.783784 val_fscore_macro: 0.501430\n",
            "Epoch: 139/300 Train steps: 313 Val steps: 35 0.65s loss: 0.932617 acc: 68.886223 fscore_macro: 0.542498 val_loss: 1.021169 val_acc: 65.585586 val_fscore_macro: 0.513879\n",
            "Epoch: 140/300 Train steps: 313 Val steps: 35 0.66s loss: 0.930454 acc: 68.886223 fscore_macro: 0.543145 val_loss: 1.020039 val_acc: 65.945946 val_fscore_macro: 0.520539\n",
            "Epoch: 141/300 Train steps: 313 Val steps: 35 0.64s loss: 0.927478 acc: 69.006199 fscore_macro: 0.544184 val_loss: 1.009740 val_acc: 65.945946 val_fscore_macro: 0.521841\n",
            "Epoch: 142/300 Train steps: 313 Val steps: 35 0.66s loss: 0.925121 acc: 69.026195 fscore_macro: 0.543095 val_loss: 1.008594 val_acc: 65.225225 val_fscore_macro: 0.512366\n",
            "Epoch: 143/300 Train steps: 313 Val steps: 35 0.66s loss: 0.921235 acc: 69.066187 fscore_macro: 0.545016 val_loss: 1.011873 val_acc: 65.405405 val_fscore_macro: 0.516598\n",
            "Epoch: 144/300 Train steps: 313 Val steps: 35 0.65s loss: 0.918835 acc: 69.246151 fscore_macro: 0.545896 val_loss: 1.006565 val_acc: 66.306306 val_fscore_macro: 0.519427\n",
            "Epoch 144: val_acc improved from 66.30631 to 66.30631, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_144.ckpt\n",
            "Epoch: 145/300 Train steps: 313 Val steps: 35 0.66s loss: 0.916530 acc: 69.426115 fscore_macro: 0.548843 val_loss: 1.003663 val_acc: 65.585586 val_fscore_macro: 0.515477\n",
            "Epoch: 146/300 Train steps: 313 Val steps: 35 0.67s loss: 0.914194 acc: 69.566087 fscore_macro: 0.549686 val_loss: 1.002405 val_acc: 65.225225 val_fscore_macro: 0.514114\n",
            "Epoch: 147/300 Train steps: 313 Val steps: 35 0.65s loss: 0.911261 acc: 69.586083 fscore_macro: 0.550693 val_loss: 1.001815 val_acc: 65.765766 val_fscore_macro: 0.516153\n",
            "Epoch: 148/300 Train steps: 313 Val steps: 35 0.66s loss: 0.908498 acc: 69.686063 fscore_macro: 0.550571 val_loss: 0.996046 val_acc: 66.126126 val_fscore_macro: 0.521417\n",
            "Epoch: 149/300 Train steps: 313 Val steps: 35 0.66s loss: 0.905941 acc: 69.706059 fscore_macro: 0.551145 val_loss: 0.994720 val_acc: 65.585586 val_fscore_macro: 0.517250\n",
            "Epoch: 150/300 Train steps: 313 Val steps: 35 0.66s loss: 0.903147 acc: 69.706059 fscore_macro: 0.550238 val_loss: 0.993448 val_acc: 65.405405 val_fscore_macro: 0.515639\n",
            "Epoch: 151/300 Train steps: 313 Val steps: 35 0.64s loss: 0.901854 acc: 69.866027 fscore_macro: 0.551893 val_loss: 0.989109 val_acc: 66.846847 val_fscore_macro: 0.524863\n",
            "Epoch 151: val_acc improved from 66.30631 to 66.84685, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_151.ckpt\n",
            "Epoch: 152/300 Train steps: 313 Val steps: 35 0.69s loss: 0.899051 acc: 69.666067 fscore_macro: 0.551757 val_loss: 0.988884 val_acc: 65.945946 val_fscore_macro: 0.519322\n",
            "Epoch: 153/300 Train steps: 313 Val steps: 35 0.66s loss: 0.896003 acc: 69.826035 fscore_macro: 0.553172 val_loss: 0.987553 val_acc: 66.126126 val_fscore_macro: 0.520781\n",
            "Epoch: 154/300 Train steps: 313 Val steps: 35 0.65s loss: 0.894482 acc: 70.145971 fscore_macro: 0.554265 val_loss: 0.982816 val_acc: 66.126126 val_fscore_macro: 0.519677\n",
            "Epoch: 155/300 Train steps: 313 Val steps: 35 0.67s loss: 0.890794 acc: 70.185963 fscore_macro: 0.555490 val_loss: 0.981760 val_acc: 66.846847 val_fscore_macro: 0.529743\n",
            "Epoch 155: val_acc improved from 66.84685 to 66.84685, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_155.ckpt\n",
            "Epoch: 156/300 Train steps: 313 Val steps: 35 0.65s loss: 0.889772 acc: 70.245951 fscore_macro: 0.556109 val_loss: 0.979139 val_acc: 65.765766 val_fscore_macro: 0.517568\n",
            "Epoch: 157/300 Train steps: 313 Val steps: 35 0.68s loss: 0.887512 acc: 70.385923 fscore_macro: 0.557158 val_loss: 0.978478 val_acc: 66.846847 val_fscore_macro: 0.524474\n",
            "Epoch: 158/300 Train steps: 313 Val steps: 35 0.70s loss: 0.884043 acc: 70.425915 fscore_macro: 0.557253 val_loss: 0.972176 val_acc: 66.486486 val_fscore_macro: 0.523131\n",
            "Epoch: 159/300 Train steps: 313 Val steps: 35 0.62s loss: 0.882258 acc: 70.145971 fscore_macro: 0.555686 val_loss: 0.973364 val_acc: 66.486486 val_fscore_macro: 0.523615\n",
            "Epoch: 160/300 Train steps: 313 Val steps: 35 0.66s loss: 0.880204 acc: 70.665867 fscore_macro: 0.559562 val_loss: 0.974243 val_acc: 67.027027 val_fscore_macro: 0.528704\n",
            "Epoch 160: val_acc improved from 66.84685 to 67.02703, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_160.ckpt\n",
            "Epoch: 161/300 Train steps: 313 Val steps: 35 0.65s loss: 0.877503 acc: 70.745851 fscore_macro: 0.559994 val_loss: 0.967509 val_acc: 66.486487 val_fscore_macro: 0.521701\n",
            "Epoch: 162/300 Train steps: 313 Val steps: 35 0.61s loss: 0.874665 acc: 70.525895 fscore_macro: 0.558421 val_loss: 0.964237 val_acc: 67.387387 val_fscore_macro: 0.530031\n",
            "Epoch 162: val_acc improved from 67.02703 to 67.38739, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_162.ckpt\n",
            "Epoch: 163/300 Train steps: 313 Val steps: 35 0.66s loss: 0.873167 acc: 70.785843 fscore_macro: 0.560149 val_loss: 0.962041 val_acc: 66.486487 val_fscore_macro: 0.518878\n",
            "Epoch: 164/300 Train steps: 313 Val steps: 35 0.69s loss: 0.871739 acc: 70.725855 fscore_macro: 0.561293 val_loss: 0.961891 val_acc: 67.387387 val_fscore_macro: 0.526120\n",
            "Epoch 164: val_acc improved from 67.38739 to 67.38739, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_164.ckpt\n",
            "Epoch: 165/300 Train steps: 313 Val steps: 35 0.66s loss: 0.869057 acc: 70.885823 fscore_macro: 0.561811 val_loss: 0.956763 val_acc: 67.207207 val_fscore_macro: 0.525992\n",
            "Epoch: 166/300 Train steps: 313 Val steps: 35 0.66s loss: 0.866828 acc: 70.765847 fscore_macro: 0.560793 val_loss: 0.956028 val_acc: 67.567568 val_fscore_macro: 0.529612\n",
            "Epoch 166: val_acc improved from 67.38739 to 67.56757, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_166.ckpt\n",
            "Epoch: 167/300 Train steps: 313 Val steps: 35 0.66s loss: 0.863875 acc: 71.325735 fscore_macro: 0.565470 val_loss: 0.954697 val_acc: 66.666667 val_fscore_macro: 0.523753\n",
            "Epoch: 168/300 Train steps: 313 Val steps: 35 0.63s loss: 0.862282 acc: 71.105779 fscore_macro: 0.564027 val_loss: 0.952132 val_acc: 67.927928 val_fscore_macro: 0.534378\n",
            "Epoch 168: val_acc improved from 67.56757 to 67.92793, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_168.ckpt\n",
            "Epoch: 169/300 Train steps: 313 Val steps: 35 0.69s loss: 0.860304 acc: 70.965807 fscore_macro: 0.563082 val_loss: 0.952800 val_acc: 67.927928 val_fscore_macro: 0.532609\n",
            "Epoch: 170/300 Train steps: 313 Val steps: 35 0.65s loss: 0.858642 acc: 71.145771 fscore_macro: 0.564252 val_loss: 0.951280 val_acc: 67.747748 val_fscore_macro: 0.530717\n",
            "Epoch: 171/300 Train steps: 313 Val steps: 35 0.65s loss: 0.855915 acc: 71.345731 fscore_macro: 0.564570 val_loss: 0.955086 val_acc: 67.207207 val_fscore_macro: 0.531233\n",
            "Epoch: 172/300 Train steps: 313 Val steps: 35 0.64s loss: 0.853853 acc: 71.185763 fscore_macro: 0.565383 val_loss: 0.943569 val_acc: 67.927928 val_fscore_macro: 0.531854\n",
            "Epoch: 173/300 Train steps: 313 Val steps: 35 0.66s loss: 0.852338 acc: 71.365727 fscore_macro: 0.565877 val_loss: 0.943558 val_acc: 67.567568 val_fscore_macro: 0.532609\n",
            "Epoch: 174/300 Train steps: 313 Val steps: 35 0.65s loss: 0.849537 acc: 71.125775 fscore_macro: 0.569551 val_loss: 0.944364 val_acc: 68.828829 val_fscore_macro: 0.540600\n",
            "Epoch 174: val_acc improved from 67.92793 to 68.82883, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_174.ckpt\n",
            "Epoch: 175/300 Train steps: 313 Val steps: 35 0.65s loss: 0.847425 acc: 71.585683 fscore_macro: 0.574301 val_loss: 0.937479 val_acc: 67.207207 val_fscore_macro: 0.530130\n",
            "Epoch: 176/300 Train steps: 313 Val steps: 35 0.68s loss: 0.846805 acc: 71.465707 fscore_macro: 0.569534 val_loss: 0.934153 val_acc: 68.468469 val_fscore_macro: 0.539919\n",
            "Epoch: 177/300 Train steps: 313 Val steps: 35 0.67s loss: 0.844900 acc: 71.685663 fscore_macro: 0.572419 val_loss: 0.940407 val_acc: 67.027027 val_fscore_macro: 0.526909\n",
            "Epoch: 178/300 Train steps: 313 Val steps: 35 0.64s loss: 0.841892 acc: 71.705659 fscore_macro: 0.574705 val_loss: 0.932951 val_acc: 68.828829 val_fscore_macro: 0.539927\n",
            "Epoch 178: val_acc improved from 68.82883 to 68.82883, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_178.ckpt\n",
            "Epoch: 179/300 Train steps: 313 Val steps: 35 0.65s loss: 0.840720 acc: 71.945611 fscore_macro: 0.579311 val_loss: 0.932731 val_acc: 68.828829 val_fscore_macro: 0.541137\n",
            "Epoch: 180/300 Train steps: 313 Val steps: 35 0.67s loss: 0.838532 acc: 71.945611 fscore_macro: 0.576960 val_loss: 0.930788 val_acc: 67.927928 val_fscore_macro: 0.532287\n",
            "Epoch: 181/300 Train steps: 313 Val steps: 35 0.65s loss: 0.837447 acc: 71.885623 fscore_macro: 0.579973 val_loss: 0.934026 val_acc: 67.927928 val_fscore_macro: 0.534981\n",
            "Epoch: 182/300 Train steps: 313 Val steps: 35 0.66s loss: 0.834333 acc: 72.245551 fscore_macro: 0.578574 val_loss: 0.925967 val_acc: 68.828829 val_fscore_macro: 0.543025\n",
            "Epoch: 183/300 Train steps: 313 Val steps: 35 0.67s loss: 0.832303 acc: 72.085583 fscore_macro: 0.580173 val_loss: 0.930867 val_acc: 67.927928 val_fscore_macro: 0.538642\n",
            "Epoch: 184/300 Train steps: 313 Val steps: 35 0.68s loss: 0.831389 acc: 72.145571 fscore_macro: 0.581755 val_loss: 0.924006 val_acc: 68.648649 val_fscore_macro: 0.539419\n",
            "Epoch: 185/300 Train steps: 313 Val steps: 35 0.66s loss: 0.828737 acc: 72.085583 fscore_macro: 0.580877 val_loss: 0.918149 val_acc: 69.009009 val_fscore_macro: 0.542158\n",
            "Epoch 185: val_acc improved from 68.82883 to 69.00901, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_185.ckpt\n",
            "Epoch: 186/300 Train steps: 313 Val steps: 35 0.67s loss: 0.826704 acc: 71.865627 fscore_macro: 0.582238 val_loss: 0.923637 val_acc: 68.828829 val_fscore_macro: 0.543386\n",
            "Epoch: 187/300 Train steps: 313 Val steps: 35 0.66s loss: 0.825733 acc: 72.405519 fscore_macro: 0.589743 val_loss: 0.918771 val_acc: 69.189189 val_fscore_macro: 0.546277\n",
            "Epoch 187: val_acc improved from 69.00901 to 69.18919, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_187.ckpt\n",
            "Epoch: 188/300 Train steps: 313 Val steps: 35 0.68s loss: 0.823306 acc: 71.985603 fscore_macro: 0.581734 val_loss: 0.920619 val_acc: 69.009009 val_fscore_macro: 0.545056\n",
            "Epoch: 189/300 Train steps: 313 Val steps: 35 0.66s loss: 0.821787 acc: 72.505499 fscore_macro: 0.590293 val_loss: 0.916768 val_acc: 68.828829 val_fscore_macro: 0.543020\n",
            "Epoch: 190/300 Train steps: 313 Val steps: 35 0.68s loss: 0.818246 acc: 72.565487 fscore_macro: 0.590156 val_loss: 0.921851 val_acc: 69.189189 val_fscore_macro: 0.566103\n",
            "Epoch: 191/300 Train steps: 313 Val steps: 35 0.65s loss: 0.817555 acc: 72.765447 fscore_macro: 0.593229 val_loss: 0.911758 val_acc: 69.369369 val_fscore_macro: 0.549096\n",
            "Epoch 191: val_acc improved from 69.18919 to 69.36937, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_191.ckpt\n",
            "Epoch: 192/300 Train steps: 313 Val steps: 35 0.64s loss: 0.817132 acc: 72.325535 fscore_macro: 0.589924 val_loss: 0.908058 val_acc: 69.189189 val_fscore_macro: 0.546053\n",
            "Epoch: 193/300 Train steps: 313 Val steps: 35 0.67s loss: 0.814671 acc: 72.245551 fscore_macro: 0.585837 val_loss: 0.908452 val_acc: 69.909910 val_fscore_macro: 0.571203\n",
            "Epoch 193: val_acc improved from 69.36937 to 69.90991, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_193.ckpt\n",
            "Epoch: 194/300 Train steps: 313 Val steps: 35 0.65s loss: 0.812503 acc: 72.505499 fscore_macro: 0.594158 val_loss: 0.909618 val_acc: 68.648649 val_fscore_macro: 0.540786\n",
            "Epoch: 195/300 Train steps: 313 Val steps: 35 0.67s loss: 0.810359 acc: 72.585483 fscore_macro: 0.588202 val_loss: 0.906070 val_acc: 70.270270 val_fscore_macro: 0.573136\n",
            "Epoch 195: val_acc improved from 69.90991 to 70.27027, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_195.ckpt\n",
            "Epoch: 196/300 Train steps: 313 Val steps: 35 0.67s loss: 0.809382 acc: 73.085383 fscore_macro: 0.593983 val_loss: 0.905851 val_acc: 69.009009 val_fscore_macro: 0.565416\n",
            "Epoch: 197/300 Train steps: 313 Val steps: 35 0.66s loss: 0.807433 acc: 73.185363 fscore_macro: 0.605841 val_loss: 0.905282 val_acc: 69.729730 val_fscore_macro: 0.584646\n",
            "Epoch: 198/300 Train steps: 313 Val steps: 35 0.68s loss: 0.805926 acc: 73.125375 fscore_macro: 0.602260 val_loss: 0.901648 val_acc: 69.189189 val_fscore_macro: 0.565252\n",
            "Epoch: 199/300 Train steps: 313 Val steps: 35 0.70s loss: 0.804674 acc: 73.245351 fscore_macro: 0.605649 val_loss: 0.900174 val_acc: 69.729730 val_fscore_macro: 0.569860\n",
            "Epoch: 200/300 Train steps: 313 Val steps: 35 0.67s loss: 0.802807 acc: 73.245351 fscore_macro: 0.601698 val_loss: 0.905446 val_acc: 69.549550 val_fscore_macro: 0.583287\n",
            "Epoch: 201/300 Train steps: 313 Val steps: 35 0.69s loss: 0.801022 acc: 73.045391 fscore_macro: 0.603374 val_loss: 0.897423 val_acc: 70.090090 val_fscore_macro: 0.571291\n",
            "Epoch: 202/300 Train steps: 313 Val steps: 35 0.68s loss: 0.798135 acc: 73.505299 fscore_macro: 0.610855 val_loss: 0.901285 val_acc: 69.549550 val_fscore_macro: 0.591263\n",
            "Epoch: 203/300 Train steps: 313 Val steps: 35 0.65s loss: 0.797263 acc: 73.365327 fscore_macro: 0.611100 val_loss: 0.899965 val_acc: 69.729730 val_fscore_macro: 0.585661\n",
            "Epoch: 204/300 Train steps: 313 Val steps: 35 0.66s loss: 0.795770 acc: 73.185363 fscore_macro: 0.606242 val_loss: 0.892628 val_acc: 70.450450 val_fscore_macro: 0.589538\n",
            "Epoch 204: val_acc improved from 70.27027 to 70.45045, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_204.ckpt\n",
            "Epoch: 205/300 Train steps: 313 Val steps: 35 0.67s loss: 0.794492 acc: 73.385323 fscore_macro: 0.609244 val_loss: 0.893715 val_acc: 70.630631 val_fscore_macro: 0.591592\n",
            "Epoch 205: val_acc improved from 70.45045 to 70.63063, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_205.ckpt\n",
            "Epoch: 206/300 Train steps: 313 Val steps: 35 0.67s loss: 0.792302 acc: 73.585283 fscore_macro: 0.615989 val_loss: 0.897460 val_acc: 69.189189 val_fscore_macro: 0.576883\n",
            "Epoch: 207/300 Train steps: 313 Val steps: 35 0.65s loss: 0.791083 acc: 73.305339 fscore_macro: 0.613178 val_loss: 0.887565 val_acc: 70.450450 val_fscore_macro: 0.588492\n",
            "Epoch: 208/300 Train steps: 313 Val steps: 35 0.66s loss: 0.789352 acc: 73.585283 fscore_macro: 0.618553 val_loss: 0.885192 val_acc: 70.810811 val_fscore_macro: 0.598045\n",
            "Epoch 208: val_acc improved from 70.63063 to 70.81081, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_208.ckpt\n",
            "Epoch: 209/300 Train steps: 313 Val steps: 35 0.66s loss: 0.787982 acc: 73.725255 fscore_macro: 0.619494 val_loss: 0.886851 val_acc: 70.990991 val_fscore_macro: 0.615721\n",
            "Epoch 209: val_acc improved from 70.81081 to 70.99099, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_209.ckpt\n",
            "Epoch: 210/300 Train steps: 313 Val steps: 35 0.66s loss: 0.786404 acc: 73.965207 fscore_macro: 0.623663 val_loss: 0.883575 val_acc: 70.810811 val_fscore_macro: 0.611501\n",
            "Epoch: 211/300 Train steps: 313 Val steps: 35 0.67s loss: 0.783965 acc: 73.745251 fscore_macro: 0.620103 val_loss: 0.883407 val_acc: 70.450450 val_fscore_macro: 0.612045\n",
            "Epoch: 212/300 Train steps: 313 Val steps: 35 0.69s loss: 0.783908 acc: 73.725255 fscore_macro: 0.618955 val_loss: 0.884721 val_acc: 70.270270 val_fscore_macro: 0.609933\n",
            "Epoch: 213/300 Train steps: 313 Val steps: 35 0.67s loss: 0.781204 acc: 73.705259 fscore_macro: 0.618687 val_loss: 0.881940 val_acc: 71.171171 val_fscore_macro: 0.608234\n",
            "Epoch 213: val_acc improved from 70.99099 to 71.17117, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_213.ckpt\n",
            "Epoch: 214/300 Train steps: 313 Val steps: 35 0.66s loss: 0.779544 acc: 73.905219 fscore_macro: 0.619539 val_loss: 0.880619 val_acc: 70.990991 val_fscore_macro: 0.614482\n",
            "Epoch: 215/300 Train steps: 313 Val steps: 35 0.67s loss: 0.778976 acc: 73.745251 fscore_macro: 0.626939 val_loss: 0.877872 val_acc: 70.810811 val_fscore_macro: 0.606754\n",
            "Epoch: 216/300 Train steps: 313 Val steps: 35 0.71s loss: 0.776752 acc: 74.125175 fscore_macro: 0.625349 val_loss: 0.886427 val_acc: 69.369369 val_fscore_macro: 0.602827\n",
            "Epoch: 217/300 Train steps: 313 Val steps: 35 0.75s loss: 0.775460 acc: 74.125175 fscore_macro: 0.630079 val_loss: 0.875235 val_acc: 71.171171 val_fscore_macro: 0.617134\n",
            "Epoch 217: val_acc improved from 71.17117 to 71.17117, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_217.ckpt\n",
            "Epoch: 218/300 Train steps: 313 Val steps: 35 0.94s loss: 0.772886 acc: 74.185163 fscore_macro: 0.632229 val_loss: 0.873941 val_acc: 70.810811 val_fscore_macro: 0.612448\n",
            "Epoch: 219/300 Train steps: 313 Val steps: 35 0.89s loss: 0.771154 acc: 74.205159 fscore_macro: 0.632226 val_loss: 0.881372 val_acc: 70.810811 val_fscore_macro: 0.618733\n",
            "Epoch: 220/300 Train steps: 313 Val steps: 35 0.85s loss: 0.771756 acc: 74.065187 fscore_macro: 0.631062 val_loss: 0.875920 val_acc: 71.531532 val_fscore_macro: 0.622220\n",
            "Epoch 220: val_acc improved from 71.17117 to 71.53153, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_220.ckpt\n",
            "Epoch: 221/300 Train steps: 313 Val steps: 35 0.92s loss: 0.770032 acc: 74.145171 fscore_macro: 0.634837 val_loss: 0.876565 val_acc: 70.630631 val_fscore_macro: 0.610257\n",
            "Epoch: 222/300 Train steps: 313 Val steps: 35 0.67s loss: 0.766251 acc: 74.285143 fscore_macro: 0.635838 val_loss: 0.871916 val_acc: 70.450450 val_fscore_macro: 0.611706\n",
            "Epoch: 223/300 Train steps: 313 Val steps: 35 0.67s loss: 0.766785 acc: 74.385123 fscore_macro: 0.636453 val_loss: 0.868778 val_acc: 71.171171 val_fscore_macro: 0.617169\n",
            "Epoch: 224/300 Train steps: 313 Val steps: 35 0.66s loss: 0.765477 acc: 74.545091 fscore_macro: 0.635057 val_loss: 0.869783 val_acc: 70.810811 val_fscore_macro: 0.614110\n",
            "Epoch: 225/300 Train steps: 313 Val steps: 35 0.65s loss: 0.762560 acc: 74.645071 fscore_macro: 0.641684 val_loss: 0.864555 val_acc: 71.891892 val_fscore_macro: 0.623110\n",
            "Epoch 225: val_acc improved from 71.53153 to 71.89189, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_225.ckpt\n",
            "Epoch: 226/300 Train steps: 313 Val steps: 35 0.66s loss: 0.762112 acc: 74.425115 fscore_macro: 0.633868 val_loss: 0.867395 val_acc: 71.351351 val_fscore_macro: 0.619387\n",
            "Epoch: 227/300 Train steps: 313 Val steps: 35 0.66s loss: 0.761048 acc: 74.685063 fscore_macro: 0.635547 val_loss: 0.864904 val_acc: 70.990991 val_fscore_macro: 0.616506\n",
            "Epoch: 228/300 Train steps: 313 Val steps: 35 0.67s loss: 0.758628 acc: 74.785043 fscore_macro: 0.642722 val_loss: 0.868522 val_acc: 70.630631 val_fscore_macro: 0.612743\n",
            "Epoch: 229/300 Train steps: 313 Val steps: 35 0.67s loss: 0.757855 acc: 74.605079 fscore_macro: 0.641859 val_loss: 0.871249 val_acc: 70.630631 val_fscore_macro: 0.613950\n",
            "Epoch: 230/300 Train steps: 313 Val steps: 35 0.68s loss: 0.756447 acc: 74.725055 fscore_macro: 0.639923 val_loss: 0.861966 val_acc: 71.711712 val_fscore_macro: 0.622304\n",
            "Epoch: 231/300 Train steps: 313 Val steps: 35 0.68s loss: 0.754678 acc: 74.685063 fscore_macro: 0.644706 val_loss: 0.864833 val_acc: 71.351351 val_fscore_macro: 0.619771\n",
            "Epoch: 232/300 Train steps: 313 Val steps: 35 0.68s loss: 0.753957 acc: 74.905019 fscore_macro: 0.645392 val_loss: 0.855782 val_acc: 70.810811 val_fscore_macro: 0.619758\n",
            "Epoch: 233/300 Train steps: 313 Val steps: 35 0.66s loss: 0.752709 acc: 74.505099 fscore_macro: 0.637235 val_loss: 0.856881 val_acc: 72.432432 val_fscore_macro: 0.628261\n",
            "Epoch 233: val_acc improved from 71.89189 to 72.43243, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_233.ckpt\n",
            "Epoch: 234/300 Train steps: 313 Val steps: 35 0.67s loss: 0.750192 acc: 74.705059 fscore_macro: 0.644365 val_loss: 0.856942 val_acc: 71.351351 val_fscore_macro: 0.622399\n",
            "Epoch: 235/300 Train steps: 313 Val steps: 35 0.69s loss: 0.749938 acc: 75.364927 fscore_macro: 0.648842 val_loss: 0.854699 val_acc: 72.072072 val_fscore_macro: 0.629685\n",
            "Epoch: 236/300 Train steps: 313 Val steps: 35 0.68s loss: 0.748243 acc: 74.905019 fscore_macro: 0.648693 val_loss: 0.851464 val_acc: 71.711712 val_fscore_macro: 0.624992\n",
            "Epoch: 237/300 Train steps: 313 Val steps: 35 0.67s loss: 0.746763 acc: 75.184963 fscore_macro: 0.650426 val_loss: 0.848403 val_acc: 71.711712 val_fscore_macro: 0.624164\n",
            "Epoch: 238/300 Train steps: 313 Val steps: 35 0.67s loss: 0.744464 acc: 75.384923 fscore_macro: 0.654959 val_loss: 0.856100 val_acc: 71.711712 val_fscore_macro: 0.624973\n",
            "Epoch: 239/300 Train steps: 313 Val steps: 35 0.67s loss: 0.744186 acc: 75.184963 fscore_macro: 0.656237 val_loss: 0.849341 val_acc: 71.531532 val_fscore_macro: 0.623074\n",
            "Epoch: 240/300 Train steps: 313 Val steps: 35 0.66s loss: 0.741833 acc: 75.384923 fscore_macro: 0.660401 val_loss: 0.857115 val_acc: 70.810811 val_fscore_macro: 0.613239\n",
            "Epoch: 241/300 Train steps: 313 Val steps: 35 0.66s loss: 0.742390 acc: 75.204959 fscore_macro: 0.653792 val_loss: 0.852661 val_acc: 71.711712 val_fscore_macro: 0.627931\n",
            "Epoch: 242/300 Train steps: 313 Val steps: 35 0.67s loss: 0.740431 acc: 75.304939 fscore_macro: 0.657954 val_loss: 0.846694 val_acc: 71.351351 val_fscore_macro: 0.623620\n",
            "Epoch: 243/300 Train steps: 313 Val steps: 35 0.67s loss: 0.739446 acc: 75.304939 fscore_macro: 0.658136 val_loss: 0.851407 val_acc: 70.810811 val_fscore_macro: 0.622937\n",
            "Epoch: 244/300 Train steps: 313 Val steps: 35 0.66s loss: 0.736146 acc: 75.384923 fscore_macro: 0.653188 val_loss: 0.849387 val_acc: 71.171171 val_fscore_macro: 0.623980\n",
            "Epoch: 245/300 Train steps: 313 Val steps: 35 0.67s loss: 0.736640 acc: 75.424915 fscore_macro: 0.662418 val_loss: 0.842240 val_acc: 71.891892 val_fscore_macro: 0.629146\n",
            "Epoch: 246/300 Train steps: 313 Val steps: 35 0.68s loss: 0.735015 acc: 76.004799 fscore_macro: 0.670178 val_loss: 0.844971 val_acc: 72.612613 val_fscore_macro: 0.633019\n",
            "Epoch 246: val_acc improved from 72.43243 to 72.61261, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_246.ckpt\n",
            "Epoch: 247/300 Train steps: 313 Val steps: 35 0.64s loss: 0.734570 acc: 75.744851 fscore_macro: 0.662173 val_loss: 0.839208 val_acc: 71.891892 val_fscore_macro: 0.629823\n",
            "Epoch: 248/300 Train steps: 313 Val steps: 35 0.66s loss: 0.732664 acc: 75.964807 fscore_macro: 0.669539 val_loss: 0.843866 val_acc: 72.072072 val_fscore_macro: 0.630041\n",
            "Epoch: 249/300 Train steps: 313 Val steps: 35 0.68s loss: 0.731338 acc: 75.684863 fscore_macro: 0.667073 val_loss: 0.845389 val_acc: 71.531532 val_fscore_macro: 0.624759\n",
            "Epoch: 250/300 Train steps: 313 Val steps: 35 0.68s loss: 0.728264 acc: 75.744851 fscore_macro: 0.664930 val_loss: 0.840134 val_acc: 71.711712 val_fscore_macro: 0.620485\n",
            "Epoch: 251/300 Train steps: 313 Val steps: 35 0.66s loss: 0.728241 acc: 75.524895 fscore_macro: 0.664983 val_loss: 0.833962 val_acc: 72.432433 val_fscore_macro: 0.632113\n",
            "Epoch: 252/300 Train steps: 313 Val steps: 35 0.66s loss: 0.727009 acc: 75.864827 fscore_macro: 0.669571 val_loss: 0.837276 val_acc: 71.711712 val_fscore_macro: 0.628086\n",
            "Epoch: 253/300 Train steps: 313 Val steps: 35 0.67s loss: 0.725197 acc: 75.764847 fscore_macro: 0.672502 val_loss: 0.843623 val_acc: 71.171171 val_fscore_macro: 0.623296\n",
            "Epoch: 254/300 Train steps: 313 Val steps: 35 0.66s loss: 0.723602 acc: 76.064787 fscore_macro: 0.670916 val_loss: 0.832096 val_acc: 72.072072 val_fscore_macro: 0.629566\n",
            "Epoch: 255/300 Train steps: 313 Val steps: 35 0.66s loss: 0.723374 acc: 76.264747 fscore_macro: 0.670887 val_loss: 0.833792 val_acc: 72.252252 val_fscore_macro: 0.627657\n",
            "Epoch: 256/300 Train steps: 313 Val steps: 35 0.68s loss: 0.721245 acc: 76.184763 fscore_macro: 0.675204 val_loss: 0.828812 val_acc: 72.072072 val_fscore_macro: 0.626283\n",
            "Epoch: 257/300 Train steps: 313 Val steps: 35 0.67s loss: 0.720938 acc: 75.964807 fscore_macro: 0.665588 val_loss: 0.833811 val_acc: 72.792793 val_fscore_macro: 0.633609\n",
            "Epoch 257: val_acc improved from 72.61261 to 72.79279, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_257.ckpt\n",
            "Epoch: 258/300 Train steps: 313 Val steps: 35 0.66s loss: 0.719068 acc: 76.144771 fscore_macro: 0.679421 val_loss: 0.841144 val_acc: 72.072072 val_fscore_macro: 0.630836\n",
            "Epoch: 259/300 Train steps: 313 Val steps: 35 0.67s loss: 0.719095 acc: 76.124775 fscore_macro: 0.674359 val_loss: 0.832859 val_acc: 72.432432 val_fscore_macro: 0.632047\n",
            "Epoch: 260/300 Train steps: 313 Val steps: 35 0.69s loss: 0.717149 acc: 76.144771 fscore_macro: 0.677006 val_loss: 0.844998 val_acc: 69.909910 val_fscore_macro: 0.613107\n",
            "Epoch: 261/300 Train steps: 313 Val steps: 35 0.65s loss: 0.716018 acc: 75.924815 fscore_macro: 0.677144 val_loss: 0.832553 val_acc: 72.432432 val_fscore_macro: 0.630477\n",
            "Epoch: 262/300 Train steps: 313 Val steps: 35 0.67s loss: 0.715304 acc: 76.384723 fscore_macro: 0.681098 val_loss: 0.824670 val_acc: 72.972973 val_fscore_macro: 0.635457\n",
            "Epoch 262: val_acc improved from 72.79279 to 72.97297, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_262.ckpt\n",
            "Epoch: 263/300 Train steps: 313 Val steps: 35 0.67s loss: 0.712489 acc: 76.244751 fscore_macro: 0.680885 val_loss: 0.828156 val_acc: 72.432432 val_fscore_macro: 0.635531\n",
            "Epoch: 264/300 Train steps: 313 Val steps: 35 0.67s loss: 0.712211 acc: 76.304739 fscore_macro: 0.679585 val_loss: 0.831057 val_acc: 71.171171 val_fscore_macro: 0.622523\n",
            "Epoch: 265/300 Train steps: 313 Val steps: 35 0.66s loss: 0.710281 acc: 76.384723 fscore_macro: 0.681707 val_loss: 0.826949 val_acc: 73.153153 val_fscore_macro: 0.635163\n",
            "Epoch 265: val_acc improved from 72.97297 to 73.15315, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_265.ckpt\n",
            "Epoch: 266/300 Train steps: 313 Val steps: 35 0.67s loss: 0.711159 acc: 76.604679 fscore_macro: 0.686079 val_loss: 0.823883 val_acc: 72.252252 val_fscore_macro: 0.626442\n",
            "Epoch: 267/300 Train steps: 313 Val steps: 35 0.75s loss: 0.708675 acc: 76.684663 fscore_macro: 0.688024 val_loss: 0.819411 val_acc: 72.432432 val_fscore_macro: 0.623796\n",
            "Epoch: 268/300 Train steps: 313 Val steps: 35 0.70s loss: 0.707075 acc: 76.704659 fscore_macro: 0.680003 val_loss: 0.820035 val_acc: 72.252252 val_fscore_macro: 0.628779\n",
            "Epoch: 269/300 Train steps: 313 Val steps: 35 0.66s loss: 0.706619 acc: 76.504699 fscore_macro: 0.682171 val_loss: 0.829443 val_acc: 70.990991 val_fscore_macro: 0.621897\n",
            "Epoch: 270/300 Train steps: 313 Val steps: 35 0.70s loss: 0.706488 acc: 76.484703 fscore_macro: 0.685474 val_loss: 0.818901 val_acc: 72.612613 val_fscore_macro: 0.625786\n",
            "Epoch: 271/300 Train steps: 313 Val steps: 35 0.67s loss: 0.703800 acc: 76.584683 fscore_macro: 0.685450 val_loss: 0.824696 val_acc: 72.432432 val_fscore_macro: 0.628210\n",
            "Epoch: 272/300 Train steps: 313 Val steps: 35 0.69s loss: 0.703376 acc: 76.664667 fscore_macro: 0.685903 val_loss: 0.818659 val_acc: 72.972973 val_fscore_macro: 0.630969\n",
            "Epoch: 273/300 Train steps: 313 Val steps: 35 0.68s loss: 0.702720 acc: 76.884623 fscore_macro: 0.688932 val_loss: 0.824204 val_acc: 72.432432 val_fscore_macro: 0.631251\n",
            "Epoch: 274/300 Train steps: 313 Val steps: 35 0.67s loss: 0.700457 acc: 76.844631 fscore_macro: 0.686009 val_loss: 0.814223 val_acc: 73.513514 val_fscore_macro: 0.633225\n",
            "Epoch 274: val_acc improved from 73.15315 to 73.51351, saving file to model/fasttext/average_embedding_mlp_300/checkpoint_epoch_274.ckpt\n",
            "Epoch: 275/300 Train steps: 313 Val steps: 35 0.69s loss: 0.699143 acc: 76.724655 fscore_macro: 0.689856 val_loss: 0.809035 val_acc: 72.972973 val_fscore_macro: 0.632915\n",
            "Epoch: 276/300 Train steps: 313 Val steps: 35 0.67s loss: 0.698127 acc: 76.684663 fscore_macro: 0.690138 val_loss: 0.810583 val_acc: 72.972973 val_fscore_macro: 0.634455\n",
            "Epoch: 277/300 Train steps: 313 Val steps: 35 0.67s loss: 0.697885 acc: 76.864627 fscore_macro: 0.691633 val_loss: 0.823861 val_acc: 71.171171 val_fscore_macro: 0.625726\n",
            "Epoch: 278/300 Train steps: 313 Val steps: 35 0.68s loss: 0.695873 acc: 77.204559 fscore_macro: 0.693135 val_loss: 0.811275 val_acc: 72.792793 val_fscore_macro: 0.632510\n",
            "Epoch: 279/300 Train steps: 313 Val steps: 35 0.68s loss: 0.695304 acc: 76.744651 fscore_macro: 0.690986 val_loss: 0.811495 val_acc: 72.612613 val_fscore_macro: 0.635851\n",
            "Epoch: 280/300 Train steps: 313 Val steps: 35 0.71s loss: 0.694947 acc: 77.024595 fscore_macro: 0.694816 val_loss: 0.810491 val_acc: 72.792793 val_fscore_macro: 0.640910\n",
            "Epoch: 281/300 Train steps: 313 Val steps: 35 0.68s loss: 0.693829 acc: 76.864627 fscore_macro: 0.692376 val_loss: 0.807538 val_acc: 73.333333 val_fscore_macro: 0.635011\n",
            "Epoch: 282/300 Train steps: 313 Val steps: 35 0.65s loss: 0.692244 acc: 76.984603 fscore_macro: 0.694938 val_loss: 0.818222 val_acc: 71.891892 val_fscore_macro: 0.639094\n",
            "Epoch: 283/300 Train steps: 313 Val steps: 35 0.68s loss: 0.691248 acc: 77.024595 fscore_macro: 0.693965 val_loss: 0.807455 val_acc: 73.513513 val_fscore_macro: 0.633875\n",
            "Epoch: 284/300 Train steps: 313 Val steps: 35 0.65s loss: 0.690599 acc: 77.264547 fscore_macro: 0.695088 val_loss: 0.810408 val_acc: 73.153153 val_fscore_macro: 0.653575\n",
            "Epoch: 285/300 Train steps: 313 Val steps: 35 0.68s loss: 0.688704 acc: 77.064587 fscore_macro: 0.692571 val_loss: 0.814950 val_acc: 71.891892 val_fscore_macro: 0.641534\n",
            "Epoch: 286/300 Train steps: 313 Val steps: 35 0.67s loss: 0.687418 acc: 76.964607 fscore_macro: 0.696939 val_loss: 0.810785 val_acc: 73.333333 val_fscore_macro: 0.637833\n",
            "Epoch: 287/300 Train steps: 313 Val steps: 35 0.66s loss: 0.686011 acc: 77.344531 fscore_macro: 0.694453 val_loss: 0.804167 val_acc: 73.333333 val_fscore_macro: 0.644399\n",
            "Epoch: 288/300 Train steps: 313 Val steps: 35 0.65s loss: 0.685332 acc: 76.884623 fscore_macro: 0.693793 val_loss: 0.809371 val_acc: 71.711712 val_fscore_macro: 0.627919\n",
            "Epoch: 289/300 Train steps: 313 Val steps: 35 0.65s loss: 0.684439 acc: 77.304539 fscore_macro: 0.697941 val_loss: 0.808987 val_acc: 73.153153 val_fscore_macro: 0.634288\n",
            "Epoch: 290/300 Train steps: 313 Val steps: 35 0.67s loss: 0.684001 acc: 77.104579 fscore_macro: 0.693100 val_loss: 0.819193 val_acc: 72.072072 val_fscore_macro: 0.644201\n",
            "Epoch: 291/300 Train steps: 313 Val steps: 35 0.66s loss: 0.683125 acc: 77.164567 fscore_macro: 0.694767 val_loss: 0.801955 val_acc: 73.333333 val_fscore_macro: 0.634982\n",
            "Epoch: 292/300 Train steps: 313 Val steps: 35 0.66s loss: 0.680538 acc: 77.284543 fscore_macro: 0.699944 val_loss: 0.802563 val_acc: 72.612613 val_fscore_macro: 0.643301\n",
            "Epoch: 293/300 Train steps: 313 Val steps: 35 0.66s loss: 0.680609 acc: 77.644471 fscore_macro: 0.702225 val_loss: 0.812003 val_acc: 72.252252 val_fscore_macro: 0.644334\n",
            "Epoch: 294/300 Train steps: 313 Val steps: 35 0.66s loss: 0.679066 acc: 77.284543 fscore_macro: 0.694989 val_loss: 0.798642 val_acc: 73.153153 val_fscore_macro: 0.651950\n",
            "Epoch: 295/300 Train steps: 313 Val steps: 35 0.68s loss: 0.677798 acc: 77.424515 fscore_macro: 0.697979 val_loss: 0.805890 val_acc: 72.972973 val_fscore_macro: 0.651985\n",
            "Epoch: 296/300 Train steps: 313 Val steps: 35 0.67s loss: 0.677426 acc: 77.384523 fscore_macro: 0.696297 val_loss: 0.808070 val_acc: 72.432432 val_fscore_macro: 0.645512\n",
            "Epoch: 297/300 Train steps: 313 Val steps: 35 0.68s loss: 0.676176 acc: 77.724455 fscore_macro: 0.698285 val_loss: 0.800220 val_acc: 72.972973 val_fscore_macro: 0.657195\n",
            "Epoch: 298/300 Train steps: 313 Val steps: 35 0.68s loss: 0.675128 acc: 77.444511 fscore_macro: 0.701413 val_loss: 0.803918 val_acc: 72.792793 val_fscore_macro: 0.654827\n",
            "Epoch: 299/300 Train steps: 313 Val steps: 35 0.68s loss: 0.673822 acc: 77.524495 fscore_macro: 0.699898 val_loss: 0.809596 val_acc: 72.252252 val_fscore_macro: 0.647822\n",
            "Epoch: 300/300 Train steps: 313 Val steps: 35 0.79s loss: 0.674705 acc: 77.584483 fscore_macro: 0.701355 val_loss: 0.793258 val_acc: 73.333333 val_fscore_macro: 0.656887\n",
            "Restoring data from model/fasttext/average_embedding_mlp_300/checkpoint_epoch_274.ckpt\n",
            "Found best checkpoint at epoch: 274\n",
            "lr: 0.01, loss: 0.700457, acc: 76.8446, fscore_macro: 0.686009, val_loss: 0.814223, val_acc: 73.5135, val_fscore_macro: 0.633225\n",
            "Loading checkpoint model/fasttext/average_embedding_mlp_300/checkpoint_epoch_274.ckpt\n",
            "Running test\n",
            "Test steps: 32 2.06s test_loss: 0.563107 test_acc: 83.600000 test_fscore_macro: 0.729357     \n",
            "\n",
            " Taille de couche cachée égale à : 300, avec les embeddings préentraînés de fasttext aggrégés avec la méthode maxpool :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Epoch:   1/300 Train steps: 313 Val steps: 35 39.11s loss: 2.031173 acc: 22.695461 fscore_macro: 0.041119 val_loss: 2.011743 val_acc: 20.720721 val_fscore_macro: 0.038200\n",
            "Epoch 1: val_acc improved from -inf to 20.72072, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_1.ckpt\n",
            "Epoch:   2/300 Train steps: 313 Val steps: 35 0.67s loss: 1.967574 acc: 22.735453 fscore_macro: 0.042079 val_loss: 2.003760 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   3/300 Train steps: 313 Val steps: 35 0.67s loss: 1.952128 acc: 24.055189 fscore_macro: 0.063659 val_loss: 1.987220 val_acc: 21.981982 val_fscore_macro: 0.053435\n",
            "Epoch 3: val_acc improved from 20.72072 to 21.98198, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_3.ckpt\n",
            "Epoch:   4/300 Train steps: 313 Val steps: 35 0.65s loss: 1.938204 acc: 23.775245 fscore_macro: 0.054191 val_loss: 1.973123 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   5/300 Train steps: 313 Val steps: 35 0.73s loss: 1.920798 acc: 23.835233 fscore_macro: 0.057317 val_loss: 1.957765 val_acc: 27.207207 val_fscore_macro: 0.111191\n",
            "Epoch 5: val_acc improved from 21.98198 to 27.20721, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_5.ckpt\n",
            "Epoch:   6/300 Train steps: 313 Val steps: 35 0.69s loss: 1.899763 acc: 28.654269 fscore_macro: 0.120122 val_loss: 1.941863 val_acc: 22.522523 val_fscore_macro: 0.072487\n",
            "Epoch:   7/300 Train steps: 313 Val steps: 35 0.66s loss: 1.878175 acc: 29.614077 fscore_macro: 0.139112 val_loss: 1.919960 val_acc: 30.810811 val_fscore_macro: 0.167900\n",
            "Epoch 7: val_acc improved from 27.20721 to 30.81081, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_7.ckpt\n",
            "Epoch:   8/300 Train steps: 313 Val steps: 35 0.66s loss: 1.854843 acc: 33.093381 fscore_macro: 0.169289 val_loss: 1.899952 val_acc: 30.270270 val_fscore_macro: 0.162964\n",
            "Epoch:   9/300 Train steps: 313 Val steps: 35 0.66s loss: 1.830889 acc: 34.413117 fscore_macro: 0.183507 val_loss: 1.878252 val_acc: 31.351351 val_fscore_macro: 0.182344\n",
            "Epoch 9: val_acc improved from 30.81081 to 31.35135, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_9.ckpt\n",
            "Epoch:  10/300 Train steps: 313 Val steps: 35 0.66s loss: 1.806928 acc: 35.972805 fscore_macro: 0.205091 val_loss: 1.857125 val_acc: 31.531532 val_fscore_macro: 0.186444\n",
            "Epoch 10: val_acc improved from 31.35135 to 31.53153, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_10.ckpt\n",
            "Epoch:  11/300 Train steps: 313 Val steps: 35 0.66s loss: 1.784501 acc: 35.812837 fscore_macro: 0.206448 val_loss: 1.837291 val_acc: 32.432432 val_fscore_macro: 0.204619\n",
            "Epoch 11: val_acc improved from 31.53153 to 32.43243, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_11.ckpt\n",
            "Epoch:  12/300 Train steps: 313 Val steps: 35 0.66s loss: 1.762361 acc: 37.512498 fscore_macro: 0.224638 val_loss: 1.821272 val_acc: 31.711712 val_fscore_macro: 0.188168\n",
            "Epoch:  13/300 Train steps: 313 Val steps: 35 0.66s loss: 1.741042 acc: 38.072386 fscore_macro: 0.230503 val_loss: 1.806431 val_acc: 32.252252 val_fscore_macro: 0.195473\n",
            "Epoch:  14/300 Train steps: 313 Val steps: 35 0.66s loss: 1.721004 acc: 38.952210 fscore_macro: 0.238231 val_loss: 1.784604 val_acc: 33.873874 val_fscore_macro: 0.220722\n",
            "Epoch 14: val_acc improved from 32.43243 to 33.87387, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_14.ckpt\n",
            "Epoch:  15/300 Train steps: 313 Val steps: 35 0.67s loss: 1.702452 acc: 39.352130 fscore_macro: 0.241327 val_loss: 1.768813 val_acc: 34.234234 val_fscore_macro: 0.216759\n",
            "Epoch 15: val_acc improved from 33.87387 to 34.23423, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_15.ckpt\n",
            "Epoch:  16/300 Train steps: 313 Val steps: 35 0.69s loss: 1.684104 acc: 40.171966 fscore_macro: 0.248324 val_loss: 1.755376 val_acc: 33.693694 val_fscore_macro: 0.216775\n",
            "Epoch:  17/300 Train steps: 313 Val steps: 35 0.70s loss: 1.667426 acc: 40.591882 fscore_macro: 0.255054 val_loss: 1.739082 val_acc: 32.612613 val_fscore_macro: 0.209667\n",
            "Epoch:  18/300 Train steps: 313 Val steps: 35 0.66s loss: 1.648743 acc: 41.551690 fscore_macro: 0.263985 val_loss: 1.725000 val_acc: 34.774775 val_fscore_macro: 0.221106\n",
            "Epoch 18: val_acc improved from 34.23423 to 34.77477, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_18.ckpt\n",
            "Epoch:  19/300 Train steps: 313 Val steps: 35 0.68s loss: 1.634963 acc: 41.291742 fscore_macro: 0.265337 val_loss: 1.710431 val_acc: 35.855856 val_fscore_macro: 0.223737\n",
            "Epoch 19: val_acc improved from 34.77477 to 35.85586, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_19.ckpt\n",
            "Epoch:  20/300 Train steps: 313 Val steps: 35 0.67s loss: 1.619887 acc: 41.731654 fscore_macro: 0.267807 val_loss: 1.693687 val_acc: 38.558559 val_fscore_macro: 0.265772\n",
            "Epoch 20: val_acc improved from 35.85586 to 38.55856, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_20.ckpt\n",
            "Epoch:  21/300 Train steps: 313 Val steps: 35 0.66s loss: 1.604731 acc: 42.471506 fscore_macro: 0.278696 val_loss: 1.679228 val_acc: 37.837838 val_fscore_macro: 0.251196\n",
            "Epoch:  22/300 Train steps: 313 Val steps: 35 0.69s loss: 1.590727 acc: 43.011398 fscore_macro: 0.285141 val_loss: 1.671902 val_acc: 35.675676 val_fscore_macro: 0.233125\n",
            "Epoch:  23/300 Train steps: 313 Val steps: 35 0.67s loss: 1.575559 acc: 43.671266 fscore_macro: 0.290942 val_loss: 1.673147 val_acc: 38.558559 val_fscore_macro: 0.258509\n",
            "Epoch 23: val_acc improved from 38.55856 to 38.55856, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_23.ckpt\n",
            "Epoch:  24/300 Train steps: 313 Val steps: 35 0.68s loss: 1.565129 acc: 43.631274 fscore_macro: 0.294037 val_loss: 1.649051 val_acc: 38.198198 val_fscore_macro: 0.247471\n",
            "Epoch:  25/300 Train steps: 313 Val steps: 35 0.65s loss: 1.551567 acc: 44.511098 fscore_macro: 0.299678 val_loss: 1.639150 val_acc: 37.837838 val_fscore_macro: 0.239691\n",
            "Epoch:  26/300 Train steps: 313 Val steps: 35 0.66s loss: 1.539798 acc: 44.671066 fscore_macro: 0.304088 val_loss: 1.619832 val_acc: 39.459459 val_fscore_macro: 0.264680\n",
            "Epoch 26: val_acc improved from 38.55856 to 39.45946, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_26.ckpt\n",
            "Epoch:  27/300 Train steps: 313 Val steps: 35 0.68s loss: 1.528716 acc: 45.370926 fscore_macro: 0.309592 val_loss: 1.615996 val_acc: 43.603604 val_fscore_macro: 0.305389\n",
            "Epoch 27: val_acc improved from 39.45946 to 43.60360, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_27.ckpt\n",
            "Epoch:  28/300 Train steps: 313 Val steps: 35 0.65s loss: 1.516816 acc: 46.170766 fscore_macro: 0.317239 val_loss: 1.599054 val_acc: 42.342342 val_fscore_macro: 0.294220\n",
            "Epoch:  29/300 Train steps: 313 Val steps: 35 0.69s loss: 1.507426 acc: 46.150770 fscore_macro: 0.319775 val_loss: 1.587749 val_acc: 44.324324 val_fscore_macro: 0.312688\n",
            "Epoch 29: val_acc improved from 43.60360 to 44.32432, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_29.ckpt\n",
            "Epoch:  30/300 Train steps: 313 Val steps: 35 0.68s loss: 1.494514 acc: 46.910618 fscore_macro: 0.326454 val_loss: 1.586696 val_acc: 43.063063 val_fscore_macro: 0.302632\n",
            "Epoch:  31/300 Train steps: 313 Val steps: 35 0.73s loss: 1.486382 acc: 47.630474 fscore_macro: 0.331594 val_loss: 1.572517 val_acc: 45.765766 val_fscore_macro: 0.321975\n",
            "Epoch 31: val_acc improved from 44.32432 to 45.76577, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_31.ckpt\n",
            "Epoch:  32/300 Train steps: 313 Val steps: 35 0.86s loss: 1.473309 acc: 48.810238 fscore_macro: 0.341514 val_loss: 1.560985 val_acc: 46.846847 val_fscore_macro: 0.332509\n",
            "Epoch 32: val_acc improved from 45.76577 to 46.84685, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_32.ckpt\n",
            "Epoch:  33/300 Train steps: 313 Val steps: 35 0.85s loss: 1.465094 acc: 48.630274 fscore_macro: 0.340514 val_loss: 1.546038 val_acc: 47.927928 val_fscore_macro: 0.341220\n",
            "Epoch 33: val_acc improved from 46.84685 to 47.92793, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_33.ckpt\n",
            "Epoch:  34/300 Train steps: 313 Val steps: 35 0.91s loss: 1.455418 acc: 49.010198 fscore_macro: 0.344780 val_loss: 1.538601 val_acc: 48.468468 val_fscore_macro: 0.347217\n",
            "Epoch 34: val_acc improved from 47.92793 to 48.46847, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_34.ckpt\n",
            "Epoch:  35/300 Train steps: 313 Val steps: 35 0.86s loss: 1.444709 acc: 49.490102 fscore_macro: 0.349979 val_loss: 1.526541 val_acc: 46.846847 val_fscore_macro: 0.332274\n",
            "Epoch:  36/300 Train steps: 313 Val steps: 35 0.81s loss: 1.434564 acc: 50.129974 fscore_macro: 0.355185 val_loss: 1.515820 val_acc: 49.009009 val_fscore_macro: 0.347127\n",
            "Epoch 36: val_acc improved from 48.46847 to 49.00901, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_36.ckpt\n",
            "Epoch:  37/300 Train steps: 313 Val steps: 35 0.66s loss: 1.424378 acc: 50.949810 fscore_macro: 0.361639 val_loss: 1.508293 val_acc: 50.270270 val_fscore_macro: 0.357685\n",
            "Epoch 37: val_acc improved from 49.00901 to 50.27027, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_37.ckpt\n",
            "Epoch:  38/300 Train steps: 313 Val steps: 35 0.66s loss: 1.415762 acc: 51.629674 fscore_macro: 0.367973 val_loss: 1.494741 val_acc: 50.450450 val_fscore_macro: 0.353177\n",
            "Epoch 38: val_acc improved from 50.27027 to 50.45045, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_38.ckpt\n",
            "Epoch:  39/300 Train steps: 313 Val steps: 35 0.66s loss: 1.403353 acc: 52.209558 fscore_macro: 0.374551 val_loss: 1.482227 val_acc: 51.351351 val_fscore_macro: 0.367066\n",
            "Epoch 39: val_acc improved from 50.45045 to 51.35135, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_39.ckpt\n",
            "Epoch:  40/300 Train steps: 313 Val steps: 35 0.67s loss: 1.393835 acc: 52.649470 fscore_macro: 0.376474 val_loss: 1.472082 val_acc: 50.810811 val_fscore_macro: 0.360228\n",
            "Epoch:  41/300 Train steps: 313 Val steps: 35 0.68s loss: 1.384778 acc: 52.709458 fscore_macro: 0.380445 val_loss: 1.471078 val_acc: 49.909910 val_fscore_macro: 0.362212\n",
            "Epoch:  42/300 Train steps: 313 Val steps: 35 0.67s loss: 1.376017 acc: 53.049390 fscore_macro: 0.384435 val_loss: 1.454107 val_acc: 53.333333 val_fscore_macro: 0.375235\n",
            "Epoch 42: val_acc improved from 51.35135 to 53.33333, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_42.ckpt\n",
            "Epoch:  43/300 Train steps: 313 Val steps: 35 0.67s loss: 1.367736 acc: 53.509298 fscore_macro: 0.387556 val_loss: 1.441581 val_acc: 53.513514 val_fscore_macro: 0.376238\n",
            "Epoch 43: val_acc improved from 53.33333 to 53.51351, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_43.ckpt\n",
            "Epoch:  44/300 Train steps: 313 Val steps: 35 0.70s loss: 1.356573 acc: 54.349130 fscore_macro: 0.393640 val_loss: 1.463263 val_acc: 49.369369 val_fscore_macro: 0.358500\n",
            "Epoch:  45/300 Train steps: 313 Val steps: 35 0.72s loss: 1.348030 acc: 54.689062 fscore_macro: 0.399492 val_loss: 1.431750 val_acc: 50.450450 val_fscore_macro: 0.365110\n",
            "Epoch:  46/300 Train steps: 313 Val steps: 35 0.68s loss: 1.338160 acc: 54.729054 fscore_macro: 0.405910 val_loss: 1.413893 val_acc: 52.972973 val_fscore_macro: 0.379083\n",
            "Epoch:  47/300 Train steps: 313 Val steps: 35 0.68s loss: 1.330576 acc: 55.028994 fscore_macro: 0.410953 val_loss: 1.409536 val_acc: 54.414414 val_fscore_macro: 0.387885\n",
            "Epoch 47: val_acc improved from 53.51351 to 54.41441, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_47.ckpt\n",
            "Epoch:  48/300 Train steps: 313 Val steps: 35 0.68s loss: 1.323974 acc: 55.248950 fscore_macro: 0.409793 val_loss: 1.399700 val_acc: 53.693694 val_fscore_macro: 0.382781\n",
            "Epoch:  49/300 Train steps: 313 Val steps: 35 0.67s loss: 1.314072 acc: 55.768846 fscore_macro: 0.417968 val_loss: 1.392000 val_acc: 54.774775 val_fscore_macro: 0.394213\n",
            "Epoch 49: val_acc improved from 54.41441 to 54.77477, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_49.ckpt\n",
            "Epoch:  50/300 Train steps: 313 Val steps: 35 0.65s loss: 1.304692 acc: 55.908818 fscore_macro: 0.418474 val_loss: 1.388591 val_acc: 54.774775 val_fscore_macro: 0.392322\n",
            "Epoch 50: val_acc improved from 54.77477 to 54.77477, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_50.ckpt\n",
            "Epoch:  51/300 Train steps: 313 Val steps: 35 0.70s loss: 1.298005 acc: 56.548690 fscore_macro: 0.424589 val_loss: 1.385627 val_acc: 54.054054 val_fscore_macro: 0.391278\n",
            "Epoch:  52/300 Train steps: 313 Val steps: 35 0.66s loss: 1.291614 acc: 56.328734 fscore_macro: 0.425376 val_loss: 1.370653 val_acc: 53.693694 val_fscore_macro: 0.389010\n",
            "Epoch:  53/300 Train steps: 313 Val steps: 35 0.68s loss: 1.282735 acc: 56.608678 fscore_macro: 0.425823 val_loss: 1.367331 val_acc: 54.234234 val_fscore_macro: 0.407111\n",
            "Epoch:  54/300 Train steps: 313 Val steps: 35 0.68s loss: 1.274558 acc: 57.208558 fscore_macro: 0.432491 val_loss: 1.365499 val_acc: 54.774775 val_fscore_macro: 0.399084\n",
            "Epoch:  55/300 Train steps: 313 Val steps: 35 0.67s loss: 1.269531 acc: 57.268546 fscore_macro: 0.436874 val_loss: 1.348428 val_acc: 56.036036 val_fscore_macro: 0.404601\n",
            "Epoch 55: val_acc improved from 54.77477 to 56.03604, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_55.ckpt\n",
            "Epoch:  56/300 Train steps: 313 Val steps: 35 0.67s loss: 1.260986 acc: 57.728454 fscore_macro: 0.438063 val_loss: 1.345066 val_acc: 56.216216 val_fscore_macro: 0.406610\n",
            "Epoch 56: val_acc improved from 56.03604 to 56.21622, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_56.ckpt\n",
            "Epoch:  57/300 Train steps: 313 Val steps: 35 0.67s loss: 1.256527 acc: 57.328534 fscore_macro: 0.435806 val_loss: 1.343401 val_acc: 53.873874 val_fscore_macro: 0.389984\n",
            "Epoch:  58/300 Train steps: 313 Val steps: 35 0.67s loss: 1.250800 acc: 58.028394 fscore_macro: 0.440138 val_loss: 1.331508 val_acc: 56.396396 val_fscore_macro: 0.407914\n",
            "Epoch 58: val_acc improved from 56.21622 to 56.39640, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_58.ckpt\n",
            "Epoch:  59/300 Train steps: 313 Val steps: 35 0.66s loss: 1.242551 acc: 57.868426 fscore_macro: 0.441300 val_loss: 1.325812 val_acc: 54.954955 val_fscore_macro: 0.409943\n",
            "Epoch:  60/300 Train steps: 313 Val steps: 35 0.64s loss: 1.236056 acc: 58.328334 fscore_macro: 0.445489 val_loss: 1.323904 val_acc: 52.612613 val_fscore_macro: 0.382325\n",
            "Epoch:  61/300 Train steps: 313 Val steps: 35 0.68s loss: 1.232053 acc: 58.108378 fscore_macro: 0.446936 val_loss: 1.322189 val_acc: 56.756757 val_fscore_macro: 0.418418\n",
            "Epoch 61: val_acc improved from 56.39640 to 56.75676, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_61.ckpt\n",
            "Epoch:  62/300 Train steps: 313 Val steps: 35 0.68s loss: 1.223764 acc: 58.388322 fscore_macro: 0.447199 val_loss: 1.337900 val_acc: 56.036036 val_fscore_macro: 0.417083\n",
            "Epoch:  63/300 Train steps: 313 Val steps: 35 0.65s loss: 1.219830 acc: 58.488302 fscore_macro: 0.448606 val_loss: 1.299049 val_acc: 57.117117 val_fscore_macro: 0.422027\n",
            "Epoch 63: val_acc improved from 56.75676 to 57.11712, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_63.ckpt\n",
            "Epoch:  64/300 Train steps: 313 Val steps: 35 0.66s loss: 1.215215 acc: 58.508298 fscore_macro: 0.447703 val_loss: 1.301614 val_acc: 57.297297 val_fscore_macro: 0.423261\n",
            "Epoch 64: val_acc improved from 57.11712 to 57.29730, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_64.ckpt\n",
            "Epoch:  65/300 Train steps: 313 Val steps: 35 0.69s loss: 1.210323 acc: 58.788242 fscore_macro: 0.449113 val_loss: 1.308027 val_acc: 56.396396 val_fscore_macro: 0.409505\n",
            "Epoch:  66/300 Train steps: 313 Val steps: 35 0.66s loss: 1.203302 acc: 59.008198 fscore_macro: 0.456915 val_loss: 1.288819 val_acc: 55.675676 val_fscore_macro: 0.422724\n",
            "Epoch:  67/300 Train steps: 313 Val steps: 35 0.66s loss: 1.198775 acc: 59.008198 fscore_macro: 0.453981 val_loss: 1.281605 val_acc: 56.216216 val_fscore_macro: 0.423362\n",
            "Epoch:  68/300 Train steps: 313 Val steps: 35 0.67s loss: 1.193918 acc: 59.328134 fscore_macro: 0.459164 val_loss: 1.285349 val_acc: 56.216216 val_fscore_macro: 0.423543\n",
            "Epoch:  69/300 Train steps: 313 Val steps: 35 0.66s loss: 1.188295 acc: 59.428114 fscore_macro: 0.461019 val_loss: 1.280064 val_acc: 56.756757 val_fscore_macro: 0.442556\n",
            "Epoch:  70/300 Train steps: 313 Val steps: 35 0.68s loss: 1.184812 acc: 59.388122 fscore_macro: 0.459751 val_loss: 1.266844 val_acc: 57.297297 val_fscore_macro: 0.442343\n",
            "Epoch 70: val_acc improved from 57.29730 to 57.29730, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_70.ckpt\n",
            "Epoch:  71/300 Train steps: 313 Val steps: 35 0.65s loss: 1.177321 acc: 59.288142 fscore_macro: 0.459239 val_loss: 1.271507 val_acc: 57.657658 val_fscore_macro: 0.455247\n",
            "Epoch 71: val_acc improved from 57.29730 to 57.65766, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_71.ckpt\n",
            "Epoch:  72/300 Train steps: 313 Val steps: 35 0.66s loss: 1.172664 acc: 60.047990 fscore_macro: 0.465544 val_loss: 1.275046 val_acc: 56.396396 val_fscore_macro: 0.418114\n",
            "Epoch:  73/300 Train steps: 313 Val steps: 35 0.69s loss: 1.169602 acc: 59.948010 fscore_macro: 0.467846 val_loss: 1.264874 val_acc: 56.576577 val_fscore_macro: 0.435978\n",
            "Epoch:  74/300 Train steps: 313 Val steps: 35 0.67s loss: 1.165619 acc: 60.227954 fscore_macro: 0.468958 val_loss: 1.278395 val_acc: 53.873874 val_fscore_macro: 0.396541\n",
            "Epoch:  75/300 Train steps: 313 Val steps: 35 0.67s loss: 1.163054 acc: 60.127974 fscore_macro: 0.466067 val_loss: 1.253754 val_acc: 56.756757 val_fscore_macro: 0.429815\n",
            "Epoch:  76/300 Train steps: 313 Val steps: 35 0.67s loss: 1.156701 acc: 60.427914 fscore_macro: 0.470198 val_loss: 1.263992 val_acc: 56.036036 val_fscore_macro: 0.421576\n",
            "Epoch:  77/300 Train steps: 313 Val steps: 35 0.67s loss: 1.153629 acc: 60.307938 fscore_macro: 0.470226 val_loss: 1.266666 val_acc: 56.576577 val_fscore_macro: 0.424258\n",
            "Epoch:  78/300 Train steps: 313 Val steps: 35 0.66s loss: 1.147938 acc: 60.727854 fscore_macro: 0.472821 val_loss: 1.238539 val_acc: 57.837838 val_fscore_macro: 0.452094\n",
            "Epoch 78: val_acc improved from 57.65766 to 57.83784, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_78.ckpt\n",
            "Epoch:  79/300 Train steps: 313 Val steps: 35 0.69s loss: 1.145402 acc: 60.887822 fscore_macro: 0.477063 val_loss: 1.253792 val_acc: 57.117117 val_fscore_macro: 0.455433\n",
            "Epoch:  80/300 Train steps: 313 Val steps: 35 0.68s loss: 1.141585 acc: 61.127774 fscore_macro: 0.477195 val_loss: 1.249865 val_acc: 56.936937 val_fscore_macro: 0.455452\n",
            "Epoch:  81/300 Train steps: 313 Val steps: 35 0.68s loss: 1.137891 acc: 60.887822 fscore_macro: 0.473730 val_loss: 1.237330 val_acc: 56.576577 val_fscore_macro: 0.460625\n",
            "Epoch:  82/300 Train steps: 313 Val steps: 35 0.67s loss: 1.133406 acc: 61.367726 fscore_macro: 0.481456 val_loss: 1.241430 val_acc: 57.297297 val_fscore_macro: 0.485277\n",
            "Epoch:  83/300 Train steps: 313 Val steps: 35 0.67s loss: 1.129922 acc: 61.007798 fscore_macro: 0.488394 val_loss: 1.236274 val_acc: 56.036036 val_fscore_macro: 0.444800\n",
            "Epoch:  84/300 Train steps: 313 Val steps: 35 0.64s loss: 1.126514 acc: 61.367726 fscore_macro: 0.483588 val_loss: 1.243581 val_acc: 59.279279 val_fscore_macro: 0.497603\n",
            "Epoch 84: val_acc improved from 57.83784 to 59.27928, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_84.ckpt\n",
            "Epoch:  85/300 Train steps: 313 Val steps: 35 0.66s loss: 1.122614 acc: 61.867626 fscore_macro: 0.495482 val_loss: 1.214159 val_acc: 57.657658 val_fscore_macro: 0.476969\n",
            "Epoch:  86/300 Train steps: 313 Val steps: 35 0.75s loss: 1.115973 acc: 62.227554 fscore_macro: 0.499398 val_loss: 1.237055 val_acc: 57.657658 val_fscore_macro: 0.475171\n",
            "Epoch:  87/300 Train steps: 313 Val steps: 35 0.70s loss: 1.114399 acc: 61.607678 fscore_macro: 0.499526 val_loss: 1.224605 val_acc: 59.459460 val_fscore_macro: 0.494428\n",
            "Epoch 87: val_acc improved from 59.27928 to 59.45946, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_87.ckpt\n",
            "Epoch:  88/300 Train steps: 313 Val steps: 35 0.67s loss: 1.108208 acc: 61.987602 fscore_macro: 0.509020 val_loss: 1.218895 val_acc: 58.738739 val_fscore_macro: 0.483441\n",
            "Epoch:  89/300 Train steps: 313 Val steps: 35 0.69s loss: 1.107888 acc: 62.007598 fscore_macro: 0.505651 val_loss: 1.231332 val_acc: 55.675676 val_fscore_macro: 0.467441\n",
            "Epoch:  90/300 Train steps: 313 Val steps: 35 0.69s loss: 1.102802 acc: 62.747451 fscore_macro: 0.511721 val_loss: 1.223882 val_acc: 57.837838 val_fscore_macro: 0.483842\n",
            "Epoch:  91/300 Train steps: 313 Val steps: 35 0.67s loss: 1.099285 acc: 61.987602 fscore_macro: 0.509816 val_loss: 1.217796 val_acc: 58.738739 val_fscore_macro: 0.499540\n",
            "Epoch:  92/300 Train steps: 313 Val steps: 35 0.65s loss: 1.096812 acc: 62.887423 fscore_macro: 0.515040 val_loss: 1.217930 val_acc: 57.657658 val_fscore_macro: 0.477431\n",
            "Epoch:  93/300 Train steps: 313 Val steps: 35 0.68s loss: 1.093612 acc: 62.547491 fscore_macro: 0.515376 val_loss: 1.213014 val_acc: 56.756757 val_fscore_macro: 0.471796\n",
            "Epoch:  94/300 Train steps: 313 Val steps: 35 0.69s loss: 1.088668 acc: 63.067387 fscore_macro: 0.524271 val_loss: 1.213797 val_acc: 58.558559 val_fscore_macro: 0.496382\n",
            "Epoch:  95/300 Train steps: 313 Val steps: 35 0.68s loss: 1.085385 acc: 63.047391 fscore_macro: 0.518529 val_loss: 1.205646 val_acc: 59.099099 val_fscore_macro: 0.508260\n",
            "Epoch:  96/300 Train steps: 313 Val steps: 35 0.69s loss: 1.085614 acc: 62.687463 fscore_macro: 0.519685 val_loss: 1.215965 val_acc: 58.198198 val_fscore_macro: 0.489591\n",
            "Epoch:  97/300 Train steps: 313 Val steps: 35 0.68s loss: 1.079700 acc: 63.307339 fscore_macro: 0.526154 val_loss: 1.245672 val_acc: 54.594595 val_fscore_macro: 0.458252\n",
            "Epoch:  98/300 Train steps: 313 Val steps: 35 0.69s loss: 1.077581 acc: 63.087383 fscore_macro: 0.526596 val_loss: 1.202023 val_acc: 58.738739 val_fscore_macro: 0.501583\n",
            "Epoch:  99/300 Train steps: 313 Val steps: 35 0.67s loss: 1.074296 acc: 62.987403 fscore_macro: 0.528070 val_loss: 1.184752 val_acc: 58.198198 val_fscore_macro: 0.489961\n",
            "Epoch: 100/300 Train steps: 313 Val steps: 35 0.69s loss: 1.070385 acc: 63.267347 fscore_macro: 0.534198 val_loss: 1.185840 val_acc: 59.279279 val_fscore_macro: 0.500932\n",
            "Epoch: 101/300 Train steps: 313 Val steps: 35 0.69s loss: 1.067257 acc: 63.107379 fscore_macro: 0.529060 val_loss: 1.186266 val_acc: 58.198198 val_fscore_macro: 0.496839\n",
            "Epoch: 102/300 Train steps: 313 Val steps: 35 0.66s loss: 1.064521 acc: 63.927215 fscore_macro: 0.541598 val_loss: 1.183250 val_acc: 60.360360 val_fscore_macro: 0.509839\n",
            "Epoch 102: val_acc improved from 59.45946 to 60.36036, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_102.ckpt\n",
            "Epoch: 103/300 Train steps: 313 Val steps: 35 0.69s loss: 1.059241 acc: 64.307139 fscore_macro: 0.544394 val_loss: 1.179646 val_acc: 58.378378 val_fscore_macro: 0.494025\n",
            "Epoch: 104/300 Train steps: 313 Val steps: 35 0.66s loss: 1.059822 acc: 63.907219 fscore_macro: 0.543064 val_loss: 1.195421 val_acc: 58.558559 val_fscore_macro: 0.495790\n",
            "Epoch: 105/300 Train steps: 313 Val steps: 35 0.67s loss: 1.054991 acc: 64.187163 fscore_macro: 0.543222 val_loss: 1.185679 val_acc: 58.378378 val_fscore_macro: 0.484767\n",
            "Epoch: 106/300 Train steps: 313 Val steps: 35 0.68s loss: 1.052809 acc: 64.007199 fscore_macro: 0.543027 val_loss: 1.176919 val_acc: 59.099099 val_fscore_macro: 0.497977\n",
            "Epoch: 107/300 Train steps: 313 Val steps: 35 0.67s loss: 1.050444 acc: 64.047191 fscore_macro: 0.547392 val_loss: 1.179763 val_acc: 57.297297 val_fscore_macro: 0.491309\n",
            "Epoch: 108/300 Train steps: 313 Val steps: 35 0.67s loss: 1.045640 acc: 64.567087 fscore_macro: 0.548615 val_loss: 1.203473 val_acc: 58.378378 val_fscore_macro: 0.486566\n",
            "Epoch: 109/300 Train steps: 313 Val steps: 35 0.68s loss: 1.044684 acc: 64.367127 fscore_macro: 0.553207 val_loss: 1.166515 val_acc: 59.099099 val_fscore_macro: 0.507452\n",
            "Epoch: 110/300 Train steps: 313 Val steps: 35 0.69s loss: 1.041605 acc: 64.827035 fscore_macro: 0.555181 val_loss: 1.178064 val_acc: 58.198198 val_fscore_macro: 0.482862\n",
            "Epoch: 111/300 Train steps: 313 Val steps: 35 0.67s loss: 1.040914 acc: 64.487103 fscore_macro: 0.553258 val_loss: 1.178768 val_acc: 57.837838 val_fscore_macro: 0.499542\n",
            "Epoch: 112/300 Train steps: 313 Val steps: 35 0.68s loss: 1.037057 acc: 64.327135 fscore_macro: 0.555552 val_loss: 1.166511 val_acc: 59.099099 val_fscore_macro: 0.501732\n",
            "Epoch: 113/300 Train steps: 313 Val steps: 35 0.68s loss: 1.035293 acc: 64.967007 fscore_macro: 0.557175 val_loss: 1.182049 val_acc: 58.198198 val_fscore_macro: 0.495867\n",
            "Epoch: 114/300 Train steps: 313 Val steps: 35 0.86s loss: 1.029899 acc: 65.066987 fscore_macro: 0.558718 val_loss: 1.156094 val_acc: 59.639640 val_fscore_macro: 0.512782\n",
            "Epoch: 115/300 Train steps: 313 Val steps: 35 0.85s loss: 1.027343 acc: 65.066987 fscore_macro: 0.561310 val_loss: 1.206272 val_acc: 58.198198 val_fscore_macro: 0.512573\n",
            "Epoch: 116/300 Train steps: 313 Val steps: 35 0.94s loss: 1.027355 acc: 65.526895 fscore_macro: 0.568853 val_loss: 1.161532 val_acc: 60.000000 val_fscore_macro: 0.511139\n",
            "Epoch: 117/300 Train steps: 313 Val steps: 35 0.85s loss: 1.022664 acc: 65.186963 fscore_macro: 0.563931 val_loss: 1.182017 val_acc: 58.558559 val_fscore_macro: 0.512120\n",
            "Epoch: 118/300 Train steps: 313 Val steps: 35 0.93s loss: 1.020032 acc: 65.246951 fscore_macro: 0.562624 val_loss: 1.152156 val_acc: 59.279279 val_fscore_macro: 0.504318\n",
            "Epoch: 119/300 Train steps: 313 Val steps: 35 0.70s loss: 1.020071 acc: 65.306939 fscore_macro: 0.571312 val_loss: 1.160525 val_acc: 60.000000 val_fscore_macro: 0.513231\n",
            "Epoch: 120/300 Train steps: 313 Val steps: 35 0.70s loss: 1.015669 acc: 65.266947 fscore_macro: 0.566005 val_loss: 1.183017 val_acc: 56.936937 val_fscore_macro: 0.494133\n",
            "Epoch: 121/300 Train steps: 313 Val steps: 35 0.68s loss: 1.016607 acc: 65.586883 fscore_macro: 0.575779 val_loss: 1.171409 val_acc: 57.477477 val_fscore_macro: 0.495832\n",
            "Epoch: 122/300 Train steps: 313 Val steps: 35 0.66s loss: 1.011097 acc: 65.806839 fscore_macro: 0.571673 val_loss: 1.150753 val_acc: 58.738739 val_fscore_macro: 0.507243\n",
            "Epoch: 123/300 Train steps: 313 Val steps: 35 0.65s loss: 1.005686 acc: 65.366927 fscore_macro: 0.571171 val_loss: 1.168215 val_acc: 58.738739 val_fscore_macro: 0.515491\n",
            "Epoch: 124/300 Train steps: 313 Val steps: 35 0.69s loss: 1.005484 acc: 65.586883 fscore_macro: 0.573238 val_loss: 1.148740 val_acc: 59.819820 val_fscore_macro: 0.512677\n",
            "Epoch: 125/300 Train steps: 313 Val steps: 35 0.71s loss: 1.006272 acc: 65.766847 fscore_macro: 0.572092 val_loss: 1.145076 val_acc: 60.360360 val_fscore_macro: 0.518100\n",
            "Epoch 125: val_acc improved from 60.36036 to 60.36036, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_125.ckpt\n",
            "Epoch: 126/300 Train steps: 313 Val steps: 35 0.70s loss: 1.003204 acc: 65.646871 fscore_macro: 0.577223 val_loss: 1.134442 val_acc: 59.639640 val_fscore_macro: 0.506272\n",
            "Epoch: 127/300 Train steps: 313 Val steps: 35 0.67s loss: 1.000175 acc: 66.206759 fscore_macro: 0.580908 val_loss: 1.137412 val_acc: 59.819820 val_fscore_macro: 0.507735\n",
            "Epoch: 128/300 Train steps: 313 Val steps: 35 0.66s loss: 0.998054 acc: 65.886823 fscore_macro: 0.579231 val_loss: 1.185294 val_acc: 57.477478 val_fscore_macro: 0.490878\n",
            "Epoch: 129/300 Train steps: 313 Val steps: 35 0.67s loss: 0.996471 acc: 66.126775 fscore_macro: 0.580399 val_loss: 1.170369 val_acc: 57.657658 val_fscore_macro: 0.490621\n",
            "Epoch: 130/300 Train steps: 313 Val steps: 35 0.67s loss: 0.992090 acc: 66.226755 fscore_macro: 0.579568 val_loss: 1.138714 val_acc: 59.459460 val_fscore_macro: 0.510244\n",
            "Epoch: 131/300 Train steps: 313 Val steps: 35 0.66s loss: 0.992085 acc: 66.426715 fscore_macro: 0.584545 val_loss: 1.134944 val_acc: 58.738739 val_fscore_macro: 0.507176\n",
            "Epoch: 132/300 Train steps: 313 Val steps: 35 0.67s loss: 0.990105 acc: 65.706859 fscore_macro: 0.574151 val_loss: 1.148720 val_acc: 60.720721 val_fscore_macro: 0.520191\n",
            "Epoch 132: val_acc improved from 60.36036 to 60.72072, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_132.ckpt\n",
            "Epoch: 133/300 Train steps: 313 Val steps: 35 0.70s loss: 0.988591 acc: 66.566687 fscore_macro: 0.588528 val_loss: 1.143567 val_acc: 58.738739 val_fscore_macro: 0.508874\n",
            "Epoch: 134/300 Train steps: 313 Val steps: 35 0.68s loss: 0.983883 acc: 66.126775 fscore_macro: 0.586794 val_loss: 1.157477 val_acc: 59.819820 val_fscore_macro: 0.515005\n",
            "Epoch: 135/300 Train steps: 313 Val steps: 35 0.70s loss: 0.985387 acc: 65.926815 fscore_macro: 0.584146 val_loss: 1.140455 val_acc: 60.720721 val_fscore_macro: 0.529636\n",
            "Epoch: 136/300 Train steps: 313 Val steps: 35 0.67s loss: 0.980274 acc: 66.646671 fscore_macro: 0.587602 val_loss: 1.147662 val_acc: 57.837838 val_fscore_macro: 0.505600\n",
            "Epoch: 137/300 Train steps: 313 Val steps: 35 0.68s loss: 0.979061 acc: 66.586683 fscore_macro: 0.588832 val_loss: 1.128322 val_acc: 60.900901 val_fscore_macro: 0.524512\n",
            "Epoch 137: val_acc improved from 60.72072 to 60.90090, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_137.ckpt\n",
            "Epoch: 138/300 Train steps: 313 Val steps: 35 0.67s loss: 0.977162 acc: 66.966607 fscore_macro: 0.595741 val_loss: 1.156429 val_acc: 59.099099 val_fscore_macro: 0.513226\n",
            "Epoch: 139/300 Train steps: 313 Val steps: 35 0.65s loss: 0.975201 acc: 66.526695 fscore_macro: 0.584668 val_loss: 1.152882 val_acc: 60.720721 val_fscore_macro: 0.533323\n",
            "Epoch: 140/300 Train steps: 313 Val steps: 35 0.69s loss: 0.973984 acc: 66.746651 fscore_macro: 0.591589 val_loss: 1.170648 val_acc: 58.558559 val_fscore_macro: 0.523451\n",
            "Epoch: 141/300 Train steps: 313 Val steps: 35 0.67s loss: 0.971270 acc: 66.666667 fscore_macro: 0.589791 val_loss: 1.167039 val_acc: 58.738739 val_fscore_macro: 0.523484\n",
            "Epoch: 142/300 Train steps: 313 Val steps: 35 0.68s loss: 0.970592 acc: 66.746651 fscore_macro: 0.592835 val_loss: 1.146996 val_acc: 59.459459 val_fscore_macro: 0.524036\n",
            "Epoch: 143/300 Train steps: 313 Val steps: 35 0.67s loss: 0.967516 acc: 66.546691 fscore_macro: 0.592544 val_loss: 1.130865 val_acc: 59.819820 val_fscore_macro: 0.524286\n",
            "Epoch: 144/300 Train steps: 313 Val steps: 35 0.67s loss: 0.966638 acc: 66.846631 fscore_macro: 0.590933 val_loss: 1.155541 val_acc: 58.738739 val_fscore_macro: 0.509748\n",
            "Epoch: 145/300 Train steps: 313 Val steps: 35 0.65s loss: 0.964026 acc: 67.546491 fscore_macro: 0.600008 val_loss: 1.127179 val_acc: 60.540541 val_fscore_macro: 0.520932\n",
            "Epoch: 146/300 Train steps: 313 Val steps: 35 0.65s loss: 0.960199 acc: 67.366527 fscore_macro: 0.599934 val_loss: 1.127086 val_acc: 60.720721 val_fscore_macro: 0.530436\n",
            "Epoch: 147/300 Train steps: 313 Val steps: 35 0.70s loss: 0.958506 acc: 67.206559 fscore_macro: 0.602858 val_loss: 1.179726 val_acc: 57.117117 val_fscore_macro: 0.515865\n",
            "Epoch: 148/300 Train steps: 313 Val steps: 35 0.68s loss: 0.956295 acc: 67.166567 fscore_macro: 0.596683 val_loss: 1.132485 val_acc: 59.099099 val_fscore_macro: 0.509759\n",
            "Epoch: 149/300 Train steps: 313 Val steps: 35 0.68s loss: 0.956308 acc: 67.206559 fscore_macro: 0.600683 val_loss: 1.143014 val_acc: 58.558559 val_fscore_macro: 0.512417\n",
            "Epoch: 150/300 Train steps: 313 Val steps: 35 0.66s loss: 0.954250 acc: 66.866627 fscore_macro: 0.594608 val_loss: 1.117676 val_acc: 60.000000 val_fscore_macro: 0.523649\n",
            "Epoch: 151/300 Train steps: 313 Val steps: 35 0.68s loss: 0.952811 acc: 66.926615 fscore_macro: 0.600668 val_loss: 1.173211 val_acc: 58.198198 val_fscore_macro: 0.507971\n",
            "Epoch: 152/300 Train steps: 313 Val steps: 35 0.68s loss: 0.950905 acc: 67.506499 fscore_macro: 0.603560 val_loss: 1.132403 val_acc: 58.918919 val_fscore_macro: 0.499492\n",
            "Epoch: 153/300 Train steps: 313 Val steps: 35 0.68s loss: 0.947155 acc: 67.406519 fscore_macro: 0.606714 val_loss: 1.113653 val_acc: 59.099099 val_fscore_macro: 0.522696\n",
            "Epoch: 154/300 Train steps: 313 Val steps: 35 0.68s loss: 0.946035 acc: 67.246551 fscore_macro: 0.603907 val_loss: 1.139260 val_acc: 58.378378 val_fscore_macro: 0.503313\n",
            "Epoch: 155/300 Train steps: 313 Val steps: 35 0.67s loss: 0.942646 acc: 67.926415 fscore_macro: 0.606773 val_loss: 1.124938 val_acc: 58.198198 val_fscore_macro: 0.510063\n",
            "Epoch: 156/300 Train steps: 313 Val steps: 35 0.68s loss: 0.945464 acc: 67.666467 fscore_macro: 0.609390 val_loss: 1.150893 val_acc: 60.360360 val_fscore_macro: 0.534970\n",
            "Epoch: 157/300 Train steps: 313 Val steps: 35 0.67s loss: 0.944062 acc: 67.546491 fscore_macro: 0.612211 val_loss: 1.112297 val_acc: 60.180180 val_fscore_macro: 0.530328\n",
            "Epoch: 158/300 Train steps: 313 Val steps: 35 0.68s loss: 0.937784 acc: 67.646471 fscore_macro: 0.610983 val_loss: 1.110308 val_acc: 60.360360 val_fscore_macro: 0.521734\n",
            "Epoch: 159/300 Train steps: 313 Val steps: 35 0.67s loss: 0.939249 acc: 68.146371 fscore_macro: 0.612465 val_loss: 1.120919 val_acc: 60.540541 val_fscore_macro: 0.534043\n",
            "Epoch: 160/300 Train steps: 313 Val steps: 35 0.67s loss: 0.936821 acc: 67.666467 fscore_macro: 0.611515 val_loss: 1.124459 val_acc: 59.639640 val_fscore_macro: 0.521022\n",
            "Epoch: 161/300 Train steps: 313 Val steps: 35 0.66s loss: 0.935144 acc: 68.046391 fscore_macro: 0.616502 val_loss: 1.139235 val_acc: 59.279279 val_fscore_macro: 0.527944\n",
            "Epoch: 162/300 Train steps: 313 Val steps: 35 0.67s loss: 0.928475 acc: 68.186363 fscore_macro: 0.618247 val_loss: 1.107918 val_acc: 60.900901 val_fscore_macro: 0.527457\n",
            "Epoch 162: val_acc improved from 60.90090 to 60.90090, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_162.ckpt\n",
            "Epoch: 163/300 Train steps: 313 Val steps: 35 0.65s loss: 0.928751 acc: 68.206359 fscore_macro: 0.618394 val_loss: 1.121927 val_acc: 60.000000 val_fscore_macro: 0.517652\n",
            "Epoch: 164/300 Train steps: 313 Val steps: 35 0.67s loss: 0.932821 acc: 67.766447 fscore_macro: 0.615871 val_loss: 1.134829 val_acc: 59.639640 val_fscore_macro: 0.512734\n",
            "Epoch: 165/300 Train steps: 313 Val steps: 35 0.69s loss: 0.928559 acc: 68.046391 fscore_macro: 0.618381 val_loss: 1.122692 val_acc: 62.702703 val_fscore_macro: 0.546041\n",
            "Epoch 165: val_acc improved from 60.90090 to 62.70270, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_165.ckpt\n",
            "Epoch: 166/300 Train steps: 313 Val steps: 35 0.74s loss: 0.928039 acc: 68.266347 fscore_macro: 0.618907 val_loss: 1.118915 val_acc: 61.081081 val_fscore_macro: 0.533411\n",
            "Epoch: 167/300 Train steps: 313 Val steps: 35 0.70s loss: 0.923884 acc: 68.266347 fscore_macro: 0.618798 val_loss: 1.140173 val_acc: 60.720721 val_fscore_macro: 0.539947\n",
            "Epoch: 168/300 Train steps: 313 Val steps: 35 0.66s loss: 0.920332 acc: 68.846231 fscore_macro: 0.624937 val_loss: 1.118814 val_acc: 61.801802 val_fscore_macro: 0.533614\n",
            "Epoch: 169/300 Train steps: 313 Val steps: 35 0.69s loss: 0.921793 acc: 68.286343 fscore_macro: 0.624987 val_loss: 1.109198 val_acc: 60.540541 val_fscore_macro: 0.533858\n",
            "Epoch: 170/300 Train steps: 313 Val steps: 35 0.65s loss: 0.919887 acc: 68.266347 fscore_macro: 0.620865 val_loss: 1.160344 val_acc: 57.657658 val_fscore_macro: 0.517516\n",
            "Epoch: 171/300 Train steps: 313 Val steps: 35 0.67s loss: 0.918204 acc: 68.346331 fscore_macro: 0.624420 val_loss: 1.134190 val_acc: 59.639640 val_fscore_macro: 0.524140\n",
            "Epoch: 172/300 Train steps: 313 Val steps: 35 0.66s loss: 0.917621 acc: 68.726255 fscore_macro: 0.627455 val_loss: 1.102552 val_acc: 60.540541 val_fscore_macro: 0.525689\n",
            "Epoch: 173/300 Train steps: 313 Val steps: 35 0.67s loss: 0.915207 acc: 68.646271 fscore_macro: 0.629781 val_loss: 1.126836 val_acc: 59.099099 val_fscore_macro: 0.524142\n",
            "Epoch: 174/300 Train steps: 313 Val steps: 35 0.69s loss: 0.911196 acc: 68.726255 fscore_macro: 0.627777 val_loss: 1.134667 val_acc: 60.180180 val_fscore_macro: 0.527314\n",
            "Epoch: 175/300 Train steps: 313 Val steps: 35 0.68s loss: 0.912330 acc: 68.666267 fscore_macro: 0.628197 val_loss: 1.115408 val_acc: 61.261261 val_fscore_macro: 0.538109\n",
            "Epoch: 176/300 Train steps: 313 Val steps: 35 0.67s loss: 0.914571 acc: 68.806239 fscore_macro: 0.621957 val_loss: 1.104268 val_acc: 60.180180 val_fscore_macro: 0.544879\n",
            "Epoch: 177/300 Train steps: 313 Val steps: 35 0.68s loss: 0.908476 acc: 68.646271 fscore_macro: 0.629579 val_loss: 1.124290 val_acc: 58.558559 val_fscore_macro: 0.513176\n",
            "Epoch: 178/300 Train steps: 313 Val steps: 35 0.69s loss: 0.907117 acc: 69.306139 fscore_macro: 0.628355 val_loss: 1.117676 val_acc: 62.162162 val_fscore_macro: 0.546641\n",
            "Epoch: 179/300 Train steps: 313 Val steps: 35 0.68s loss: 0.906831 acc: 69.006199 fscore_macro: 0.629874 val_loss: 1.097420 val_acc: 61.441441 val_fscore_macro: 0.527996\n",
            "Epoch: 180/300 Train steps: 313 Val steps: 35 0.68s loss: 0.903422 acc: 69.006199 fscore_macro: 0.628744 val_loss: 1.126895 val_acc: 59.639640 val_fscore_macro: 0.532579\n",
            "Epoch: 181/300 Train steps: 313 Val steps: 35 0.68s loss: 0.904715 acc: 68.786243 fscore_macro: 0.631192 val_loss: 1.112754 val_acc: 59.819820 val_fscore_macro: 0.528691\n",
            "Epoch: 182/300 Train steps: 313 Val steps: 35 0.69s loss: 0.902819 acc: 68.826235 fscore_macro: 0.630135 val_loss: 1.097003 val_acc: 60.180180 val_fscore_macro: 0.548054\n",
            "Epoch: 183/300 Train steps: 313 Val steps: 35 0.68s loss: 0.898977 acc: 68.826235 fscore_macro: 0.633518 val_loss: 1.180712 val_acc: 58.198198 val_fscore_macro: 0.516355\n",
            "Epoch: 184/300 Train steps: 313 Val steps: 35 0.68s loss: 0.899398 acc: 68.706259 fscore_macro: 0.631948 val_loss: 1.110930 val_acc: 59.279279 val_fscore_macro: 0.531856\n",
            "Epoch: 185/300 Train steps: 313 Val steps: 35 0.66s loss: 0.895421 acc: 69.326135 fscore_macro: 0.635611 val_loss: 1.102155 val_acc: 60.720721 val_fscore_macro: 0.559699\n",
            "Epoch: 186/300 Train steps: 313 Val steps: 35 0.66s loss: 0.894778 acc: 69.146171 fscore_macro: 0.637327 val_loss: 1.118582 val_acc: 59.639640 val_fscore_macro: 0.547645\n",
            "Epoch: 187/300 Train steps: 313 Val steps: 35 0.69s loss: 0.895077 acc: 69.266147 fscore_macro: 0.635271 val_loss: 1.112392 val_acc: 58.738739 val_fscore_macro: 0.532999\n",
            "Epoch: 188/300 Train steps: 313 Val steps: 35 0.68s loss: 0.890763 acc: 69.466107 fscore_macro: 0.641057 val_loss: 1.129199 val_acc: 60.540541 val_fscore_macro: 0.553688\n",
            "Epoch: 189/300 Train steps: 313 Val steps: 35 0.67s loss: 0.890369 acc: 69.366127 fscore_macro: 0.641362 val_loss: 1.109625 val_acc: 61.621622 val_fscore_macro: 0.541538\n",
            "Epoch: 190/300 Train steps: 313 Val steps: 35 0.68s loss: 0.883850 acc: 70.025995 fscore_macro: 0.646073 val_loss: 1.133427 val_acc: 57.837838 val_fscore_macro: 0.525179\n",
            "Epoch: 191/300 Train steps: 313 Val steps: 35 0.68s loss: 0.887532 acc: 69.466107 fscore_macro: 0.638370 val_loss: 1.112511 val_acc: 60.180180 val_fscore_macro: 0.530501\n",
            "Epoch: 192/300 Train steps: 313 Val steps: 35 0.68s loss: 0.887731 acc: 69.106179 fscore_macro: 0.641280 val_loss: 1.107880 val_acc: 58.558559 val_fscore_macro: 0.529298\n",
            "Epoch: 193/300 Train steps: 313 Val steps: 35 0.68s loss: 0.884873 acc: 69.386123 fscore_macro: 0.641899 val_loss: 1.089514 val_acc: 61.801802 val_fscore_macro: 0.551677\n",
            "Epoch: 194/300 Train steps: 313 Val steps: 35 0.69s loss: 0.881412 acc: 69.966007 fscore_macro: 0.652686 val_loss: 1.102299 val_acc: 59.819820 val_fscore_macro: 0.533562\n",
            "Epoch: 195/300 Train steps: 313 Val steps: 35 0.68s loss: 0.881627 acc: 69.506099 fscore_macro: 0.643247 val_loss: 1.099889 val_acc: 62.162162 val_fscore_macro: 0.540606\n",
            "Epoch: 196/300 Train steps: 313 Val steps: 35 0.70s loss: 0.877744 acc: 69.806039 fscore_macro: 0.649893 val_loss: 1.133217 val_acc: 58.198198 val_fscore_macro: 0.518372\n",
            "Epoch: 197/300 Train steps: 313 Val steps: 35 0.69s loss: 0.879666 acc: 69.666067 fscore_macro: 0.643459 val_loss: 1.120457 val_acc: 59.279279 val_fscore_macro: 0.529362\n",
            "Epoch: 198/300 Train steps: 313 Val steps: 35 0.70s loss: 0.877020 acc: 69.826035 fscore_macro: 0.646278 val_loss: 1.091075 val_acc: 62.522523 val_fscore_macro: 0.562527\n",
            "Epoch: 199/300 Train steps: 313 Val steps: 35 0.80s loss: 0.876560 acc: 69.726055 fscore_macro: 0.649346 val_loss: 1.128169 val_acc: 59.639640 val_fscore_macro: 0.527898\n",
            "Epoch: 200/300 Train steps: 313 Val steps: 35 0.91s loss: 0.876404 acc: 69.866027 fscore_macro: 0.648541 val_loss: 1.124865 val_acc: 60.360360 val_fscore_macro: 0.549329\n",
            "Epoch: 201/300 Train steps: 313 Val steps: 35 0.96s loss: 0.872093 acc: 70.025995 fscore_macro: 0.650922 val_loss: 1.094868 val_acc: 60.360360 val_fscore_macro: 0.526093\n",
            "Epoch: 202/300 Train steps: 313 Val steps: 35 0.87s loss: 0.871441 acc: 70.505899 fscore_macro: 0.656690 val_loss: 1.094033 val_acc: 61.441441 val_fscore_macro: 0.542191\n",
            "Epoch: 203/300 Train steps: 313 Val steps: 35 0.99s loss: 0.870794 acc: 69.566087 fscore_macro: 0.647626 val_loss: 1.093313 val_acc: 61.981982 val_fscore_macro: 0.560461\n",
            "Epoch: 204/300 Train steps: 313 Val steps: 35 0.67s loss: 0.867543 acc: 70.225955 fscore_macro: 0.652130 val_loss: 1.098552 val_acc: 61.081081 val_fscore_macro: 0.554365\n",
            "Epoch: 205/300 Train steps: 313 Val steps: 35 0.67s loss: 0.867027 acc: 69.786043 fscore_macro: 0.649422 val_loss: 1.102817 val_acc: 61.441441 val_fscore_macro: 0.541679\n",
            "Epoch: 206/300 Train steps: 313 Val steps: 35 0.66s loss: 0.865482 acc: 70.185963 fscore_macro: 0.651056 val_loss: 1.110010 val_acc: 61.981982 val_fscore_macro: 0.556916\n",
            "Epoch: 207/300 Train steps: 313 Val steps: 35 0.67s loss: 0.864391 acc: 70.605879 fscore_macro: 0.660354 val_loss: 1.097153 val_acc: 59.099099 val_fscore_macro: 0.534533\n",
            "Epoch: 208/300 Train steps: 313 Val steps: 35 0.70s loss: 0.864736 acc: 70.065987 fscore_macro: 0.656948 val_loss: 1.098679 val_acc: 60.180180 val_fscore_macro: 0.536214\n",
            "Epoch: 209/300 Train steps: 313 Val steps: 35 0.65s loss: 0.859889 acc: 70.025995 fscore_macro: 0.660645 val_loss: 1.128667 val_acc: 60.900901 val_fscore_macro: 0.540686\n",
            "Epoch: 210/300 Train steps: 313 Val steps: 35 0.67s loss: 0.863616 acc: 70.145971 fscore_macro: 0.656514 val_loss: 1.094455 val_acc: 62.882883 val_fscore_macro: 0.555309\n",
            "Epoch 210: val_acc improved from 62.70270 to 62.88288, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_210.ckpt\n",
            "Epoch: 211/300 Train steps: 313 Val steps: 35 0.69s loss: 0.857782 acc: 70.905819 fscore_macro: 0.659751 val_loss: 1.108273 val_acc: 60.180180 val_fscore_macro: 0.564546\n",
            "Epoch: 212/300 Train steps: 313 Val steps: 35 0.69s loss: 0.860520 acc: 70.505899 fscore_macro: 0.660742 val_loss: 1.102108 val_acc: 60.000000 val_fscore_macro: 0.524568\n",
            "Epoch: 213/300 Train steps: 313 Val steps: 35 0.65s loss: 0.855345 acc: 70.805839 fscore_macro: 0.658251 val_loss: 1.100328 val_acc: 60.540541 val_fscore_macro: 0.532117\n",
            "Epoch: 214/300 Train steps: 313 Val steps: 35 0.68s loss: 0.855692 acc: 70.725855 fscore_macro: 0.660862 val_loss: 1.110617 val_acc: 60.000000 val_fscore_macro: 0.559568\n",
            "Epoch: 215/300 Train steps: 313 Val steps: 35 0.66s loss: 0.853464 acc: 70.565887 fscore_macro: 0.657320 val_loss: 1.127443 val_acc: 60.540541 val_fscore_macro: 0.536782\n",
            "Epoch: 216/300 Train steps: 313 Val steps: 35 0.68s loss: 0.855283 acc: 70.065987 fscore_macro: 0.655704 val_loss: 1.111459 val_acc: 60.720721 val_fscore_macro: 0.538546\n",
            "Epoch: 217/300 Train steps: 313 Val steps: 35 0.68s loss: 0.850855 acc: 70.585883 fscore_macro: 0.657748 val_loss: 1.085382 val_acc: 61.441441 val_fscore_macro: 0.550435\n",
            "Epoch: 218/300 Train steps: 313 Val steps: 35 0.69s loss: 0.847958 acc: 70.885823 fscore_macro: 0.667831 val_loss: 1.096726 val_acc: 59.819820 val_fscore_macro: 0.534265\n",
            "Epoch: 219/300 Train steps: 313 Val steps: 35 0.69s loss: 0.846018 acc: 71.065787 fscore_macro: 0.662109 val_loss: 1.107152 val_acc: 60.360360 val_fscore_macro: 0.548803\n",
            "Epoch: 220/300 Train steps: 313 Val steps: 35 0.66s loss: 0.849287 acc: 70.585883 fscore_macro: 0.665018 val_loss: 1.117523 val_acc: 61.441441 val_fscore_macro: 0.540032\n",
            "Epoch: 221/300 Train steps: 313 Val steps: 35 0.68s loss: 0.846397 acc: 70.885823 fscore_macro: 0.667993 val_loss: 1.151190 val_acc: 58.918919 val_fscore_macro: 0.526694\n",
            "Epoch: 222/300 Train steps: 313 Val steps: 35 0.68s loss: 0.841547 acc: 70.845831 fscore_macro: 0.668790 val_loss: 1.149459 val_acc: 58.198198 val_fscore_macro: 0.499376\n",
            "Epoch: 223/300 Train steps: 313 Val steps: 35 0.65s loss: 0.844305 acc: 70.765847 fscore_macro: 0.667716 val_loss: 1.085487 val_acc: 62.522523 val_fscore_macro: 0.559626\n",
            "Epoch: 224/300 Train steps: 313 Val steps: 35 0.67s loss: 0.840025 acc: 70.565887 fscore_macro: 0.662417 val_loss: 1.093157 val_acc: 61.981982 val_fscore_macro: 0.570784\n",
            "Epoch: 225/300 Train steps: 313 Val steps: 35 0.68s loss: 0.840099 acc: 70.765847 fscore_macro: 0.664817 val_loss: 1.095627 val_acc: 60.000000 val_fscore_macro: 0.523126\n",
            "Epoch: 226/300 Train steps: 313 Val steps: 35 0.67s loss: 0.836173 acc: 71.005799 fscore_macro: 0.672757 val_loss: 1.099870 val_acc: 61.801802 val_fscore_macro: 0.559543\n",
            "Epoch: 227/300 Train steps: 313 Val steps: 35 0.69s loss: 0.839003 acc: 70.825835 fscore_macro: 0.667208 val_loss: 1.107839 val_acc: 61.621622 val_fscore_macro: 0.579273\n",
            "Epoch: 228/300 Train steps: 313 Val steps: 35 0.67s loss: 0.835275 acc: 71.085783 fscore_macro: 0.670584 val_loss: 1.096304 val_acc: 61.801802 val_fscore_macro: 0.550973\n",
            "Epoch: 229/300 Train steps: 313 Val steps: 35 0.68s loss: 0.835472 acc: 71.065787 fscore_macro: 0.668628 val_loss: 1.128491 val_acc: 59.459460 val_fscore_macro: 0.562200\n",
            "Epoch: 230/300 Train steps: 313 Val steps: 35 0.68s loss: 0.833360 acc: 71.245751 fscore_macro: 0.668166 val_loss: 1.122682 val_acc: 60.000000 val_fscore_macro: 0.545145\n",
            "Epoch: 231/300 Train steps: 313 Val steps: 35 0.69s loss: 0.830078 acc: 71.225755 fscore_macro: 0.676325 val_loss: 1.128317 val_acc: 60.360360 val_fscore_macro: 0.549153\n",
            "Epoch: 232/300 Train steps: 313 Val steps: 35 0.70s loss: 0.832123 acc: 70.845831 fscore_macro: 0.670309 val_loss: 1.086745 val_acc: 61.081081 val_fscore_macro: 0.569092\n",
            "Epoch: 233/300 Train steps: 313 Val steps: 35 0.68s loss: 0.831469 acc: 71.345731 fscore_macro: 0.670852 val_loss: 1.089930 val_acc: 61.441441 val_fscore_macro: 0.574343\n",
            "Epoch: 234/300 Train steps: 313 Val steps: 35 0.67s loss: 0.828564 acc: 71.865627 fscore_macro: 0.675852 val_loss: 1.107178 val_acc: 59.819820 val_fscore_macro: 0.552547\n",
            "Epoch: 235/300 Train steps: 313 Val steps: 35 0.69s loss: 0.827663 acc: 71.325735 fscore_macro: 0.672047 val_loss: 1.093871 val_acc: 61.981982 val_fscore_macro: 0.552383\n",
            "Epoch: 236/300 Train steps: 313 Val steps: 35 0.68s loss: 0.825437 acc: 71.965607 fscore_macro: 0.684073 val_loss: 1.106013 val_acc: 61.441441 val_fscore_macro: 0.555359\n",
            "Epoch: 237/300 Train steps: 313 Val steps: 35 0.65s loss: 0.823751 acc: 71.825635 fscore_macro: 0.679887 val_loss: 1.087237 val_acc: 62.342342 val_fscore_macro: 0.582371\n",
            "Epoch: 238/300 Train steps: 313 Val steps: 35 0.67s loss: 0.824531 acc: 72.245551 fscore_macro: 0.685894 val_loss: 1.131069 val_acc: 59.639640 val_fscore_macro: 0.546559\n",
            "Epoch: 239/300 Train steps: 313 Val steps: 35 0.68s loss: 0.823290 acc: 71.705659 fscore_macro: 0.680853 val_loss: 1.089498 val_acc: 61.441441 val_fscore_macro: 0.560814\n",
            "Epoch: 240/300 Train steps: 313 Val steps: 35 0.68s loss: 0.820520 acc: 71.645671 fscore_macro: 0.677617 val_loss: 1.119785 val_acc: 60.900901 val_fscore_macro: 0.548692\n",
            "Epoch: 241/300 Train steps: 313 Val steps: 35 0.69s loss: 0.820602 acc: 72.125575 fscore_macro: 0.676965 val_loss: 1.108991 val_acc: 61.441441 val_fscore_macro: 0.564273\n",
            "Epoch: 242/300 Train steps: 313 Val steps: 35 0.68s loss: 0.818736 acc: 72.165567 fscore_macro: 0.684955 val_loss: 1.099969 val_acc: 59.819820 val_fscore_macro: 0.537476\n",
            "Epoch: 243/300 Train steps: 313 Val steps: 35 0.67s loss: 0.817802 acc: 71.705659 fscore_macro: 0.681959 val_loss: 1.109613 val_acc: 60.000000 val_fscore_macro: 0.541706\n",
            "Epoch: 244/300 Train steps: 313 Val steps: 35 0.67s loss: 0.812830 acc: 72.085583 fscore_macro: 0.678996 val_loss: 1.085799 val_acc: 63.603604 val_fscore_macro: 0.593123\n",
            "Epoch 244: val_acc improved from 62.88288 to 63.60360, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_244.ckpt\n",
            "Epoch: 245/300 Train steps: 313 Val steps: 35 0.68s loss: 0.811829 acc: 72.385523 fscore_macro: 0.685384 val_loss: 1.085880 val_acc: 60.540541 val_fscore_macro: 0.550649\n",
            "Epoch: 246/300 Train steps: 313 Val steps: 35 0.77s loss: 0.815602 acc: 71.925615 fscore_macro: 0.676913 val_loss: 1.078541 val_acc: 62.522523 val_fscore_macro: 0.576689\n",
            "Epoch: 247/300 Train steps: 313 Val steps: 35 0.73s loss: 0.813948 acc: 72.185563 fscore_macro: 0.688311 val_loss: 1.096008 val_acc: 62.342342 val_fscore_macro: 0.586665\n",
            "Epoch: 248/300 Train steps: 313 Val steps: 35 0.69s loss: 0.810783 acc: 72.145571 fscore_macro: 0.686172 val_loss: 1.109379 val_acc: 62.162162 val_fscore_macro: 0.584202\n",
            "Epoch: 249/300 Train steps: 313 Val steps: 35 0.67s loss: 0.807168 acc: 72.005599 fscore_macro: 0.680604 val_loss: 1.098744 val_acc: 60.720721 val_fscore_macro: 0.548481\n",
            "Epoch: 250/300 Train steps: 313 Val steps: 35 0.68s loss: 0.803978 acc: 72.145571 fscore_macro: 0.682884 val_loss: 1.126949 val_acc: 61.261261 val_fscore_macro: 0.560577\n",
            "Epoch: 251/300 Train steps: 313 Val steps: 35 0.67s loss: 0.807371 acc: 71.985603 fscore_macro: 0.680667 val_loss: 1.095478 val_acc: 61.801802 val_fscore_macro: 0.575525\n",
            "Epoch: 252/300 Train steps: 313 Val steps: 35 0.65s loss: 0.803881 acc: 72.245551 fscore_macro: 0.682235 val_loss: 1.079071 val_acc: 61.801802 val_fscore_macro: 0.584488\n",
            "Epoch: 253/300 Train steps: 313 Val steps: 35 0.68s loss: 0.803215 acc: 72.385523 fscore_macro: 0.687589 val_loss: 1.098217 val_acc: 60.720721 val_fscore_macro: 0.544331\n",
            "Epoch: 254/300 Train steps: 313 Val steps: 35 0.73s loss: 0.801216 acc: 72.625475 fscore_macro: 0.688787 val_loss: 1.094020 val_acc: 60.900901 val_fscore_macro: 0.546842\n",
            "Epoch: 255/300 Train steps: 313 Val steps: 35 0.69s loss: 0.800892 acc: 72.185563 fscore_macro: 0.682504 val_loss: 1.089894 val_acc: 61.801802 val_fscore_macro: 0.574645\n",
            "Epoch: 256/300 Train steps: 313 Val steps: 35 0.67s loss: 0.799314 acc: 72.105579 fscore_macro: 0.686279 val_loss: 1.115975 val_acc: 61.081081 val_fscore_macro: 0.579251\n",
            "Epoch: 257/300 Train steps: 313 Val steps: 35 0.66s loss: 0.798875 acc: 72.145571 fscore_macro: 0.690463 val_loss: 1.085777 val_acc: 61.801802 val_fscore_macro: 0.579788\n",
            "Epoch: 258/300 Train steps: 313 Val steps: 35 0.68s loss: 0.797361 acc: 72.145571 fscore_macro: 0.692249 val_loss: 1.127983 val_acc: 61.801802 val_fscore_macro: 0.570377\n",
            "Epoch: 259/300 Train steps: 313 Val steps: 35 0.67s loss: 0.799329 acc: 71.665667 fscore_macro: 0.684032 val_loss: 1.114202 val_acc: 61.081081 val_fscore_macro: 0.578998\n",
            "Epoch: 260/300 Train steps: 313 Val steps: 35 0.67s loss: 0.795267 acc: 72.545491 fscore_macro: 0.691633 val_loss: 1.122536 val_acc: 59.639640 val_fscore_macro: 0.535686\n",
            "Epoch: 261/300 Train steps: 313 Val steps: 35 0.67s loss: 0.793776 acc: 72.845431 fscore_macro: 0.696546 val_loss: 1.102059 val_acc: 60.180180 val_fscore_macro: 0.564534\n",
            "Epoch: 262/300 Train steps: 313 Val steps: 35 0.68s loss: 0.796874 acc: 72.565487 fscore_macro: 0.694479 val_loss: 1.092306 val_acc: 62.522523 val_fscore_macro: 0.591631\n",
            "Epoch: 263/300 Train steps: 313 Val steps: 35 0.69s loss: 0.791504 acc: 72.625475 fscore_macro: 0.694154 val_loss: 1.098332 val_acc: 62.162162 val_fscore_macro: 0.583149\n",
            "Epoch: 264/300 Train steps: 313 Val steps: 35 0.66s loss: 0.789665 acc: 72.825435 fscore_macro: 0.698288 val_loss: 1.135685 val_acc: 60.000000 val_fscore_macro: 0.540213\n",
            "Epoch: 265/300 Train steps: 313 Val steps: 35 0.67s loss: 0.786700 acc: 72.785443 fscore_macro: 0.693900 val_loss: 1.102321 val_acc: 63.963964 val_fscore_macro: 0.600088\n",
            "Epoch 265: val_acc improved from 63.60360 to 63.96396, saving file to model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_265.ckpt\n",
            "Epoch: 266/300 Train steps: 313 Val steps: 35 0.69s loss: 0.788688 acc: 72.425515 fscore_macro: 0.692308 val_loss: 1.091225 val_acc: 61.981982 val_fscore_macro: 0.554098\n",
            "Epoch: 267/300 Train steps: 313 Val steps: 35 0.70s loss: 0.787380 acc: 72.605479 fscore_macro: 0.695125 val_loss: 1.106261 val_acc: 60.720721 val_fscore_macro: 0.567109\n",
            "Epoch: 268/300 Train steps: 313 Val steps: 35 0.66s loss: 0.782679 acc: 73.125375 fscore_macro: 0.702522 val_loss: 1.107365 val_acc: 62.162162 val_fscore_macro: 0.584682\n",
            "Epoch: 269/300 Train steps: 313 Val steps: 35 0.68s loss: 0.785554 acc: 72.925415 fscore_macro: 0.699329 val_loss: 1.103321 val_acc: 60.180180 val_fscore_macro: 0.548815\n",
            "Epoch: 270/300 Train steps: 313 Val steps: 35 0.69s loss: 0.784054 acc: 72.785443 fscore_macro: 0.698219 val_loss: 1.107200 val_acc: 61.261261 val_fscore_macro: 0.558136\n",
            "Epoch: 271/300 Train steps: 313 Val steps: 35 0.69s loss: 0.781498 acc: 73.045391 fscore_macro: 0.696016 val_loss: 1.085735 val_acc: 60.720721 val_fscore_macro: 0.559990\n",
            "Epoch: 272/300 Train steps: 313 Val steps: 35 0.69s loss: 0.779929 acc: 72.885423 fscore_macro: 0.696408 val_loss: 1.120238 val_acc: 61.441441 val_fscore_macro: 0.575991\n",
            "Epoch: 273/300 Train steps: 313 Val steps: 35 0.67s loss: 0.781378 acc: 73.085383 fscore_macro: 0.702418 val_loss: 1.097702 val_acc: 60.360360 val_fscore_macro: 0.549012\n",
            "Epoch: 274/300 Train steps: 313 Val steps: 35 0.69s loss: 0.776589 acc: 73.605279 fscore_macro: 0.705956 val_loss: 1.106994 val_acc: 59.819820 val_fscore_macro: 0.554247\n",
            "Epoch: 275/300 Train steps: 313 Val steps: 35 0.66s loss: 0.777289 acc: 73.165367 fscore_macro: 0.703073 val_loss: 1.080571 val_acc: 61.801802 val_fscore_macro: 0.570558\n",
            "Epoch: 276/300 Train steps: 313 Val steps: 35 0.70s loss: 0.774758 acc: 73.305339 fscore_macro: 0.700701 val_loss: 1.126567 val_acc: 61.261261 val_fscore_macro: 0.582468\n",
            "Epoch: 277/300 Train steps: 313 Val steps: 35 0.68s loss: 0.776003 acc: 73.045391 fscore_macro: 0.700569 val_loss: 1.104437 val_acc: 61.801802 val_fscore_macro: 0.551515\n",
            "Epoch: 278/300 Train steps: 313 Val steps: 35 0.67s loss: 0.771198 acc: 73.385323 fscore_macro: 0.701533 val_loss: 1.160622 val_acc: 60.900901 val_fscore_macro: 0.579882\n",
            "Epoch: 279/300 Train steps: 313 Val steps: 35 0.68s loss: 0.771766 acc: 73.025395 fscore_macro: 0.702409 val_loss: 1.123015 val_acc: 60.900901 val_fscore_macro: 0.572338\n",
            "Epoch: 280/300 Train steps: 313 Val steps: 35 0.71s loss: 0.771303 acc: 73.265347 fscore_macro: 0.709287 val_loss: 1.116315 val_acc: 60.900901 val_fscore_macro: 0.578315\n",
            "Epoch: 281/300 Train steps: 313 Val steps: 35 0.87s loss: 0.769592 acc: 73.165367 fscore_macro: 0.701126 val_loss: 1.076317 val_acc: 62.702703 val_fscore_macro: 0.588467\n",
            "Epoch: 282/300 Train steps: 313 Val steps: 35 0.89s loss: 0.768202 acc: 73.365327 fscore_macro: 0.706067 val_loss: 1.158794 val_acc: 57.117117 val_fscore_macro: 0.548844\n",
            "Epoch: 283/300 Train steps: 313 Val steps: 35 0.90s loss: 0.768979 acc: 73.765247 fscore_macro: 0.705738 val_loss: 1.084964 val_acc: 61.261261 val_fscore_macro: 0.568587\n",
            "Epoch: 284/300 Train steps: 313 Val steps: 35 0.91s loss: 0.767360 acc: 73.265347 fscore_macro: 0.706407 val_loss: 1.102926 val_acc: 61.981982 val_fscore_macro: 0.586778\n",
            "Epoch: 285/300 Train steps: 313 Val steps: 35 0.95s loss: 0.764218 acc: 73.865227 fscore_macro: 0.715403 val_loss: 1.122271 val_acc: 59.279279 val_fscore_macro: 0.555827\n",
            "Epoch: 286/300 Train steps: 313 Val steps: 35 0.68s loss: 0.762574 acc: 73.445311 fscore_macro: 0.704585 val_loss: 1.104254 val_acc: 61.261261 val_fscore_macro: 0.552797\n",
            "Epoch: 287/300 Train steps: 313 Val steps: 35 0.72s loss: 0.756352 acc: 73.705259 fscore_macro: 0.707781 val_loss: 1.079455 val_acc: 61.801802 val_fscore_macro: 0.581086\n",
            "Epoch: 288/300 Train steps: 313 Val steps: 35 0.66s loss: 0.761652 acc: 73.665267 fscore_macro: 0.707341 val_loss: 1.111699 val_acc: 60.360360 val_fscore_macro: 0.556530\n",
            "Epoch: 289/300 Train steps: 313 Val steps: 35 0.68s loss: 0.759297 acc: 73.965207 fscore_macro: 0.711687 val_loss: 1.106876 val_acc: 62.882883 val_fscore_macro: 0.582796\n",
            "Epoch: 290/300 Train steps: 313 Val steps: 35 0.69s loss: 0.759340 acc: 73.445311 fscore_macro: 0.710284 val_loss: 1.117964 val_acc: 60.720721 val_fscore_macro: 0.573469\n",
            "Epoch: 291/300 Train steps: 313 Val steps: 35 0.68s loss: 0.759171 acc: 73.845231 fscore_macro: 0.709818 val_loss: 1.091165 val_acc: 61.441441 val_fscore_macro: 0.570204\n",
            "Epoch: 292/300 Train steps: 313 Val steps: 35 0.67s loss: 0.756047 acc: 74.045191 fscore_macro: 0.713589 val_loss: 1.110635 val_acc: 62.162162 val_fscore_macro: 0.594628\n",
            "Epoch: 293/300 Train steps: 313 Val steps: 35 0.67s loss: 0.753819 acc: 74.645071 fscore_macro: 0.725055 val_loss: 1.132471 val_acc: 60.540541 val_fscore_macro: 0.566719\n",
            "Epoch: 294/300 Train steps: 313 Val steps: 35 0.66s loss: 0.750746 acc: 74.165167 fscore_macro: 0.713942 val_loss: 1.107289 val_acc: 61.441441 val_fscore_macro: 0.578901\n",
            "Epoch: 295/300 Train steps: 313 Val steps: 35 0.68s loss: 0.750167 acc: 74.345131 fscore_macro: 0.714923 val_loss: 1.102321 val_acc: 62.702703 val_fscore_macro: 0.601697\n",
            "Epoch: 296/300 Train steps: 313 Val steps: 35 0.66s loss: 0.754414 acc: 73.825235 fscore_macro: 0.715932 val_loss: 1.109201 val_acc: 60.900901 val_fscore_macro: 0.555394\n",
            "Epoch: 297/300 Train steps: 313 Val steps: 35 0.70s loss: 0.748835 acc: 74.125175 fscore_macro: 0.710356 val_loss: 1.102910 val_acc: 60.180180 val_fscore_macro: 0.566298\n",
            "Epoch: 298/300 Train steps: 313 Val steps: 35 0.68s loss: 0.747713 acc: 74.185163 fscore_macro: 0.715153 val_loss: 1.108337 val_acc: 62.162162 val_fscore_macro: 0.583978\n",
            "Epoch: 299/300 Train steps: 313 Val steps: 35 0.66s loss: 0.746542 acc: 74.365127 fscore_macro: 0.719590 val_loss: 1.127586 val_acc: 59.279279 val_fscore_macro: 0.524409\n",
            "Epoch: 300/300 Train steps: 313 Val steps: 35 0.67s loss: 0.747639 acc: 74.365127 fscore_macro: 0.716270 val_loss: 1.145374 val_acc: 60.900901 val_fscore_macro: 0.555581\n",
            "Restoring data from model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_265.ckpt\n",
            "Found best checkpoint at epoch: 265\n",
            "lr: 0.01, loss: 0.7867, acc: 72.7854, fscore_macro: 0.6939, val_loss: 1.10232, val_acc: 63.964, val_fscore_macro: 0.600088\n",
            "Loading checkpoint model/fasttext/maxpool_embedding_mlp_300/checkpoint_epoch_265.ckpt\n",
            "Running test\n",
            "Test steps: 32 1.38s test_loss: 0.702920 test_acc: 76.600000 test_fscore_macro: 0.690894    \n",
            "\n",
            " Taille de couche cachée égale à : 400, avec les embeddings préentraînés de fasttext aggrégés avec la méthode average :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Epoch:   1/300 Train steps: 313 Val steps: 35 40.58s loss: 2.115649 acc: 22.595481 fscore_macro: 0.041011 val_loss: 2.076182 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch 1: val_acc improved from -inf to 20.72072, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_1.ckpt\n",
            "Epoch:   2/300 Train steps: 313 Val steps: 35 0.73s loss: 2.028147 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.035765 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   3/300 Train steps: 313 Val steps: 35 0.73s loss: 1.995807 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.020923 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   4/300 Train steps: 313 Val steps: 35 0.73s loss: 1.981589 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.013184 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   5/300 Train steps: 313 Val steps: 35 0.71s loss: 1.972192 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.007038 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   6/300 Train steps: 313 Val steps: 35 0.73s loss: 1.963583 acc: 22.695461 fscore_macro: 0.041105 val_loss: 2.000597 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   7/300 Train steps: 313 Val steps: 35 0.76s loss: 1.955215 acc: 22.695461 fscore_macro: 0.041105 val_loss: 1.992447 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   8/300 Train steps: 313 Val steps: 35 0.85s loss: 1.945815 acc: 22.695461 fscore_macro: 0.041105 val_loss: 1.984489 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   9/300 Train steps: 313 Val steps: 35 0.95s loss: 1.935314 acc: 22.695461 fscore_macro: 0.041105 val_loss: 1.974585 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:  10/300 Train steps: 313 Val steps: 35 0.93s loss: 1.923452 acc: 23.115377 fscore_macro: 0.049540 val_loss: 1.963166 val_acc: 22.162162 val_fscore_macro: 0.066927\n",
            "Epoch 10: val_acc improved from 20.72072 to 22.16216, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_10.ckpt\n",
            "Epoch:  11/300 Train steps: 313 Val steps: 35 0.96s loss: 1.910299 acc: 24.715057 fscore_macro: 0.078400 val_loss: 1.950311 val_acc: 23.963964 val_fscore_macro: 0.093625\n",
            "Epoch 11: val_acc improved from 22.16216 to 23.96396, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_11.ckpt\n",
            "Epoch:  12/300 Train steps: 313 Val steps: 35 0.98s loss: 1.895897 acc: 27.114577 fscore_macro: 0.113877 val_loss: 1.937952 val_acc: 25.765766 val_fscore_macro: 0.118411\n",
            "Epoch 12: val_acc improved from 23.96396 to 25.76577, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_12.ckpt\n",
            "Epoch:  13/300 Train steps: 313 Val steps: 35 0.85s loss: 1.880302 acc: 29.974005 fscore_macro: 0.148749 val_loss: 1.923625 val_acc: 26.846847 val_fscore_macro: 0.131775\n",
            "Epoch 13: val_acc improved from 25.76577 to 26.84685, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_13.ckpt\n",
            "Epoch:  14/300 Train steps: 313 Val steps: 35 0.72s loss: 1.863605 acc: 30.693861 fscore_macro: 0.156618 val_loss: 1.907345 val_acc: 30.990991 val_fscore_macro: 0.177589\n",
            "Epoch 14: val_acc improved from 26.84685 to 30.99099, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_14.ckpt\n",
            "Epoch:  15/300 Train steps: 313 Val steps: 35 0.71s loss: 1.847312 acc: 34.313137 fscore_macro: 0.190520 val_loss: 1.892998 val_acc: 33.153153 val_fscore_macro: 0.193366\n",
            "Epoch 15: val_acc improved from 30.99099 to 33.15315, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_15.ckpt\n",
            "Epoch:  16/300 Train steps: 313 Val steps: 35 0.74s loss: 1.830091 acc: 36.372725 fscore_macro: 0.204709 val_loss: 1.877691 val_acc: 32.252252 val_fscore_macro: 0.186693\n",
            "Epoch:  17/300 Train steps: 313 Val steps: 35 0.73s loss: 1.812945 acc: 36.792641 fscore_macro: 0.208403 val_loss: 1.862327 val_acc: 32.792793 val_fscore_macro: 0.189896\n",
            "Epoch:  18/300 Train steps: 313 Val steps: 35 0.73s loss: 1.795509 acc: 37.172565 fscore_macro: 0.212524 val_loss: 1.846415 val_acc: 34.414414 val_fscore_macro: 0.196856\n",
            "Epoch 18: val_acc improved from 33.15315 to 34.41441, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_18.ckpt\n",
            "Epoch:  19/300 Train steps: 313 Val steps: 35 0.73s loss: 1.778992 acc: 37.512498 fscore_macro: 0.215009 val_loss: 1.833155 val_acc: 35.855856 val_fscore_macro: 0.205852\n",
            "Epoch 19: val_acc improved from 34.41441 to 35.85586, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_19.ckpt\n",
            "Epoch:  20/300 Train steps: 313 Val steps: 35 0.73s loss: 1.763110 acc: 38.052390 fscore_macro: 0.216238 val_loss: 1.816563 val_acc: 35.135135 val_fscore_macro: 0.203450\n",
            "Epoch:  21/300 Train steps: 313 Val steps: 35 0.71s loss: 1.746977 acc: 38.252350 fscore_macro: 0.218536 val_loss: 1.801634 val_acc: 36.936937 val_fscore_macro: 0.210618\n",
            "Epoch 21: val_acc improved from 35.85586 to 36.93694, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_21.ckpt\n",
            "Epoch:  22/300 Train steps: 313 Val steps: 35 0.73s loss: 1.731692 acc: 38.252350 fscore_macro: 0.218066 val_loss: 1.787157 val_acc: 35.495496 val_fscore_macro: 0.204779\n",
            "Epoch:  23/300 Train steps: 313 Val steps: 35 0.70s loss: 1.716035 acc: 38.632274 fscore_macro: 0.221873 val_loss: 1.775636 val_acc: 36.576577 val_fscore_macro: 0.205727\n",
            "Epoch:  24/300 Train steps: 313 Val steps: 35 0.73s loss: 1.701837 acc: 38.512298 fscore_macro: 0.221125 val_loss: 1.760084 val_acc: 36.756757 val_fscore_macro: 0.218654\n",
            "Epoch:  25/300 Train steps: 313 Val steps: 35 0.74s loss: 1.687266 acc: 39.392122 fscore_macro: 0.230977 val_loss: 1.749200 val_acc: 37.297297 val_fscore_macro: 0.212707\n",
            "Epoch 25: val_acc improved from 36.93694 to 37.29730, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_25.ckpt\n",
            "Epoch:  26/300 Train steps: 313 Val steps: 35 0.74s loss: 1.673484 acc: 39.552090 fscore_macro: 0.230847 val_loss: 1.733319 val_acc: 37.297297 val_fscore_macro: 0.226135\n",
            "Epoch:  27/300 Train steps: 313 Val steps: 35 0.71s loss: 1.660336 acc: 39.792042 fscore_macro: 0.238643 val_loss: 1.720293 val_acc: 38.378378 val_fscore_macro: 0.233611\n",
            "Epoch 27: val_acc improved from 37.29730 to 38.37838, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_27.ckpt\n",
            "Epoch:  28/300 Train steps: 313 Val steps: 35 0.75s loss: 1.647127 acc: 40.571886 fscore_macro: 0.245951 val_loss: 1.707929 val_acc: 38.558559 val_fscore_macro: 0.234519\n",
            "Epoch 28: val_acc improved from 38.37838 to 38.55856, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_28.ckpt\n",
            "Epoch:  29/300 Train steps: 313 Val steps: 35 0.75s loss: 1.634725 acc: 40.951810 fscore_macro: 0.251410 val_loss: 1.695240 val_acc: 40.000000 val_fscore_macro: 0.254494\n",
            "Epoch 29: val_acc improved from 38.55856 to 40.00000, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_29.ckpt\n",
            "Epoch:  30/300 Train steps: 313 Val steps: 35 0.73s loss: 1.621569 acc: 42.451510 fscore_macro: 0.269475 val_loss: 1.685368 val_acc: 38.738739 val_fscore_macro: 0.240814\n",
            "Epoch:  31/300 Train steps: 313 Val steps: 35 0.78s loss: 1.610422 acc: 42.111578 fscore_macro: 0.267807 val_loss: 1.671694 val_acc: 41.801802 val_fscore_macro: 0.270506\n",
            "Epoch 31: val_acc improved from 40.00000 to 41.80180, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_31.ckpt\n",
            "Epoch:  32/300 Train steps: 313 Val steps: 35 0.76s loss: 1.598344 acc: 43.951210 fscore_macro: 0.289880 val_loss: 1.659761 val_acc: 41.801802 val_fscore_macro: 0.280535\n",
            "Epoch:  33/300 Train steps: 313 Val steps: 35 0.74s loss: 1.587367 acc: 44.071186 fscore_macro: 0.291071 val_loss: 1.647885 val_acc: 42.702703 val_fscore_macro: 0.292321\n",
            "Epoch 33: val_acc improved from 41.80180 to 42.70270, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_33.ckpt\n",
            "Epoch:  34/300 Train steps: 313 Val steps: 35 0.72s loss: 1.576298 acc: 45.150970 fscore_macro: 0.304573 val_loss: 1.636829 val_acc: 43.783784 val_fscore_macro: 0.298751\n",
            "Epoch 34: val_acc improved from 42.70270 to 43.78378, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_34.ckpt\n",
            "Epoch:  35/300 Train steps: 313 Val steps: 35 0.75s loss: 1.564917 acc: 46.010798 fscore_macro: 0.314548 val_loss: 1.625884 val_acc: 43.423423 val_fscore_macro: 0.295760\n",
            "Epoch:  36/300 Train steps: 313 Val steps: 35 0.75s loss: 1.554204 acc: 46.870626 fscore_macro: 0.320162 val_loss: 1.615787 val_acc: 44.864865 val_fscore_macro: 0.311258\n",
            "Epoch 36: val_acc improved from 43.78378 to 44.86486, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_36.ckpt\n",
            "Epoch:  37/300 Train steps: 313 Val steps: 35 0.74s loss: 1.543465 acc: 47.690462 fscore_macro: 0.329289 val_loss: 1.604604 val_acc: 45.225225 val_fscore_macro: 0.315348\n",
            "Epoch 37: val_acc improved from 44.86486 to 45.22523, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_37.ckpt\n",
            "Epoch:  38/300 Train steps: 313 Val steps: 35 0.73s loss: 1.532800 acc: 47.970406 fscore_macro: 0.333168 val_loss: 1.593305 val_acc: 46.486487 val_fscore_macro: 0.327293\n",
            "Epoch 38: val_acc improved from 45.22523 to 46.48649, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_38.ckpt\n",
            "Epoch:  39/300 Train steps: 313 Val steps: 35 0.74s loss: 1.521551 acc: 49.390122 fscore_macro: 0.345023 val_loss: 1.583920 val_acc: 46.486486 val_fscore_macro: 0.326542\n",
            "Epoch:  40/300 Train steps: 313 Val steps: 35 0.72s loss: 1.511157 acc: 49.190162 fscore_macro: 0.344997 val_loss: 1.573719 val_acc: 48.288288 val_fscore_macro: 0.338552\n",
            "Epoch 40: val_acc improved from 46.48649 to 48.28829, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_40.ckpt\n",
            "Epoch:  41/300 Train steps: 313 Val steps: 35 0.74s loss: 1.501117 acc: 50.589882 fscore_macro: 0.356379 val_loss: 1.564090 val_acc: 48.648649 val_fscore_macro: 0.340976\n",
            "Epoch 41: val_acc improved from 48.28829 to 48.64865, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_41.ckpt\n",
            "Epoch:  42/300 Train steps: 313 Val steps: 35 0.72s loss: 1.490556 acc: 50.569886 fscore_macro: 0.355653 val_loss: 1.552287 val_acc: 48.288288 val_fscore_macro: 0.342238\n",
            "Epoch:  43/300 Train steps: 313 Val steps: 35 0.71s loss: 1.480247 acc: 51.389722 fscore_macro: 0.362185 val_loss: 1.541676 val_acc: 49.189189 val_fscore_macro: 0.350362\n",
            "Epoch 43: val_acc improved from 48.64865 to 49.18919, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_43.ckpt\n",
            "Epoch:  44/300 Train steps: 313 Val steps: 35 0.74s loss: 1.469531 acc: 51.589682 fscore_macro: 0.364651 val_loss: 1.535164 val_acc: 49.729730 val_fscore_macro: 0.349444\n",
            "Epoch 44: val_acc improved from 49.18919 to 49.72973, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_44.ckpt\n",
            "Epoch:  45/300 Train steps: 313 Val steps: 35 0.75s loss: 1.458663 acc: 52.469506 fscore_macro: 0.370902 val_loss: 1.521273 val_acc: 49.369369 val_fscore_macro: 0.349003\n",
            "Epoch:  46/300 Train steps: 313 Val steps: 35 0.76s loss: 1.448644 acc: 52.309538 fscore_macro: 0.370617 val_loss: 1.514623 val_acc: 50.630631 val_fscore_macro: 0.358387\n",
            "Epoch 46: val_acc improved from 49.72973 to 50.63063, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_46.ckpt\n",
            "Epoch:  47/300 Train steps: 313 Val steps: 35 0.72s loss: 1.438874 acc: 53.309338 fscore_macro: 0.377102 val_loss: 1.500190 val_acc: 50.990991 val_fscore_macro: 0.363688\n",
            "Epoch 47: val_acc improved from 50.63063 to 50.99099, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_47.ckpt\n",
            "Epoch:  48/300 Train steps: 313 Val steps: 35 0.74s loss: 1.429024 acc: 53.629274 fscore_macro: 0.379528 val_loss: 1.488689 val_acc: 52.252252 val_fscore_macro: 0.374308\n",
            "Epoch 48: val_acc improved from 50.99099 to 52.25225, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_48.ckpt\n",
            "Epoch:  49/300 Train steps: 313 Val steps: 35 0.76s loss: 1.418381 acc: 53.829234 fscore_macro: 0.381119 val_loss: 1.479572 val_acc: 52.432432 val_fscore_macro: 0.374396\n",
            "Epoch 49: val_acc improved from 52.25225 to 52.43243, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_49.ckpt\n",
            "Epoch:  50/300 Train steps: 313 Val steps: 35 0.76s loss: 1.408023 acc: 54.289142 fscore_macro: 0.384220 val_loss: 1.469214 val_acc: 53.153153 val_fscore_macro: 0.381522\n",
            "Epoch 50: val_acc improved from 52.43243 to 53.15315, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_50.ckpt\n",
            "Epoch:  51/300 Train steps: 313 Val steps: 35 0.72s loss: 1.398681 acc: 54.689062 fscore_macro: 0.387858 val_loss: 1.459334 val_acc: 53.513514 val_fscore_macro: 0.384940\n",
            "Epoch 51: val_acc improved from 53.15315 to 53.51351, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_51.ckpt\n",
            "Epoch:  52/300 Train steps: 313 Val steps: 35 0.74s loss: 1.388928 acc: 55.048990 fscore_macro: 0.389256 val_loss: 1.450126 val_acc: 54.054054 val_fscore_macro: 0.388216\n",
            "Epoch 52: val_acc improved from 53.51351 to 54.05405, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_52.ckpt\n",
            "Epoch:  53/300 Train steps: 313 Val steps: 35 0.76s loss: 1.378684 acc: 55.488902 fscore_macro: 0.393155 val_loss: 1.440265 val_acc: 53.873874 val_fscore_macro: 0.385148\n",
            "Epoch:  54/300 Train steps: 313 Val steps: 35 0.74s loss: 1.367926 acc: 55.628874 fscore_macro: 0.394107 val_loss: 1.433960 val_acc: 53.333333 val_fscore_macro: 0.381743\n",
            "Epoch:  55/300 Train steps: 313 Val steps: 35 0.73s loss: 1.359852 acc: 55.788842 fscore_macro: 0.395530 val_loss: 1.419478 val_acc: 55.675676 val_fscore_macro: 0.397601\n",
            "Epoch 55: val_acc improved from 54.05405 to 55.67568, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_55.ckpt\n",
            "Epoch:  56/300 Train steps: 313 Val steps: 35 0.75s loss: 1.349619 acc: 56.408718 fscore_macro: 0.398988 val_loss: 1.414569 val_acc: 54.054054 val_fscore_macro: 0.390772\n",
            "Epoch:  57/300 Train steps: 313 Val steps: 35 0.74s loss: 1.340720 acc: 56.648670 fscore_macro: 0.400498 val_loss: 1.401785 val_acc: 55.675676 val_fscore_macro: 0.398098\n",
            "Epoch:  58/300 Train steps: 313 Val steps: 35 0.74s loss: 1.331576 acc: 56.948610 fscore_macro: 0.402622 val_loss: 1.395007 val_acc: 55.315315 val_fscore_macro: 0.396812\n",
            "Epoch:  59/300 Train steps: 313 Val steps: 35 0.75s loss: 1.321755 acc: 57.088582 fscore_macro: 0.403807 val_loss: 1.385219 val_acc: 55.495496 val_fscore_macro: 0.396624\n",
            "Epoch:  60/300 Train steps: 313 Val steps: 35 0.76s loss: 1.313087 acc: 57.488502 fscore_macro: 0.406316 val_loss: 1.374526 val_acc: 55.495495 val_fscore_macro: 0.393480\n",
            "Epoch:  61/300 Train steps: 313 Val steps: 35 0.71s loss: 1.304680 acc: 57.528494 fscore_macro: 0.405569 val_loss: 1.367290 val_acc: 55.495496 val_fscore_macro: 0.399606\n",
            "Epoch:  62/300 Train steps: 313 Val steps: 35 0.75s loss: 1.295686 acc: 57.728454 fscore_macro: 0.408274 val_loss: 1.360256 val_acc: 55.495496 val_fscore_macro: 0.398336\n",
            "Epoch:  63/300 Train steps: 313 Val steps: 35 0.74s loss: 1.287021 acc: 58.428314 fscore_macro: 0.412517 val_loss: 1.349870 val_acc: 54.774775 val_fscore_macro: 0.393346\n",
            "Epoch:  64/300 Train steps: 313 Val steps: 35 0.73s loss: 1.278229 acc: 58.728254 fscore_macro: 0.414691 val_loss: 1.344084 val_acc: 56.396396 val_fscore_macro: 0.401283\n",
            "Epoch 64: val_acc improved from 55.67568 to 56.39640, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_64.ckpt\n",
            "Epoch:  65/300 Train steps: 313 Val steps: 35 0.72s loss: 1.270695 acc: 58.708258 fscore_macro: 0.413410 val_loss: 1.336023 val_acc: 56.216216 val_fscore_macro: 0.406639\n",
            "Epoch:  66/300 Train steps: 313 Val steps: 35 0.74s loss: 1.262285 acc: 58.468306 fscore_macro: 0.412880 val_loss: 1.327330 val_acc: 56.756757 val_fscore_macro: 0.405435\n",
            "Epoch 66: val_acc improved from 56.39640 to 56.75676, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_66.ckpt\n",
            "Epoch:  67/300 Train steps: 313 Val steps: 35 0.74s loss: 1.254187 acc: 58.728254 fscore_macro: 0.413779 val_loss: 1.318892 val_acc: 56.036036 val_fscore_macro: 0.402920\n",
            "Epoch:  68/300 Train steps: 313 Val steps: 35 0.76s loss: 1.246438 acc: 59.188162 fscore_macro: 0.416611 val_loss: 1.310666 val_acc: 55.855856 val_fscore_macro: 0.400766\n",
            "Epoch:  69/300 Train steps: 313 Val steps: 35 0.72s loss: 1.238953 acc: 59.128174 fscore_macro: 0.416733 val_loss: 1.303080 val_acc: 55.855856 val_fscore_macro: 0.399615\n",
            "Epoch:  70/300 Train steps: 313 Val steps: 35 0.75s loss: 1.231322 acc: 59.208158 fscore_macro: 0.417267 val_loss: 1.294702 val_acc: 56.576577 val_fscore_macro: 0.404871\n",
            "Epoch:  71/300 Train steps: 313 Val steps: 35 0.74s loss: 1.223191 acc: 59.728054 fscore_macro: 0.421414 val_loss: 1.291265 val_acc: 56.576577 val_fscore_macro: 0.406042\n",
            "Epoch:  72/300 Train steps: 313 Val steps: 35 0.72s loss: 1.216316 acc: 60.147970 fscore_macro: 0.425522 val_loss: 1.281938 val_acc: 56.756757 val_fscore_macro: 0.404657\n",
            "Epoch 72: val_acc improved from 56.75676 to 56.75676, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_72.ckpt\n",
            "Epoch:  73/300 Train steps: 313 Val steps: 35 0.74s loss: 1.209165 acc: 59.828034 fscore_macro: 0.420776 val_loss: 1.274582 val_acc: 56.936937 val_fscore_macro: 0.407914\n",
            "Epoch 73: val_acc improved from 56.75676 to 56.93694, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_73.ckpt\n",
            "Epoch:  74/300 Train steps: 313 Val steps: 35 0.70s loss: 1.202177 acc: 60.107978 fscore_macro: 0.426847 val_loss: 1.268814 val_acc: 56.036036 val_fscore_macro: 0.396962\n",
            "Epoch:  75/300 Train steps: 313 Val steps: 35 0.73s loss: 1.195372 acc: 60.167966 fscore_macro: 0.425653 val_loss: 1.261285 val_acc: 56.576577 val_fscore_macro: 0.405433\n",
            "Epoch:  76/300 Train steps: 313 Val steps: 35 0.72s loss: 1.188079 acc: 60.487902 fscore_macro: 0.431144 val_loss: 1.254931 val_acc: 57.117117 val_fscore_macro: 0.407649\n",
            "Epoch 76: val_acc improved from 56.93694 to 57.11712, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_76.ckpt\n",
            "Epoch:  77/300 Train steps: 313 Val steps: 35 0.73s loss: 1.182039 acc: 60.527894 fscore_macro: 0.430523 val_loss: 1.250626 val_acc: 57.837838 val_fscore_macro: 0.423013\n",
            "Epoch 77: val_acc improved from 57.11712 to 57.83784, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_77.ckpt\n",
            "Epoch:  78/300 Train steps: 313 Val steps: 35 0.72s loss: 1.174809 acc: 60.687862 fscore_macro: 0.433734 val_loss: 1.244598 val_acc: 57.297297 val_fscore_macro: 0.419196\n",
            "Epoch:  79/300 Train steps: 313 Val steps: 35 0.71s loss: 1.169146 acc: 61.067786 fscore_macro: 0.438824 val_loss: 1.241193 val_acc: 58.198198 val_fscore_macro: 0.426085\n",
            "Epoch 79: val_acc improved from 57.83784 to 58.19820, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_79.ckpt\n",
            "Epoch:  80/300 Train steps: 313 Val steps: 35 0.74s loss: 1.162555 acc: 61.167766 fscore_macro: 0.438412 val_loss: 1.234965 val_acc: 58.378378 val_fscore_macro: 0.423447\n",
            "Epoch 80: val_acc improved from 58.19820 to 58.37838, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_80.ckpt\n",
            "Epoch:  81/300 Train steps: 313 Val steps: 35 0.77s loss: 1.156527 acc: 61.107778 fscore_macro: 0.438305 val_loss: 1.226211 val_acc: 57.117117 val_fscore_macro: 0.416152\n",
            "Epoch:  82/300 Train steps: 313 Val steps: 35 0.76s loss: 1.150438 acc: 61.667666 fscore_macro: 0.442991 val_loss: 1.222162 val_acc: 57.837838 val_fscore_macro: 0.421801\n",
            "Epoch:  83/300 Train steps: 313 Val steps: 35 0.72s loss: 1.144507 acc: 61.607678 fscore_macro: 0.445546 val_loss: 1.214195 val_acc: 58.918919 val_fscore_macro: 0.441027\n",
            "Epoch 83: val_acc improved from 58.37838 to 58.91892, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_83.ckpt\n",
            "Epoch:  84/300 Train steps: 313 Val steps: 35 0.75s loss: 1.138658 acc: 61.487702 fscore_macro: 0.442934 val_loss: 1.209422 val_acc: 58.378378 val_fscore_macro: 0.432700\n",
            "Epoch:  85/300 Train steps: 313 Val steps: 35 0.77s loss: 1.132481 acc: 62.127574 fscore_macro: 0.449765 val_loss: 1.205310 val_acc: 59.639640 val_fscore_macro: 0.446666\n",
            "Epoch 85: val_acc improved from 58.91892 to 59.63964, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_85.ckpt\n",
            "Epoch:  86/300 Train steps: 313 Val steps: 35 0.92s loss: 1.126457 acc: 62.227554 fscore_macro: 0.454319 val_loss: 1.198410 val_acc: 58.738739 val_fscore_macro: 0.439568\n",
            "Epoch:  87/300 Train steps: 313 Val steps: 35 0.96s loss: 1.121372 acc: 62.307538 fscore_macro: 0.451489 val_loss: 1.194479 val_acc: 59.459460 val_fscore_macro: 0.445327\n",
            "Epoch:  88/300 Train steps: 313 Val steps: 35 0.93s loss: 1.115236 acc: 62.147570 fscore_macro: 0.454344 val_loss: 1.189463 val_acc: 58.558559 val_fscore_macro: 0.438915\n",
            "Epoch:  89/300 Train steps: 313 Val steps: 35 0.92s loss: 1.110847 acc: 62.427514 fscore_macro: 0.455482 val_loss: 1.182700 val_acc: 59.459460 val_fscore_macro: 0.443564\n",
            "Epoch:  90/300 Train steps: 313 Val steps: 35 1.03s loss: 1.104848 acc: 62.527495 fscore_macro: 0.462565 val_loss: 1.181560 val_acc: 59.099099 val_fscore_macro: 0.440195\n",
            "Epoch:  91/300 Train steps: 313 Val steps: 35 0.89s loss: 1.099818 acc: 62.667467 fscore_macro: 0.462750 val_loss: 1.175395 val_acc: 59.279279 val_fscore_macro: 0.449224\n",
            "Epoch:  92/300 Train steps: 313 Val steps: 35 0.75s loss: 1.094900 acc: 63.027395 fscore_macro: 0.469604 val_loss: 1.170679 val_acc: 60.900901 val_fscore_macro: 0.455440\n",
            "Epoch 92: val_acc improved from 59.63964 to 60.90090, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_92.ckpt\n",
            "Epoch:  93/300 Train steps: 313 Val steps: 35 0.72s loss: 1.089707 acc: 63.047391 fscore_macro: 0.468637 val_loss: 1.165546 val_acc: 60.540541 val_fscore_macro: 0.457206\n",
            "Epoch:  94/300 Train steps: 313 Val steps: 35 0.74s loss: 1.084730 acc: 63.127375 fscore_macro: 0.474112 val_loss: 1.159460 val_acc: 59.459459 val_fscore_macro: 0.448946\n",
            "Epoch:  95/300 Train steps: 313 Val steps: 35 0.74s loss: 1.079637 acc: 63.527295 fscore_macro: 0.474772 val_loss: 1.156550 val_acc: 59.639640 val_fscore_macro: 0.449227\n",
            "Epoch:  96/300 Train steps: 313 Val steps: 35 0.75s loss: 1.076060 acc: 63.167367 fscore_macro: 0.472569 val_loss: 1.150321 val_acc: 60.540541 val_fscore_macro: 0.455771\n",
            "Epoch:  97/300 Train steps: 313 Val steps: 35 0.73s loss: 1.070474 acc: 63.927215 fscore_macro: 0.481806 val_loss: 1.149474 val_acc: 59.459459 val_fscore_macro: 0.448779\n",
            "Epoch:  98/300 Train steps: 313 Val steps: 35 0.74s loss: 1.066176 acc: 63.747251 fscore_macro: 0.479071 val_loss: 1.143209 val_acc: 60.180180 val_fscore_macro: 0.454890\n",
            "Epoch:  99/300 Train steps: 313 Val steps: 35 0.74s loss: 1.061257 acc: 64.247151 fscore_macro: 0.486007 val_loss: 1.135387 val_acc: 61.261261 val_fscore_macro: 0.465376\n",
            "Epoch 99: val_acc improved from 60.90090 to 61.26126, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_99.ckpt\n",
            "Epoch: 100/300 Train steps: 313 Val steps: 35 0.70s loss: 1.056817 acc: 63.907219 fscore_macro: 0.482458 val_loss: 1.133361 val_acc: 61.261261 val_fscore_macro: 0.458812\n",
            "Epoch: 101/300 Train steps: 313 Val steps: 35 0.74s loss: 1.051214 acc: 64.507099 fscore_macro: 0.490735 val_loss: 1.137537 val_acc: 60.720721 val_fscore_macro: 0.457074\n",
            "Epoch: 102/300 Train steps: 313 Val steps: 35 0.72s loss: 1.048248 acc: 64.827035 fscore_macro: 0.493491 val_loss: 1.126664 val_acc: 61.981982 val_fscore_macro: 0.470619\n",
            "Epoch 102: val_acc improved from 61.26126 to 61.98198, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_102.ckpt\n",
            "Epoch: 103/300 Train steps: 313 Val steps: 35 0.77s loss: 1.043055 acc: 64.707059 fscore_macro: 0.495214 val_loss: 1.123085 val_acc: 61.261261 val_fscore_macro: 0.463482\n",
            "Epoch: 104/300 Train steps: 313 Val steps: 35 0.72s loss: 1.040110 acc: 65.066987 fscore_macro: 0.497768 val_loss: 1.115598 val_acc: 61.981982 val_fscore_macro: 0.469721\n",
            "Epoch: 105/300 Train steps: 313 Val steps: 35 0.73s loss: 1.034897 acc: 64.727055 fscore_macro: 0.496692 val_loss: 1.112610 val_acc: 62.162162 val_fscore_macro: 0.476712\n",
            "Epoch 105: val_acc improved from 61.98198 to 62.16216, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_105.ckpt\n",
            "Epoch: 106/300 Train steps: 313 Val steps: 35 0.74s loss: 1.031810 acc: 64.787043 fscore_macro: 0.497723 val_loss: 1.109920 val_acc: 61.441441 val_fscore_macro: 0.466596\n",
            "Epoch: 107/300 Train steps: 313 Val steps: 35 0.74s loss: 1.027040 acc: 65.726855 fscore_macro: 0.506544 val_loss: 1.106686 val_acc: 60.900901 val_fscore_macro: 0.461951\n",
            "Epoch: 108/300 Train steps: 313 Val steps: 35 0.75s loss: 1.021909 acc: 65.506899 fscore_macro: 0.505990 val_loss: 1.102840 val_acc: 62.342342 val_fscore_macro: 0.471073\n",
            "Epoch 108: val_acc improved from 62.16216 to 62.34234, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_108.ckpt\n",
            "Epoch: 109/300 Train steps: 313 Val steps: 35 0.75s loss: 1.018852 acc: 65.626875 fscore_macro: 0.503960 val_loss: 1.097340 val_acc: 62.882883 val_fscore_macro: 0.483179\n",
            "Epoch 109: val_acc improved from 62.34234 to 62.88288, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_109.ckpt\n",
            "Epoch: 110/300 Train steps: 313 Val steps: 35 0.76s loss: 1.015150 acc: 65.966807 fscore_macro: 0.510292 val_loss: 1.093273 val_acc: 62.702703 val_fscore_macro: 0.477061\n",
            "Epoch: 111/300 Train steps: 313 Val steps: 35 0.74s loss: 1.011742 acc: 65.806839 fscore_macro: 0.509043 val_loss: 1.091131 val_acc: 62.702703 val_fscore_macro: 0.475473\n",
            "Epoch: 112/300 Train steps: 313 Val steps: 35 0.78s loss: 1.007716 acc: 66.166767 fscore_macro: 0.511750 val_loss: 1.087536 val_acc: 62.522523 val_fscore_macro: 0.474619\n",
            "Epoch: 113/300 Train steps: 313 Val steps: 35 0.80s loss: 1.003597 acc: 66.246751 fscore_macro: 0.514096 val_loss: 1.086186 val_acc: 62.342342 val_fscore_macro: 0.478595\n",
            "Epoch: 114/300 Train steps: 313 Val steps: 35 0.74s loss: 1.000149 acc: 66.526695 fscore_macro: 0.516189 val_loss: 1.079247 val_acc: 63.243243 val_fscore_macro: 0.479364\n",
            "Epoch 114: val_acc improved from 62.88288 to 63.24324, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_114.ckpt\n",
            "Epoch: 115/300 Train steps: 313 Val steps: 35 0.76s loss: 0.996484 acc: 66.606679 fscore_macro: 0.519429 val_loss: 1.079375 val_acc: 62.702703 val_fscore_macro: 0.480761\n",
            "Epoch: 116/300 Train steps: 313 Val steps: 35 0.75s loss: 0.993232 acc: 66.566687 fscore_macro: 0.517944 val_loss: 1.071753 val_acc: 62.702703 val_fscore_macro: 0.476219\n",
            "Epoch: 117/300 Train steps: 313 Val steps: 35 0.74s loss: 0.989188 acc: 66.606679 fscore_macro: 0.517719 val_loss: 1.077542 val_acc: 63.063063 val_fscore_macro: 0.492538\n",
            "Epoch: 118/300 Train steps: 313 Val steps: 35 0.73s loss: 0.986059 acc: 66.666667 fscore_macro: 0.521248 val_loss: 1.068228 val_acc: 63.243243 val_fscore_macro: 0.491941\n",
            "Epoch 118: val_acc improved from 63.24324 to 63.24324, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_118.ckpt\n",
            "Epoch: 119/300 Train steps: 313 Val steps: 35 0.74s loss: 0.983033 acc: 67.026595 fscore_macro: 0.523781 val_loss: 1.063552 val_acc: 64.684685 val_fscore_macro: 0.513219\n",
            "Epoch 119: val_acc improved from 63.24324 to 64.68468, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_119.ckpt\n",
            "Epoch: 120/300 Train steps: 313 Val steps: 35 0.75s loss: 0.979355 acc: 67.106579 fscore_macro: 0.524888 val_loss: 1.064423 val_acc: 64.324324 val_fscore_macro: 0.513017\n",
            "Epoch: 121/300 Train steps: 313 Val steps: 35 0.73s loss: 0.976283 acc: 67.206559 fscore_macro: 0.522571 val_loss: 1.059794 val_acc: 63.423423 val_fscore_macro: 0.494719\n",
            "Epoch: 122/300 Train steps: 313 Val steps: 35 0.80s loss: 0.972864 acc: 67.526495 fscore_macro: 0.529603 val_loss: 1.054907 val_acc: 63.783784 val_fscore_macro: 0.501789\n",
            "Epoch: 123/300 Train steps: 313 Val steps: 35 0.79s loss: 0.969048 acc: 67.766447 fscore_macro: 0.530784 val_loss: 1.051684 val_acc: 63.783784 val_fscore_macro: 0.501555\n",
            "Epoch: 124/300 Train steps: 313 Val steps: 35 0.74s loss: 0.965443 acc: 67.626475 fscore_macro: 0.529281 val_loss: 1.050849 val_acc: 64.504505 val_fscore_macro: 0.507653\n",
            "Epoch: 125/300 Train steps: 313 Val steps: 35 0.75s loss: 0.963603 acc: 68.046391 fscore_macro: 0.534761 val_loss: 1.044149 val_acc: 64.324324 val_fscore_macro: 0.501805\n",
            "Epoch: 126/300 Train steps: 313 Val steps: 35 0.73s loss: 0.960188 acc: 68.046391 fscore_macro: 0.534266 val_loss: 1.044010 val_acc: 64.864865 val_fscore_macro: 0.512377\n",
            "Epoch 126: val_acc improved from 64.68468 to 64.86486, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_126.ckpt\n",
            "Epoch: 127/300 Train steps: 313 Val steps: 35 0.73s loss: 0.956848 acc: 67.906419 fscore_macro: 0.532818 val_loss: 1.037562 val_acc: 65.585586 val_fscore_macro: 0.518375\n",
            "Epoch 127: val_acc improved from 64.86486 to 65.58559, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_127.ckpt\n",
            "Epoch: 128/300 Train steps: 313 Val steps: 35 0.75s loss: 0.953871 acc: 68.066387 fscore_macro: 0.535248 val_loss: 1.039210 val_acc: 65.045045 val_fscore_macro: 0.514295\n",
            "Epoch: 129/300 Train steps: 313 Val steps: 35 0.74s loss: 0.950756 acc: 68.066387 fscore_macro: 0.536075 val_loss: 1.033615 val_acc: 65.045045 val_fscore_macro: 0.512992\n",
            "Epoch: 130/300 Train steps: 313 Val steps: 35 0.74s loss: 0.947673 acc: 68.206359 fscore_macro: 0.535224 val_loss: 1.029415 val_acc: 64.864865 val_fscore_macro: 0.511223\n",
            "Epoch: 131/300 Train steps: 313 Val steps: 35 0.75s loss: 0.945220 acc: 68.426315 fscore_macro: 0.538515 val_loss: 1.025486 val_acc: 65.405405 val_fscore_macro: 0.515759\n",
            "Epoch: 132/300 Train steps: 313 Val steps: 35 0.76s loss: 0.941064 acc: 68.746251 fscore_macro: 0.542895 val_loss: 1.025447 val_acc: 65.045045 val_fscore_macro: 0.511916\n",
            "Epoch: 133/300 Train steps: 313 Val steps: 35 0.77s loss: 0.939492 acc: 68.466307 fscore_macro: 0.540704 val_loss: 1.021604 val_acc: 64.684685 val_fscore_macro: 0.509620\n",
            "Epoch: 134/300 Train steps: 313 Val steps: 35 0.75s loss: 0.935713 acc: 68.786243 fscore_macro: 0.542301 val_loss: 1.018371 val_acc: 65.765766 val_fscore_macro: 0.520797\n",
            "Epoch 134: val_acc improved from 65.58559 to 65.76577, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_134.ckpt\n",
            "Epoch: 135/300 Train steps: 313 Val steps: 35 0.77s loss: 0.932884 acc: 69.066187 fscore_macro: 0.545717 val_loss: 1.021413 val_acc: 65.765766 val_fscore_macro: 0.519112\n",
            "Epoch 135: val_acc improved from 65.76577 to 65.76577, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_135.ckpt\n",
            "Epoch: 136/300 Train steps: 313 Val steps: 35 0.74s loss: 0.929377 acc: 69.286143 fscore_macro: 0.547287 val_loss: 1.014089 val_acc: 66.126126 val_fscore_macro: 0.521551\n",
            "Epoch 136: val_acc improved from 65.76577 to 66.12613, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_136.ckpt\n",
            "Epoch: 137/300 Train steps: 313 Val steps: 35 0.74s loss: 0.927331 acc: 69.086183 fscore_macro: 0.545631 val_loss: 1.011454 val_acc: 65.585586 val_fscore_macro: 0.513031\n",
            "Epoch: 138/300 Train steps: 313 Val steps: 35 0.72s loss: 0.924941 acc: 69.246151 fscore_macro: 0.547021 val_loss: 1.010591 val_acc: 64.324324 val_fscore_macro: 0.506054\n",
            "Epoch: 139/300 Train steps: 313 Val steps: 35 0.73s loss: 0.922002 acc: 69.126175 fscore_macro: 0.545973 val_loss: 1.010496 val_acc: 65.765766 val_fscore_macro: 0.515623\n",
            "Epoch: 140/300 Train steps: 313 Val steps: 35 0.77s loss: 0.919992 acc: 69.046191 fscore_macro: 0.546120 val_loss: 1.009495 val_acc: 65.765766 val_fscore_macro: 0.517489\n",
            "Epoch: 141/300 Train steps: 313 Val steps: 35 0.78s loss: 0.917131 acc: 69.346131 fscore_macro: 0.547623 val_loss: 0.999193 val_acc: 66.306306 val_fscore_macro: 0.520112\n",
            "Epoch 141: val_acc improved from 66.12613 to 66.30631, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_141.ckpt\n",
            "Epoch: 142/300 Train steps: 313 Val steps: 35 0.79s loss: 0.914842 acc: 69.566087 fscore_macro: 0.550127 val_loss: 0.998182 val_acc: 65.405405 val_fscore_macro: 0.515962\n",
            "Epoch: 143/300 Train steps: 313 Val steps: 35 0.76s loss: 0.911027 acc: 69.546091 fscore_macro: 0.549245 val_loss: 1.001783 val_acc: 66.306306 val_fscore_macro: 0.522440\n",
            "Epoch: 144/300 Train steps: 313 Val steps: 35 0.76s loss: 0.908732 acc: 69.666067 fscore_macro: 0.551259 val_loss: 0.996293 val_acc: 67.027027 val_fscore_macro: 0.523349\n",
            "Epoch 144: val_acc improved from 66.30631 to 67.02703, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_144.ckpt\n",
            "Epoch: 145/300 Train steps: 313 Val steps: 35 0.78s loss: 0.906493 acc: 69.746051 fscore_macro: 0.552549 val_loss: 0.993786 val_acc: 65.405405 val_fscore_macro: 0.517046\n",
            "Epoch: 146/300 Train steps: 313 Val steps: 35 0.77s loss: 0.904252 acc: 69.846031 fscore_macro: 0.553531 val_loss: 0.992401 val_acc: 65.405405 val_fscore_macro: 0.514225\n",
            "Epoch: 147/300 Train steps: 313 Val steps: 35 0.76s loss: 0.901357 acc: 69.866027 fscore_macro: 0.553659 val_loss: 0.991805 val_acc: 66.846847 val_fscore_macro: 0.526134\n",
            "Epoch: 148/300 Train steps: 313 Val steps: 35 0.77s loss: 0.898703 acc: 69.906019 fscore_macro: 0.553260 val_loss: 0.986495 val_acc: 66.846847 val_fscore_macro: 0.527108\n",
            "Epoch: 149/300 Train steps: 313 Val steps: 35 0.74s loss: 0.896283 acc: 69.906019 fscore_macro: 0.554060 val_loss: 0.985040 val_acc: 65.585586 val_fscore_macro: 0.517307\n",
            "Epoch: 150/300 Train steps: 313 Val steps: 35 0.74s loss: 0.893477 acc: 70.125975 fscore_macro: 0.554565 val_loss: 0.983808 val_acc: 65.765766 val_fscore_macro: 0.519352\n",
            "Epoch: 151/300 Train steps: 313 Val steps: 35 0.75s loss: 0.892251 acc: 70.165967 fscore_macro: 0.555689 val_loss: 0.979922 val_acc: 66.846847 val_fscore_macro: 0.523753\n",
            "Epoch: 152/300 Train steps: 313 Val steps: 35 0.80s loss: 0.889618 acc: 70.065987 fscore_macro: 0.555168 val_loss: 0.979388 val_acc: 66.486487 val_fscore_macro: 0.525312\n",
            "Epoch: 153/300 Train steps: 313 Val steps: 35 0.75s loss: 0.886626 acc: 70.205959 fscore_macro: 0.556123 val_loss: 0.978230 val_acc: 66.306306 val_fscore_macro: 0.519693\n",
            "Epoch: 154/300 Train steps: 313 Val steps: 35 0.75s loss: 0.885165 acc: 70.665867 fscore_macro: 0.559653 val_loss: 0.973622 val_acc: 67.027027 val_fscore_macro: 0.528626\n",
            "Epoch: 155/300 Train steps: 313 Val steps: 35 0.77s loss: 0.881491 acc: 70.705859 fscore_macro: 0.561005 val_loss: 0.972646 val_acc: 67.387387 val_fscore_macro: 0.531838\n",
            "Epoch 155: val_acc improved from 67.02703 to 67.38739, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_155.ckpt\n",
            "Epoch: 156/300 Train steps: 313 Val steps: 35 0.75s loss: 0.880614 acc: 70.445911 fscore_macro: 0.558665 val_loss: 0.969874 val_acc: 66.306306 val_fscore_macro: 0.526397\n",
            "Epoch: 157/300 Train steps: 313 Val steps: 35 0.76s loss: 0.878385 acc: 70.405919 fscore_macro: 0.558596 val_loss: 0.969141 val_acc: 67.207207 val_fscore_macro: 0.530582\n",
            "Epoch: 158/300 Train steps: 313 Val steps: 35 0.79s loss: 0.875054 acc: 70.825835 fscore_macro: 0.563870 val_loss: 0.962963 val_acc: 67.207207 val_fscore_macro: 0.528251\n",
            "Epoch: 159/300 Train steps: 313 Val steps: 35 0.79s loss: 0.873264 acc: 70.665867 fscore_macro: 0.563906 val_loss: 0.964699 val_acc: 66.846847 val_fscore_macro: 0.527627\n",
            "Epoch: 160/300 Train steps: 313 Val steps: 35 0.71s loss: 0.871315 acc: 70.885823 fscore_macro: 0.565099 val_loss: 0.965439 val_acc: 67.027027 val_fscore_macro: 0.527910\n",
            "Epoch: 161/300 Train steps: 313 Val steps: 35 0.73s loss: 0.868653 acc: 70.905819 fscore_macro: 0.564840 val_loss: 0.958739 val_acc: 67.207207 val_fscore_macro: 0.527202\n",
            "Epoch: 162/300 Train steps: 313 Val steps: 35 0.75s loss: 0.865869 acc: 70.865827 fscore_macro: 0.564803 val_loss: 0.955918 val_acc: 67.207207 val_fscore_macro: 0.529856\n",
            "Epoch: 163/300 Train steps: 313 Val steps: 35 0.91s loss: 0.864427 acc: 71.165767 fscore_macro: 0.570720 val_loss: 0.953441 val_acc: 66.666667 val_fscore_macro: 0.522091\n",
            "Epoch: 164/300 Train steps: 313 Val steps: 35 0.93s loss: 0.863062 acc: 70.685863 fscore_macro: 0.566746 val_loss: 0.953219 val_acc: 67.567568 val_fscore_macro: 0.528906\n",
            "Epoch 164: val_acc improved from 67.38739 to 67.56757, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_164.ckpt\n",
            "Epoch: 165/300 Train steps: 313 Val steps: 35 0.99s loss: 0.860448 acc: 71.225755 fscore_macro: 0.567551 val_loss: 0.948139 val_acc: 67.927928 val_fscore_macro: 0.534702\n",
            "Epoch 165: val_acc improved from 67.56757 to 67.92793, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_165.ckpt\n",
            "Epoch: 166/300 Train steps: 313 Val steps: 35 0.95s loss: 0.858308 acc: 71.185763 fscore_macro: 0.573535 val_loss: 0.947698 val_acc: 67.927928 val_fscore_macro: 0.533588\n",
            "Epoch: 167/300 Train steps: 313 Val steps: 35 1.00s loss: 0.855375 acc: 71.465707 fscore_macro: 0.574841 val_loss: 0.946315 val_acc: 67.027027 val_fscore_macro: 0.526400\n",
            "Epoch: 168/300 Train steps: 313 Val steps: 35 0.76s loss: 0.853859 acc: 71.545691 fscore_macro: 0.577169 val_loss: 0.944090 val_acc: 68.468469 val_fscore_macro: 0.539838\n",
            "Epoch 168: val_acc improved from 67.92793 to 68.46847, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_168.ckpt\n",
            "Epoch: 169/300 Train steps: 313 Val steps: 35 0.75s loss: 0.851940 acc: 71.385723 fscore_macro: 0.572652 val_loss: 0.944688 val_acc: 68.108108 val_fscore_macro: 0.535701\n",
            "Epoch: 170/300 Train steps: 313 Val steps: 35 0.74s loss: 0.850282 acc: 71.685663 fscore_macro: 0.578253 val_loss: 0.943121 val_acc: 68.468469 val_fscore_macro: 0.540338\n",
            "Epoch: 171/300 Train steps: 313 Val steps: 35 0.75s loss: 0.847663 acc: 71.705659 fscore_macro: 0.576966 val_loss: 0.947351 val_acc: 67.567568 val_fscore_macro: 0.536435\n",
            "Epoch: 172/300 Train steps: 313 Val steps: 35 0.75s loss: 0.845624 acc: 71.565687 fscore_macro: 0.578806 val_loss: 0.935438 val_acc: 68.828829 val_fscore_macro: 0.543996\n",
            "Epoch 172: val_acc improved from 68.46847 to 68.82883, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_172.ckpt\n",
            "Epoch: 173/300 Train steps: 313 Val steps: 35 0.76s loss: 0.844202 acc: 71.765647 fscore_macro: 0.580943 val_loss: 0.935199 val_acc: 68.288288 val_fscore_macro: 0.537362\n",
            "Epoch: 174/300 Train steps: 313 Val steps: 35 0.74s loss: 0.841322 acc: 71.185763 fscore_macro: 0.576931 val_loss: 0.936306 val_acc: 68.288288 val_fscore_macro: 0.538930\n",
            "Epoch: 175/300 Train steps: 313 Val steps: 35 0.75s loss: 0.839372 acc: 71.845631 fscore_macro: 0.584767 val_loss: 0.929233 val_acc: 67.567568 val_fscore_macro: 0.533429\n",
            "Epoch: 176/300 Train steps: 313 Val steps: 35 0.74s loss: 0.838783 acc: 71.905619 fscore_macro: 0.584670 val_loss: 0.926461 val_acc: 68.828829 val_fscore_macro: 0.542983\n",
            "Epoch 176: val_acc improved from 68.82883 to 68.82883, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_176.ckpt\n",
            "Epoch: 177/300 Train steps: 313 Val steps: 35 0.77s loss: 0.836943 acc: 71.965607 fscore_macro: 0.586418 val_loss: 0.932435 val_acc: 67.387387 val_fscore_macro: 0.531933\n",
            "Epoch: 178/300 Train steps: 313 Val steps: 35 0.76s loss: 0.833986 acc: 72.005599 fscore_macro: 0.585777 val_loss: 0.925140 val_acc: 68.288288 val_fscore_macro: 0.534958\n",
            "Epoch: 179/300 Train steps: 313 Val steps: 35 0.75s loss: 0.832806 acc: 72.185563 fscore_macro: 0.591614 val_loss: 0.925059 val_acc: 69.009009 val_fscore_macro: 0.547094\n",
            "Epoch 179: val_acc improved from 68.82883 to 69.00901, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_179.ckpt\n",
            "Epoch: 180/300 Train steps: 313 Val steps: 35 0.76s loss: 0.830707 acc: 72.085583 fscore_macro: 0.586198 val_loss: 0.922754 val_acc: 68.468468 val_fscore_macro: 0.538454\n",
            "Epoch: 181/300 Train steps: 313 Val steps: 35 0.78s loss: 0.829661 acc: 71.965607 fscore_macro: 0.585658 val_loss: 0.926315 val_acc: 68.468469 val_fscore_macro: 0.544040\n",
            "Epoch: 182/300 Train steps: 313 Val steps: 35 0.72s loss: 0.826533 acc: 72.465507 fscore_macro: 0.588221 val_loss: 0.918361 val_acc: 69.549550 val_fscore_macro: 0.551340\n",
            "Epoch 182: val_acc improved from 69.00901 to 69.54955, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_182.ckpt\n",
            "Epoch: 183/300 Train steps: 313 Val steps: 35 0.74s loss: 0.824544 acc: 72.185563 fscore_macro: 0.586646 val_loss: 0.923139 val_acc: 67.747748 val_fscore_macro: 0.537786\n",
            "Epoch: 184/300 Train steps: 313 Val steps: 35 0.72s loss: 0.823715 acc: 72.445511 fscore_macro: 0.596627 val_loss: 0.916423 val_acc: 69.549550 val_fscore_macro: 0.549248\n",
            "Epoch 184: val_acc improved from 69.54955 to 69.54955, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_184.ckpt\n",
            "Epoch: 185/300 Train steps: 313 Val steps: 35 0.75s loss: 0.821091 acc: 72.605479 fscore_macro: 0.595680 val_loss: 0.910618 val_acc: 69.369369 val_fscore_macro: 0.548674\n",
            "Epoch: 186/300 Train steps: 313 Val steps: 35 0.74s loss: 0.819101 acc: 72.145571 fscore_macro: 0.589576 val_loss: 0.915866 val_acc: 69.189189 val_fscore_macro: 0.579335\n",
            "Epoch: 187/300 Train steps: 313 Val steps: 35 0.78s loss: 0.818149 acc: 72.645471 fscore_macro: 0.598421 val_loss: 0.911211 val_acc: 69.369369 val_fscore_macro: 0.568552\n",
            "Epoch: 188/300 Train steps: 313 Val steps: 35 0.74s loss: 0.815808 acc: 72.385523 fscore_macro: 0.596290 val_loss: 0.912968 val_acc: 69.549550 val_fscore_macro: 0.583222\n",
            "Epoch: 189/300 Train steps: 313 Val steps: 35 0.70s loss: 0.814285 acc: 72.825435 fscore_macro: 0.601130 val_loss: 0.909180 val_acc: 69.549550 val_fscore_macro: 0.551248\n",
            "Epoch 189: val_acc improved from 69.54955 to 69.54955, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_189.ckpt\n",
            "Epoch: 190/300 Train steps: 313 Val steps: 35 0.76s loss: 0.810747 acc: 72.685463 fscore_macro: 0.599965 val_loss: 0.914397 val_acc: 69.729730 val_fscore_macro: 0.599149\n",
            "Epoch 190: val_acc improved from 69.54955 to 69.72973, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_190.ckpt\n",
            "Epoch: 191/300 Train steps: 313 Val steps: 35 0.75s loss: 0.810099 acc: 73.005399 fscore_macro: 0.606592 val_loss: 0.904207 val_acc: 69.729730 val_fscore_macro: 0.570904\n",
            "Epoch 191: val_acc improved from 69.72973 to 69.72973, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_191.ckpt\n",
            "Epoch: 192/300 Train steps: 313 Val steps: 35 0.76s loss: 0.809722 acc: 72.665467 fscore_macro: 0.604209 val_loss: 0.900923 val_acc: 70.630631 val_fscore_macro: 0.591632\n",
            "Epoch 192: val_acc improved from 69.72973 to 70.63063, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_192.ckpt\n",
            "Epoch: 193/300 Train steps: 313 Val steps: 35 0.74s loss: 0.807345 acc: 72.725455 fscore_macro: 0.603891 val_loss: 0.901032 val_acc: 70.630631 val_fscore_macro: 0.602511\n",
            "Epoch: 194/300 Train steps: 313 Val steps: 35 0.81s loss: 0.805176 acc: 72.925415 fscore_macro: 0.607621 val_loss: 0.902337 val_acc: 69.729730 val_fscore_macro: 0.569475\n",
            "Epoch: 195/300 Train steps: 313 Val steps: 35 0.74s loss: 0.802984 acc: 73.005399 fscore_macro: 0.605486 val_loss: 0.898675 val_acc: 70.450450 val_fscore_macro: 0.575268\n",
            "Epoch: 196/300 Train steps: 313 Val steps: 35 0.76s loss: 0.802137 acc: 73.365327 fscore_macro: 0.612827 val_loss: 0.898452 val_acc: 69.909910 val_fscore_macro: 0.590876\n",
            "Epoch: 197/300 Train steps: 313 Val steps: 35 0.75s loss: 0.800148 acc: 73.425315 fscore_macro: 0.613497 val_loss: 0.897802 val_acc: 70.090090 val_fscore_macro: 0.602138\n",
            "Epoch: 198/300 Train steps: 313 Val steps: 35 0.75s loss: 0.798661 acc: 73.405319 fscore_macro: 0.612051 val_loss: 0.894530 val_acc: 70.270270 val_fscore_macro: 0.603960\n",
            "Epoch: 199/300 Train steps: 313 Val steps: 35 0.73s loss: 0.797571 acc: 73.465307 fscore_macro: 0.613935 val_loss: 0.892927 val_acc: 69.909910 val_fscore_macro: 0.572895\n",
            "Epoch: 200/300 Train steps: 313 Val steps: 35 0.73s loss: 0.795697 acc: 73.585283 fscore_macro: 0.615997 val_loss: 0.898522 val_acc: 70.090090 val_fscore_macro: 0.601327\n",
            "Epoch: 201/300 Train steps: 313 Val steps: 35 0.76s loss: 0.793896 acc: 73.505299 fscore_macro: 0.615626 val_loss: 0.890322 val_acc: 70.810811 val_fscore_macro: 0.607583\n",
            "Epoch 201: val_acc improved from 70.63063 to 70.81081, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_201.ckpt\n",
            "Epoch: 202/300 Train steps: 313 Val steps: 35 0.74s loss: 0.791026 acc: 73.825235 fscore_macro: 0.618240 val_loss: 0.893870 val_acc: 70.090090 val_fscore_macro: 0.611025\n",
            "Epoch: 203/300 Train steps: 313 Val steps: 35 0.77s loss: 0.790199 acc: 73.525295 fscore_macro: 0.621461 val_loss: 0.892748 val_acc: 69.909910 val_fscore_macro: 0.599205\n",
            "Epoch: 204/300 Train steps: 313 Val steps: 35 0.72s loss: 0.788758 acc: 73.765247 fscore_macro: 0.620768 val_loss: 0.885831 val_acc: 71.171171 val_fscore_macro: 0.618862\n",
            "Epoch 204: val_acc improved from 70.81081 to 71.17117, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_204.ckpt\n",
            "Epoch: 205/300 Train steps: 313 Val steps: 35 0.74s loss: 0.787493 acc: 73.825235 fscore_macro: 0.620388 val_loss: 0.886669 val_acc: 71.351351 val_fscore_macro: 0.611699\n",
            "Epoch 205: val_acc improved from 71.17117 to 71.35135, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_205.ckpt\n",
            "Epoch: 206/300 Train steps: 313 Val steps: 35 0.77s loss: 0.785359 acc: 73.825235 fscore_macro: 0.626943 val_loss: 0.890537 val_acc: 70.270270 val_fscore_macro: 0.593167\n",
            "Epoch: 207/300 Train steps: 313 Val steps: 35 0.75s loss: 0.784132 acc: 73.505299 fscore_macro: 0.624684 val_loss: 0.880555 val_acc: 71.531532 val_fscore_macro: 0.613119\n",
            "Epoch 207: val_acc improved from 71.35135 to 71.53153, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_207.ckpt\n",
            "Epoch: 208/300 Train steps: 313 Val steps: 35 0.76s loss: 0.782464 acc: 73.885223 fscore_macro: 0.630580 val_loss: 0.878314 val_acc: 71.351351 val_fscore_macro: 0.622322\n",
            "Epoch: 209/300 Train steps: 313 Val steps: 35 0.74s loss: 0.781157 acc: 73.825235 fscore_macro: 0.628655 val_loss: 0.879871 val_acc: 70.630631 val_fscore_macro: 0.615096\n",
            "Epoch: 210/300 Train steps: 313 Val steps: 35 0.77s loss: 0.779576 acc: 74.105179 fscore_macro: 0.631568 val_loss: 0.876896 val_acc: 70.810811 val_fscore_macro: 0.612981\n",
            "Epoch: 211/300 Train steps: 313 Val steps: 35 0.77s loss: 0.777166 acc: 73.905219 fscore_macro: 0.627218 val_loss: 0.876693 val_acc: 71.711712 val_fscore_macro: 0.627331\n",
            "Epoch 211: val_acc improved from 71.53153 to 71.71171, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_211.ckpt\n",
            "Epoch: 212/300 Train steps: 313 Val steps: 35 0.76s loss: 0.777176 acc: 74.065187 fscore_macro: 0.629247 val_loss: 0.878057 val_acc: 71.351351 val_fscore_macro: 0.624124\n",
            "Epoch: 213/300 Train steps: 313 Val steps: 35 0.77s loss: 0.774440 acc: 74.225155 fscore_macro: 0.632463 val_loss: 0.874838 val_acc: 71.171171 val_fscore_macro: 0.613970\n",
            "Epoch: 214/300 Train steps: 313 Val steps: 35 0.79s loss: 0.772831 acc: 74.245151 fscore_macro: 0.632540 val_loss: 0.874067 val_acc: 70.810811 val_fscore_macro: 0.614615\n",
            "Epoch: 215/300 Train steps: 313 Val steps: 35 0.74s loss: 0.772335 acc: 74.165167 fscore_macro: 0.631687 val_loss: 0.871018 val_acc: 71.171171 val_fscore_macro: 0.620125\n",
            "Epoch: 216/300 Train steps: 313 Val steps: 35 0.76s loss: 0.770122 acc: 74.205159 fscore_macro: 0.631922 val_loss: 0.880536 val_acc: 69.909910 val_fscore_macro: 0.601927\n",
            "Epoch: 217/300 Train steps: 313 Val steps: 35 0.77s loss: 0.768902 acc: 74.325135 fscore_macro: 0.636999 val_loss: 0.868439 val_acc: 70.810811 val_fscore_macro: 0.617438\n",
            "Epoch: 218/300 Train steps: 313 Val steps: 35 0.74s loss: 0.766287 acc: 74.225155 fscore_macro: 0.633492 val_loss: 0.867396 val_acc: 72.072072 val_fscore_macro: 0.630232\n",
            "Epoch 218: val_acc improved from 71.71171 to 72.07207, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_218.ckpt\n",
            "Epoch: 219/300 Train steps: 313 Val steps: 35 0.74s loss: 0.764603 acc: 74.525095 fscore_macro: 0.635572 val_loss: 0.874440 val_acc: 70.810811 val_fscore_macro: 0.620433\n",
            "Epoch: 220/300 Train steps: 313 Val steps: 35 0.72s loss: 0.765175 acc: 74.485103 fscore_macro: 0.636553 val_loss: 0.869032 val_acc: 71.531532 val_fscore_macro: 0.623073\n",
            "Epoch: 221/300 Train steps: 313 Val steps: 35 0.72s loss: 0.763537 acc: 74.545091 fscore_macro: 0.641484 val_loss: 0.869557 val_acc: 71.531532 val_fscore_macro: 0.621368\n",
            "Epoch: 222/300 Train steps: 313 Val steps: 35 0.75s loss: 0.759709 acc: 74.645071 fscore_macro: 0.640401 val_loss: 0.865696 val_acc: 71.351351 val_fscore_macro: 0.626464\n",
            "Epoch: 223/300 Train steps: 313 Val steps: 35 0.76s loss: 0.760268 acc: 74.705059 fscore_macro: 0.645275 val_loss: 0.862131 val_acc: 71.711712 val_fscore_macro: 0.623570\n",
            "Epoch: 224/300 Train steps: 313 Val steps: 35 0.76s loss: 0.759021 acc: 75.064987 fscore_macro: 0.652720 val_loss: 0.863111 val_acc: 71.351351 val_fscore_macro: 0.624492\n",
            "Epoch: 225/300 Train steps: 313 Val steps: 35 0.73s loss: 0.756170 acc: 74.825035 fscore_macro: 0.647282 val_loss: 0.857862 val_acc: 72.072072 val_fscore_macro: 0.627049\n",
            "Epoch 225: val_acc improved from 72.07207 to 72.07207, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_225.ckpt\n",
            "Epoch: 226/300 Train steps: 313 Val steps: 35 0.77s loss: 0.755740 acc: 74.525095 fscore_macro: 0.641654 val_loss: 0.860661 val_acc: 71.351351 val_fscore_macro: 0.621630\n",
            "Epoch: 227/300 Train steps: 313 Val steps: 35 0.72s loss: 0.754685 acc: 74.865027 fscore_macro: 0.642335 val_loss: 0.858223 val_acc: 71.711712 val_fscore_macro: 0.625696\n",
            "Epoch: 228/300 Train steps: 313 Val steps: 35 0.73s loss: 0.752415 acc: 74.945011 fscore_macro: 0.652773 val_loss: 0.862266 val_acc: 70.090090 val_fscore_macro: 0.609742\n",
            "Epoch: 229/300 Train steps: 313 Val steps: 35 0.83s loss: 0.751606 acc: 74.965007 fscore_macro: 0.649050 val_loss: 0.864632 val_acc: 71.171171 val_fscore_macro: 0.618987\n",
            "Epoch: 230/300 Train steps: 313 Val steps: 35 0.76s loss: 0.750183 acc: 75.244951 fscore_macro: 0.652466 val_loss: 0.855498 val_acc: 72.432432 val_fscore_macro: 0.632791\n",
            "Epoch 230: val_acc improved from 72.07207 to 72.43243, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_230.ckpt\n",
            "Epoch: 231/300 Train steps: 313 Val steps: 35 0.80s loss: 0.748382 acc: 75.244951 fscore_macro: 0.658452 val_loss: 0.858614 val_acc: 71.891892 val_fscore_macro: 0.626432\n",
            "Epoch: 232/300 Train steps: 313 Val steps: 35 0.78s loss: 0.747805 acc: 75.424915 fscore_macro: 0.657342 val_loss: 0.849555 val_acc: 71.531532 val_fscore_macro: 0.627478\n",
            "Epoch: 233/300 Train steps: 313 Val steps: 35 0.77s loss: 0.746503 acc: 74.865027 fscore_macro: 0.650774 val_loss: 0.850665 val_acc: 71.711712 val_fscore_macro: 0.626558\n",
            "Epoch: 234/300 Train steps: 313 Val steps: 35 0.78s loss: 0.744075 acc: 74.985003 fscore_macro: 0.654482 val_loss: 0.850965 val_acc: 71.351351 val_fscore_macro: 0.622771\n",
            "Epoch: 235/300 Train steps: 313 Val steps: 35 0.77s loss: 0.743879 acc: 75.764847 fscore_macro: 0.664553 val_loss: 0.848591 val_acc: 72.072072 val_fscore_macro: 0.629110\n",
            "Epoch: 236/300 Train steps: 313 Val steps: 35 0.93s loss: 0.742172 acc: 75.284943 fscore_macro: 0.659711 val_loss: 0.845182 val_acc: 71.891892 val_fscore_macro: 0.628934\n",
            "Epoch: 237/300 Train steps: 313 Val steps: 35 0.92s loss: 0.740709 acc: 75.344931 fscore_macro: 0.659970 val_loss: 0.842331 val_acc: 72.432432 val_fscore_macro: 0.633235\n",
            "Epoch: 238/300 Train steps: 313 Val steps: 35 0.87s loss: 0.738454 acc: 75.464907 fscore_macro: 0.666050 val_loss: 0.850091 val_acc: 72.252252 val_fscore_macro: 0.633301\n",
            "Epoch: 239/300 Train steps: 313 Val steps: 35 0.90s loss: 0.738146 acc: 75.444911 fscore_macro: 0.671670 val_loss: 0.843561 val_acc: 71.891892 val_fscore_macro: 0.623676\n",
            "Epoch: 240/300 Train steps: 313 Val steps: 35 0.99s loss: 0.735944 acc: 75.684863 fscore_macro: 0.671104 val_loss: 0.851203 val_acc: 70.090090 val_fscore_macro: 0.609980\n",
            "Epoch: 241/300 Train steps: 313 Val steps: 35 0.91s loss: 0.736466 acc: 75.784843 fscore_macro: 0.671524 val_loss: 0.846813 val_acc: 71.351351 val_fscore_macro: 0.625107\n",
            "Epoch: 242/300 Train steps: 313 Val steps: 35 0.75s loss: 0.734502 acc: 75.724855 fscore_macro: 0.670908 val_loss: 0.840827 val_acc: 71.531532 val_fscore_macro: 0.625495\n",
            "Epoch: 243/300 Train steps: 313 Val steps: 35 0.76s loss: 0.733608 acc: 75.624875 fscore_macro: 0.669573 val_loss: 0.845513 val_acc: 70.630631 val_fscore_macro: 0.621696\n",
            "Epoch: 244/300 Train steps: 313 Val steps: 35 0.76s loss: 0.730332 acc: 75.804839 fscore_macro: 0.672011 val_loss: 0.843581 val_acc: 71.171171 val_fscore_macro: 0.621722\n",
            "Epoch: 245/300 Train steps: 313 Val steps: 35 0.78s loss: 0.730836 acc: 75.604879 fscore_macro: 0.673200 val_loss: 0.836468 val_acc: 71.891892 val_fscore_macro: 0.629206\n",
            "Epoch: 246/300 Train steps: 313 Val steps: 35 0.73s loss: 0.729257 acc: 76.164767 fscore_macro: 0.675877 val_loss: 0.839281 val_acc: 72.072072 val_fscore_macro: 0.628665\n",
            "Epoch: 247/300 Train steps: 313 Val steps: 35 0.76s loss: 0.728802 acc: 76.144771 fscore_macro: 0.673514 val_loss: 0.833383 val_acc: 71.891892 val_fscore_macro: 0.622992\n",
            "Epoch: 248/300 Train steps: 313 Val steps: 35 0.79s loss: 0.726903 acc: 75.904819 fscore_macro: 0.675306 val_loss: 0.837942 val_acc: 71.891892 val_fscore_macro: 0.624679\n",
            "Epoch: 249/300 Train steps: 313 Val steps: 35 0.77s loss: 0.725640 acc: 76.204759 fscore_macro: 0.680716 val_loss: 0.839554 val_acc: 72.432432 val_fscore_macro: 0.633381\n",
            "Epoch: 250/300 Train steps: 313 Val steps: 35 0.78s loss: 0.722604 acc: 76.344731 fscore_macro: 0.677715 val_loss: 0.834366 val_acc: 72.072072 val_fscore_macro: 0.627402\n",
            "Epoch: 251/300 Train steps: 313 Val steps: 35 0.76s loss: 0.722596 acc: 75.804839 fscore_macro: 0.677967 val_loss: 0.828210 val_acc: 71.891892 val_fscore_macro: 0.624180\n",
            "Epoch: 252/300 Train steps: 313 Val steps: 35 0.76s loss: 0.721378 acc: 76.244751 fscore_macro: 0.679635 val_loss: 0.831516 val_acc: 71.891892 val_fscore_macro: 0.624238\n",
            "Epoch: 253/300 Train steps: 313 Val steps: 35 0.72s loss: 0.719642 acc: 76.244751 fscore_macro: 0.684635 val_loss: 0.837904 val_acc: 71.351351 val_fscore_macro: 0.626229\n",
            "Epoch: 254/300 Train steps: 313 Val steps: 35 0.74s loss: 0.718046 acc: 76.304739 fscore_macro: 0.678626 val_loss: 0.826575 val_acc: 72.612613 val_fscore_macro: 0.632837\n",
            "Epoch 254: val_acc improved from 72.43243 to 72.61261, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_254.ckpt\n",
            "Epoch: 255/300 Train steps: 313 Val steps: 35 0.78s loss: 0.717878 acc: 76.404719 fscore_macro: 0.677966 val_loss: 0.827965 val_acc: 71.891892 val_fscore_macro: 0.624535\n",
            "Epoch: 256/300 Train steps: 313 Val steps: 35 0.74s loss: 0.715700 acc: 76.384723 fscore_macro: 0.681393 val_loss: 0.823227 val_acc: 72.252252 val_fscore_macro: 0.626171\n",
            "Epoch: 257/300 Train steps: 313 Val steps: 35 0.74s loss: 0.715447 acc: 76.324735 fscore_macro: 0.677595 val_loss: 0.828248 val_acc: 72.252252 val_fscore_macro: 0.636694\n",
            "Epoch: 258/300 Train steps: 313 Val steps: 35 0.77s loss: 0.713647 acc: 76.184763 fscore_macro: 0.681843 val_loss: 0.835612 val_acc: 72.612613 val_fscore_macro: 0.632451\n",
            "Epoch: 259/300 Train steps: 313 Val steps: 35 0.74s loss: 0.713676 acc: 76.104779 fscore_macro: 0.678487 val_loss: 0.827018 val_acc: 72.252252 val_fscore_macro: 0.628197\n",
            "Epoch: 260/300 Train steps: 313 Val steps: 35 0.76s loss: 0.711766 acc: 76.144771 fscore_macro: 0.679435 val_loss: 0.839391 val_acc: 70.990991 val_fscore_macro: 0.624324\n",
            "Epoch: 261/300 Train steps: 313 Val steps: 35 0.75s loss: 0.710684 acc: 76.084783 fscore_macro: 0.681481 val_loss: 0.826961 val_acc: 72.252252 val_fscore_macro: 0.640292\n",
            "Epoch: 262/300 Train steps: 313 Val steps: 35 0.74s loss: 0.710023 acc: 76.644671 fscore_macro: 0.685975 val_loss: 0.819146 val_acc: 73.153153 val_fscore_macro: 0.632769\n",
            "Epoch 262: val_acc improved from 72.61261 to 73.15315, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_262.ckpt\n",
            "Epoch: 263/300 Train steps: 313 Val steps: 35 0.75s loss: 0.707248 acc: 76.644671 fscore_macro: 0.687558 val_loss: 0.822324 val_acc: 72.612613 val_fscore_macro: 0.634968\n",
            "Epoch: 264/300 Train steps: 313 Val steps: 35 0.74s loss: 0.706909 acc: 76.524695 fscore_macro: 0.685465 val_loss: 0.826112 val_acc: 71.531532 val_fscore_macro: 0.626662\n",
            "Epoch: 265/300 Train steps: 313 Val steps: 35 0.77s loss: 0.704968 acc: 76.624675 fscore_macro: 0.688805 val_loss: 0.821468 val_acc: 72.792793 val_fscore_macro: 0.634201\n",
            "Epoch: 266/300 Train steps: 313 Val steps: 35 0.79s loss: 0.705965 acc: 76.724655 fscore_macro: 0.687752 val_loss: 0.818343 val_acc: 72.432432 val_fscore_macro: 0.629386\n",
            "Epoch: 267/300 Train steps: 313 Val steps: 35 0.73s loss: 0.703476 acc: 76.884623 fscore_macro: 0.692183 val_loss: 0.813909 val_acc: 72.792793 val_fscore_macro: 0.629601\n",
            "Epoch: 268/300 Train steps: 313 Val steps: 35 0.72s loss: 0.701892 acc: 76.844631 fscore_macro: 0.684887 val_loss: 0.814353 val_acc: 72.612613 val_fscore_macro: 0.641189\n",
            "Epoch: 269/300 Train steps: 313 Val steps: 35 0.74s loss: 0.701417 acc: 76.584683 fscore_macro: 0.687159 val_loss: 0.824404 val_acc: 71.171171 val_fscore_macro: 0.619879\n",
            "Epoch: 270/300 Train steps: 313 Val steps: 35 0.78s loss: 0.701409 acc: 76.744651 fscore_macro: 0.687624 val_loss: 0.813507 val_acc: 73.153153 val_fscore_macro: 0.643116\n",
            "Epoch: 271/300 Train steps: 313 Val steps: 35 0.76s loss: 0.698687 acc: 76.804639 fscore_macro: 0.688900 val_loss: 0.819750 val_acc: 72.612613 val_fscore_macro: 0.631753\n",
            "Epoch: 272/300 Train steps: 313 Val steps: 35 0.73s loss: 0.698322 acc: 76.744651 fscore_macro: 0.689408 val_loss: 0.813523 val_acc: 72.792793 val_fscore_macro: 0.632859\n",
            "Epoch: 273/300 Train steps: 313 Val steps: 35 0.77s loss: 0.697639 acc: 77.184563 fscore_macro: 0.695107 val_loss: 0.818756 val_acc: 72.432432 val_fscore_macro: 0.633832\n",
            "Epoch: 274/300 Train steps: 313 Val steps: 35 0.76s loss: 0.695451 acc: 76.884623 fscore_macro: 0.689175 val_loss: 0.809039 val_acc: 73.333333 val_fscore_macro: 0.640891\n",
            "Epoch 274: val_acc improved from 73.15315 to 73.33333, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_274.ckpt\n",
            "Epoch: 275/300 Train steps: 313 Val steps: 35 0.74s loss: 0.694192 acc: 76.924615 fscore_macro: 0.690576 val_loss: 0.804036 val_acc: 73.153153 val_fscore_macro: 0.637029\n",
            "Epoch: 276/300 Train steps: 313 Val steps: 35 0.76s loss: 0.693197 acc: 76.904619 fscore_macro: 0.692858 val_loss: 0.805550 val_acc: 72.432432 val_fscore_macro: 0.631636\n",
            "Epoch: 277/300 Train steps: 313 Val steps: 35 0.74s loss: 0.692813 acc: 76.944611 fscore_macro: 0.694733 val_loss: 0.819170 val_acc: 71.171171 val_fscore_macro: 0.625563\n",
            "Epoch: 278/300 Train steps: 313 Val steps: 35 0.73s loss: 0.690987 acc: 77.484503 fscore_macro: 0.700627 val_loss: 0.806017 val_acc: 72.972973 val_fscore_macro: 0.644484\n",
            "Epoch: 279/300 Train steps: 313 Val steps: 35 0.73s loss: 0.690387 acc: 76.824635 fscore_macro: 0.691748 val_loss: 0.805967 val_acc: 72.072072 val_fscore_macro: 0.640847\n",
            "Epoch: 280/300 Train steps: 313 Val steps: 35 0.75s loss: 0.690127 acc: 77.024595 fscore_macro: 0.693442 val_loss: 0.804774 val_acc: 72.792793 val_fscore_macro: 0.640605\n",
            "Epoch: 281/300 Train steps: 313 Val steps: 35 0.75s loss: 0.688983 acc: 77.044591 fscore_macro: 0.694331 val_loss: 0.802129 val_acc: 72.972973 val_fscore_macro: 0.642167\n",
            "Epoch: 282/300 Train steps: 313 Val steps: 35 0.77s loss: 0.687386 acc: 77.104579 fscore_macro: 0.695922 val_loss: 0.813137 val_acc: 72.072072 val_fscore_macro: 0.640630\n",
            "Epoch: 283/300 Train steps: 313 Val steps: 35 0.70s loss: 0.686435 acc: 77.084583 fscore_macro: 0.694420 val_loss: 0.802153 val_acc: 73.153153 val_fscore_macro: 0.634174\n",
            "Epoch: 284/300 Train steps: 313 Val steps: 35 0.77s loss: 0.685849 acc: 77.364527 fscore_macro: 0.696539 val_loss: 0.805065 val_acc: 73.333333 val_fscore_macro: 0.645874\n",
            "Epoch 284: val_acc improved from 73.33333 to 73.33333, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_284.ckpt\n",
            "Epoch: 285/300 Train steps: 313 Val steps: 35 0.77s loss: 0.683971 acc: 77.304539 fscore_macro: 0.695585 val_loss: 0.809997 val_acc: 71.531532 val_fscore_macro: 0.633176\n",
            "Epoch: 286/300 Train steps: 313 Val steps: 35 0.73s loss: 0.682617 acc: 77.184563 fscore_macro: 0.700211 val_loss: 0.805658 val_acc: 72.972973 val_fscore_macro: 0.643007\n",
            "Epoch: 287/300 Train steps: 313 Val steps: 35 0.77s loss: 0.681305 acc: 77.344531 fscore_macro: 0.697536 val_loss: 0.798831 val_acc: 72.972973 val_fscore_macro: 0.647793\n",
            "Epoch: 288/300 Train steps: 313 Val steps: 35 0.72s loss: 0.680630 acc: 77.004599 fscore_macro: 0.694576 val_loss: 0.803911 val_acc: 72.252252 val_fscore_macro: 0.643056\n",
            "Epoch: 289/300 Train steps: 313 Val steps: 35 0.75s loss: 0.679777 acc: 77.404519 fscore_macro: 0.702584 val_loss: 0.803485 val_acc: 73.153153 val_fscore_macro: 0.646462\n",
            "Epoch: 290/300 Train steps: 313 Val steps: 35 0.77s loss: 0.679420 acc: 77.024595 fscore_macro: 0.693266 val_loss: 0.813986 val_acc: 72.432432 val_fscore_macro: 0.637242\n",
            "Epoch: 291/300 Train steps: 313 Val steps: 35 0.75s loss: 0.678585 acc: 77.384523 fscore_macro: 0.699720 val_loss: 0.796916 val_acc: 73.333333 val_fscore_macro: 0.654241\n",
            "Epoch: 292/300 Train steps: 313 Val steps: 35 0.73s loss: 0.675869 acc: 77.624475 fscore_macro: 0.704044 val_loss: 0.796994 val_acc: 72.792793 val_fscore_macro: 0.650037\n",
            "Epoch: 293/300 Train steps: 313 Val steps: 35 0.75s loss: 0.675995 acc: 77.704459 fscore_macro: 0.702594 val_loss: 0.807006 val_acc: 71.711712 val_fscore_macro: 0.632094\n",
            "Epoch: 294/300 Train steps: 313 Val steps: 35 0.74s loss: 0.674481 acc: 77.504499 fscore_macro: 0.701909 val_loss: 0.793404 val_acc: 73.153153 val_fscore_macro: 0.643012\n",
            "Epoch: 295/300 Train steps: 313 Val steps: 35 0.74s loss: 0.673305 acc: 77.624475 fscore_macro: 0.701450 val_loss: 0.800444 val_acc: 72.972973 val_fscore_macro: 0.646141\n",
            "Epoch: 296/300 Train steps: 313 Val steps: 35 0.74s loss: 0.672860 acc: 77.664467 fscore_macro: 0.703284 val_loss: 0.802963 val_acc: 72.432432 val_fscore_macro: 0.645740\n",
            "Epoch: 297/300 Train steps: 313 Val steps: 35 0.74s loss: 0.671634 acc: 77.844431 fscore_macro: 0.702351 val_loss: 0.795030 val_acc: 72.612613 val_fscore_macro: 0.644127\n",
            "Epoch: 298/300 Train steps: 313 Val steps: 35 0.75s loss: 0.670681 acc: 77.644471 fscore_macro: 0.706002 val_loss: 0.798284 val_acc: 72.432432 val_fscore_macro: 0.643465\n",
            "Epoch: 299/300 Train steps: 313 Val steps: 35 0.76s loss: 0.669271 acc: 77.644471 fscore_macro: 0.705407 val_loss: 0.805176 val_acc: 72.252252 val_fscore_macro: 0.639039\n",
            "Epoch: 300/300 Train steps: 313 Val steps: 35 0.78s loss: 0.670216 acc: 77.804439 fscore_macro: 0.705719 val_loss: 0.787918 val_acc: 73.513513 val_fscore_macro: 0.650527\n",
            "Epoch 300: val_acc improved from 73.33333 to 73.51351, saving file to model/fasttext/average_embedding_mlp_400/checkpoint_epoch_300.ckpt\n",
            "Restoring data from model/fasttext/average_embedding_mlp_400/checkpoint_epoch_300.ckpt\n",
            "Found best checkpoint at epoch: 300\n",
            "lr: 0.01, loss: 0.670216, acc: 77.8044, fscore_macro: 0.705719, val_loss: 0.787918, val_acc: 73.5135, val_fscore_macro: 0.650527\n",
            "Loading checkpoint model/fasttext/average_embedding_mlp_400/checkpoint_epoch_300.ckpt\n",
            "Running test\n",
            "Test steps: 32 1.53s test_loss: 0.544068 test_acc: 83.800000 test_fscore_macro: 0.732189     \n",
            "\n",
            " Taille de couche cachée égale à : 400, avec les embeddings préentraînés de fasttext aggrégés avec la méthode maxpool :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Epoch:   1/300 Train steps: 313 Val steps: 35 42.67s loss: 2.029200 acc: 22.695461 fscore_macro: 0.041367 val_loss: 2.012471 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch 1: val_acc improved from -inf to 20.72072, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_1.ckpt\n",
            "Epoch:   2/300 Train steps: 313 Val steps: 35 0.78s loss: 1.964442 acc: 23.035393 fscore_macro: 0.046898 val_loss: 2.003202 val_acc: 20.720721 val_fscore_macro: 0.038143\n",
            "Epoch:   3/300 Train steps: 313 Val steps: 35 0.74s loss: 1.947121 acc: 24.135173 fscore_macro: 0.066409 val_loss: 1.984878 val_acc: 23.603604 val_fscore_macro: 0.071926\n",
            "Epoch 3: val_acc improved from 20.72072 to 23.60360, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_3.ckpt\n",
            "Epoch:   4/300 Train steps: 313 Val steps: 35 0.74s loss: 1.931482 acc: 24.655069 fscore_macro: 0.065907 val_loss: 1.969209 val_acc: 21.081081 val_fscore_macro: 0.042886\n",
            "Epoch:   5/300 Train steps: 313 Val steps: 35 0.75s loss: 1.912293 acc: 24.955009 fscore_macro: 0.071439 val_loss: 1.952251 val_acc: 27.927928 val_fscore_macro: 0.129270\n",
            "Epoch 5: val_acc improved from 23.60360 to 27.92793, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_5.ckpt\n",
            "Epoch:   6/300 Train steps: 313 Val steps: 35 0.79s loss: 1.889483 acc: 29.934013 fscore_macro: 0.135095 val_loss: 1.935247 val_acc: 24.144144 val_fscore_macro: 0.095773\n",
            "Epoch:   7/300 Train steps: 313 Val steps: 35 0.76s loss: 1.866730 acc: 30.253949 fscore_macro: 0.148696 val_loss: 1.912145 val_acc: 29.729730 val_fscore_macro: 0.164800\n",
            "Epoch 7: val_acc improved from 27.92793 to 29.72973, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_7.ckpt\n",
            "Epoch:   8/300 Train steps: 313 Val steps: 35 0.76s loss: 1.842619 acc: 32.933413 fscore_macro: 0.170902 val_loss: 1.892070 val_acc: 30.810811 val_fscore_macro: 0.175762\n",
            "Epoch 8: val_acc improved from 29.72973 to 30.81081, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_8.ckpt\n",
            "Epoch:   9/300 Train steps: 313 Val steps: 35 0.75s loss: 1.818608 acc: 34.533093 fscore_macro: 0.188441 val_loss: 1.870476 val_acc: 31.171171 val_fscore_macro: 0.189438\n",
            "Epoch 9: val_acc improved from 30.81081 to 31.17117, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_9.ckpt\n",
            "Epoch:  10/300 Train steps: 313 Val steps: 35 0.75s loss: 1.795129 acc: 35.752849 fscore_macro: 0.205492 val_loss: 1.849998 val_acc: 30.630631 val_fscore_macro: 0.185288\n",
            "Epoch:  11/300 Train steps: 313 Val steps: 35 0.75s loss: 1.773244 acc: 36.052789 fscore_macro: 0.210688 val_loss: 1.830715 val_acc: 31.711712 val_fscore_macro: 0.201210\n",
            "Epoch 11: val_acc improved from 31.17117 to 31.71171, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_11.ckpt\n",
            "Epoch:  12/300 Train steps: 313 Val steps: 35 0.73s loss: 1.751705 acc: 37.412517 fscore_macro: 0.226315 val_loss: 1.815526 val_acc: 32.792793 val_fscore_macro: 0.196920\n",
            "Epoch 12: val_acc improved from 31.71171 to 32.79279, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_12.ckpt\n",
            "Epoch:  13/300 Train steps: 313 Val steps: 35 0.77s loss: 1.731057 acc: 38.152370 fscore_macro: 0.234349 val_loss: 1.800771 val_acc: 32.432432 val_fscore_macro: 0.199302\n",
            "Epoch:  14/300 Train steps: 313 Val steps: 35 0.76s loss: 1.711332 acc: 39.092182 fscore_macro: 0.243787 val_loss: 1.779456 val_acc: 34.234234 val_fscore_macro: 0.229006\n",
            "Epoch 14: val_acc improved from 32.79279 to 34.23423, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_14.ckpt\n",
            "Epoch:  15/300 Train steps: 313 Val steps: 35 0.75s loss: 1.693172 acc: 39.972006 fscore_macro: 0.247910 val_loss: 1.763680 val_acc: 34.054054 val_fscore_macro: 0.216714\n",
            "Epoch:  16/300 Train steps: 313 Val steps: 35 0.74s loss: 1.675118 acc: 40.811838 fscore_macro: 0.256081 val_loss: 1.749822 val_acc: 34.414414 val_fscore_macro: 0.227624\n",
            "Epoch 16: val_acc improved from 34.23423 to 34.41441, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_16.ckpt\n",
            "Epoch:  17/300 Train steps: 313 Val steps: 35 0.76s loss: 1.658692 acc: 41.211758 fscore_macro: 0.262011 val_loss: 1.734442 val_acc: 34.774775 val_fscore_macro: 0.226237\n",
            "Epoch 17: val_acc improved from 34.41441 to 34.77477, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_17.ckpt\n",
            "Epoch:  18/300 Train steps: 313 Val steps: 35 0.70s loss: 1.640334 acc: 41.911618 fscore_macro: 0.269413 val_loss: 1.720191 val_acc: 34.774775 val_fscore_macro: 0.221894\n",
            "Epoch:  19/300 Train steps: 313 Val steps: 35 0.78s loss: 1.626633 acc: 42.071586 fscore_macro: 0.274554 val_loss: 1.705376 val_acc: 36.396396 val_fscore_macro: 0.230036\n",
            "Epoch 19: val_acc improved from 34.77477 to 36.39640, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_19.ckpt\n",
            "Epoch:  20/300 Train steps: 313 Val steps: 35 0.78s loss: 1.611595 acc: 42.211558 fscore_macro: 0.274094 val_loss: 1.688747 val_acc: 39.279279 val_fscore_macro: 0.271985\n",
            "Epoch 20: val_acc improved from 36.39640 to 39.27928, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_20.ckpt\n",
            "Epoch:  21/300 Train steps: 313 Val steps: 35 0.74s loss: 1.596497 acc: 43.451310 fscore_macro: 0.287662 val_loss: 1.674075 val_acc: 39.459459 val_fscore_macro: 0.263874\n",
            "Epoch 21: val_acc improved from 39.27928 to 39.45946, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_21.ckpt\n",
            "Epoch:  22/300 Train steps: 313 Val steps: 35 0.77s loss: 1.582535 acc: 43.791242 fscore_macro: 0.292343 val_loss: 1.666581 val_acc: 37.297297 val_fscore_macro: 0.245477\n",
            "Epoch:  23/300 Train steps: 313 Val steps: 35 0.78s loss: 1.567262 acc: 44.451110 fscore_macro: 0.298127 val_loss: 1.667987 val_acc: 38.558559 val_fscore_macro: 0.258361\n",
            "Epoch:  24/300 Train steps: 313 Val steps: 35 0.76s loss: 1.556932 acc: 44.131174 fscore_macro: 0.299605 val_loss: 1.644312 val_acc: 38.738739 val_fscore_macro: 0.251348\n",
            "Epoch:  25/300 Train steps: 313 Val steps: 35 0.76s loss: 1.543277 acc: 45.170966 fscore_macro: 0.306233 val_loss: 1.633924 val_acc: 38.738739 val_fscore_macro: 0.253848\n",
            "Epoch:  26/300 Train steps: 313 Val steps: 35 0.75s loss: 1.531462 acc: 45.250950 fscore_macro: 0.311005 val_loss: 1.614048 val_acc: 41.441441 val_fscore_macro: 0.282057\n",
            "Epoch 26: val_acc improved from 39.45946 to 41.44144, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_26.ckpt\n",
            "Epoch:  27/300 Train steps: 313 Val steps: 35 0.75s loss: 1.520303 acc: 45.530894 fscore_macro: 0.311920 val_loss: 1.609383 val_acc: 43.423423 val_fscore_macro: 0.304181\n",
            "Epoch 27: val_acc improved from 41.44144 to 43.42342, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_27.ckpt\n",
            "Epoch:  28/300 Train steps: 313 Val steps: 35 0.73s loss: 1.508235 acc: 46.490702 fscore_macro: 0.320537 val_loss: 1.593201 val_acc: 42.522523 val_fscore_macro: 0.295374\n",
            "Epoch:  29/300 Train steps: 313 Val steps: 35 0.76s loss: 1.498827 acc: 46.530694 fscore_macro: 0.322636 val_loss: 1.581375 val_acc: 45.405405 val_fscore_macro: 0.320626\n",
            "Epoch 29: val_acc improved from 43.42342 to 45.40541, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_29.ckpt\n",
            "Epoch:  30/300 Train steps: 313 Val steps: 35 0.76s loss: 1.485735 acc: 47.370526 fscore_macro: 0.330688 val_loss: 1.580793 val_acc: 44.324324 val_fscore_macro: 0.310846\n",
            "Epoch:  31/300 Train steps: 313 Val steps: 35 0.75s loss: 1.477405 acc: 48.070386 fscore_macro: 0.336404 val_loss: 1.567283 val_acc: 45.585586 val_fscore_macro: 0.321195\n",
            "Epoch 31: val_acc improved from 45.40541 to 45.58559, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_31.ckpt\n",
            "Epoch:  32/300 Train steps: 313 Val steps: 35 0.78s loss: 1.464182 acc: 49.050190 fscore_macro: 0.343086 val_loss: 1.554403 val_acc: 47.387387 val_fscore_macro: 0.337248\n",
            "Epoch 32: val_acc improved from 45.58559 to 47.38739, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_32.ckpt\n",
            "Epoch:  33/300 Train steps: 313 Val steps: 35 0.77s loss: 1.455779 acc: 49.210158 fscore_macro: 0.348049 val_loss: 1.539721 val_acc: 48.468468 val_fscore_macro: 0.345047\n",
            "Epoch 33: val_acc improved from 47.38739 to 48.46847, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_33.ckpt\n",
            "Epoch:  34/300 Train steps: 313 Val steps: 35 1.00s loss: 1.445992 acc: 49.510098 fscore_macro: 0.351042 val_loss: 1.531813 val_acc: 49.549550 val_fscore_macro: 0.355702\n",
            "Epoch 34: val_acc improved from 48.46847 to 49.54955, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_34.ckpt\n",
            "Epoch:  35/300 Train steps: 313 Val steps: 35 0.98s loss: 1.435132 acc: 50.349930 fscore_macro: 0.356572 val_loss: 1.519630 val_acc: 47.207207 val_fscore_macro: 0.336126\n",
            "Epoch:  36/300 Train steps: 313 Val steps: 35 1.08s loss: 1.424812 acc: 50.369926 fscore_macro: 0.356487 val_loss: 1.509910 val_acc: 48.828829 val_fscore_macro: 0.346819\n",
            "Epoch:  37/300 Train steps: 313 Val steps: 35 1.12s loss: 1.414701 acc: 51.669666 fscore_macro: 0.367833 val_loss: 1.501223 val_acc: 50.990991 val_fscore_macro: 0.366185\n",
            "Epoch 37: val_acc improved from 49.54955 to 50.99099, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_37.ckpt\n",
            "Epoch:  38/300 Train steps: 313 Val steps: 35 1.01s loss: 1.405753 acc: 52.009598 fscore_macro: 0.371242 val_loss: 1.487679 val_acc: 49.549550 val_fscore_macro: 0.348739\n",
            "Epoch:  39/300 Train steps: 313 Val steps: 35 0.76s loss: 1.393520 acc: 52.609478 fscore_macro: 0.381598 val_loss: 1.475696 val_acc: 50.810811 val_fscore_macro: 0.363285\n",
            "Epoch:  40/300 Train steps: 313 Val steps: 35 0.74s loss: 1.383800 acc: 53.289342 fscore_macro: 0.386591 val_loss: 1.465205 val_acc: 51.711712 val_fscore_macro: 0.367247\n",
            "Epoch 40: val_acc improved from 50.99099 to 51.71171, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_40.ckpt\n",
            "Epoch:  41/300 Train steps: 313 Val steps: 35 0.74s loss: 1.374834 acc: 52.969406 fscore_macro: 0.384391 val_loss: 1.463813 val_acc: 50.090090 val_fscore_macro: 0.363356\n",
            "Epoch:  42/300 Train steps: 313 Val steps: 35 0.78s loss: 1.365954 acc: 53.649270 fscore_macro: 0.396411 val_loss: 1.448382 val_acc: 53.153153 val_fscore_macro: 0.383824\n",
            "Epoch 42: val_acc improved from 51.71171 to 53.15315, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_42.ckpt\n",
            "Epoch:  43/300 Train steps: 313 Val steps: 35 0.78s loss: 1.357799 acc: 54.169166 fscore_macro: 0.398046 val_loss: 1.435928 val_acc: 53.153153 val_fscore_macro: 0.381830\n",
            "Epoch:  44/300 Train steps: 313 Val steps: 35 0.70s loss: 1.346677 acc: 54.949010 fscore_macro: 0.406988 val_loss: 1.456705 val_acc: 49.009009 val_fscore_macro: 0.357188\n",
            "Epoch:  45/300 Train steps: 313 Val steps: 35 0.76s loss: 1.338164 acc: 54.789042 fscore_macro: 0.403884 val_loss: 1.425416 val_acc: 50.630631 val_fscore_macro: 0.367360\n",
            "Epoch:  46/300 Train steps: 313 Val steps: 35 0.77s loss: 1.328339 acc: 55.588882 fscore_macro: 0.419516 val_loss: 1.407108 val_acc: 52.432432 val_fscore_macro: 0.376049\n",
            "Epoch:  47/300 Train steps: 313 Val steps: 35 0.76s loss: 1.320685 acc: 55.608878 fscore_macro: 0.420818 val_loss: 1.404996 val_acc: 54.594595 val_fscore_macro: 0.389387\n",
            "Epoch 47: val_acc improved from 53.15315 to 54.59459, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_47.ckpt\n",
            "Epoch:  48/300 Train steps: 313 Val steps: 35 0.78s loss: 1.314169 acc: 55.768846 fscore_macro: 0.419269 val_loss: 1.394375 val_acc: 54.054054 val_fscore_macro: 0.385165\n",
            "Epoch:  49/300 Train steps: 313 Val steps: 35 0.76s loss: 1.304509 acc: 55.988802 fscore_macro: 0.426340 val_loss: 1.386333 val_acc: 55.675676 val_fscore_macro: 0.400028\n",
            "Epoch 49: val_acc improved from 54.59459 to 55.67568, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_49.ckpt\n",
            "Epoch:  50/300 Train steps: 313 Val steps: 35 0.75s loss: 1.295036 acc: 56.228754 fscore_macro: 0.423035 val_loss: 1.382288 val_acc: 54.414414 val_fscore_macro: 0.389863\n",
            "Epoch:  51/300 Train steps: 313 Val steps: 35 0.75s loss: 1.288350 acc: 56.748650 fscore_macro: 0.429337 val_loss: 1.379338 val_acc: 54.054054 val_fscore_macro: 0.390852\n",
            "Epoch:  52/300 Train steps: 313 Val steps: 35 0.74s loss: 1.281926 acc: 56.808638 fscore_macro: 0.432025 val_loss: 1.365105 val_acc: 54.594595 val_fscore_macro: 0.403978\n",
            "Epoch:  53/300 Train steps: 313 Val steps: 35 0.76s loss: 1.273249 acc: 56.928614 fscore_macro: 0.429964 val_loss: 1.361955 val_acc: 54.594595 val_fscore_macro: 0.408887\n",
            "Epoch:  54/300 Train steps: 313 Val steps: 35 0.76s loss: 1.265227 acc: 57.208558 fscore_macro: 0.437480 val_loss: 1.361011 val_acc: 55.315315 val_fscore_macro: 0.402273\n",
            "Epoch:  55/300 Train steps: 313 Val steps: 35 0.73s loss: 1.260262 acc: 57.548490 fscore_macro: 0.440799 val_loss: 1.342785 val_acc: 56.396396 val_fscore_macro: 0.414857\n",
            "Epoch 55: val_acc improved from 55.67568 to 56.39640, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_55.ckpt\n",
            "Epoch:  56/300 Train steps: 313 Val steps: 35 0.75s loss: 1.251741 acc: 57.868426 fscore_macro: 0.443458 val_loss: 1.339861 val_acc: 56.576577 val_fscore_macro: 0.417815\n",
            "Epoch 56: val_acc improved from 56.39640 to 56.57658, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_56.ckpt\n",
            "Epoch:  57/300 Train steps: 313 Val steps: 35 0.77s loss: 1.247467 acc: 57.448510 fscore_macro: 0.438268 val_loss: 1.337215 val_acc: 54.414414 val_fscore_macro: 0.393706\n",
            "Epoch:  58/300 Train steps: 313 Val steps: 35 0.82s loss: 1.241793 acc: 58.308338 fscore_macro: 0.444026 val_loss: 1.326735 val_acc: 56.936937 val_fscore_macro: 0.419263\n",
            "Epoch 58: val_acc improved from 56.57658 to 56.93694, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_58.ckpt\n",
            "Epoch:  59/300 Train steps: 313 Val steps: 35 0.80s loss: 1.233389 acc: 58.108378 fscore_macro: 0.444869 val_loss: 1.320943 val_acc: 55.855856 val_fscore_macro: 0.421159\n",
            "Epoch:  60/300 Train steps: 313 Val steps: 35 0.76s loss: 1.227098 acc: 58.528294 fscore_macro: 0.449939 val_loss: 1.318780 val_acc: 53.873874 val_fscore_macro: 0.397636\n",
            "Epoch:  61/300 Train steps: 313 Val steps: 35 0.75s loss: 1.223093 acc: 58.708258 fscore_macro: 0.452065 val_loss: 1.317575 val_acc: 56.936937 val_fscore_macro: 0.419458\n",
            "Epoch 61: val_acc improved from 56.93694 to 56.93694, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_61.ckpt\n",
            "Epoch:  62/300 Train steps: 313 Val steps: 35 0.76s loss: 1.214841 acc: 58.768246 fscore_macro: 0.452148 val_loss: 1.332175 val_acc: 55.855856 val_fscore_macro: 0.421508\n",
            "Epoch:  63/300 Train steps: 313 Val steps: 35 0.75s loss: 1.211077 acc: 58.668266 fscore_macro: 0.453473 val_loss: 1.295067 val_acc: 57.297297 val_fscore_macro: 0.432849\n",
            "Epoch 63: val_acc improved from 56.93694 to 57.29730, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_63.ckpt\n",
            "Epoch:  64/300 Train steps: 313 Val steps: 35 0.74s loss: 1.206349 acc: 58.888222 fscore_macro: 0.452687 val_loss: 1.298258 val_acc: 57.837838 val_fscore_macro: 0.445610\n",
            "Epoch 64: val_acc improved from 57.29730 to 57.83784, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_64.ckpt\n",
            "Epoch:  65/300 Train steps: 313 Val steps: 35 0.76s loss: 1.201607 acc: 59.208158 fscore_macro: 0.455445 val_loss: 1.305010 val_acc: 56.756757 val_fscore_macro: 0.411968\n",
            "Epoch:  66/300 Train steps: 313 Val steps: 35 0.75s loss: 1.194666 acc: 58.908218 fscore_macro: 0.455867 val_loss: 1.283897 val_acc: 56.756757 val_fscore_macro: 0.437975\n",
            "Epoch:  67/300 Train steps: 313 Val steps: 35 0.74s loss: 1.190261 acc: 59.388122 fscore_macro: 0.460126 val_loss: 1.277705 val_acc: 57.477477 val_fscore_macro: 0.442208\n",
            "Epoch:  68/300 Train steps: 313 Val steps: 35 0.80s loss: 1.185445 acc: 59.568086 fscore_macro: 0.462436 val_loss: 1.280425 val_acc: 57.477477 val_fscore_macro: 0.445418\n",
            "Epoch:  69/300 Train steps: 313 Val steps: 35 0.77s loss: 1.179905 acc: 59.888022 fscore_macro: 0.465363 val_loss: 1.275771 val_acc: 57.117117 val_fscore_macro: 0.447021\n",
            "Epoch:  70/300 Train steps: 313 Val steps: 35 0.72s loss: 1.176412 acc: 60.027994 fscore_macro: 0.466308 val_loss: 1.262406 val_acc: 58.198198 val_fscore_macro: 0.451520\n",
            "Epoch 70: val_acc improved from 57.83784 to 58.19820, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_70.ckpt\n",
            "Epoch:  71/300 Train steps: 313 Val steps: 35 0.77s loss: 1.168944 acc: 59.708058 fscore_macro: 0.467278 val_loss: 1.267233 val_acc: 57.837838 val_fscore_macro: 0.458737\n",
            "Epoch:  72/300 Train steps: 313 Val steps: 35 0.75s loss: 1.164376 acc: 60.367926 fscore_macro: 0.474033 val_loss: 1.270067 val_acc: 57.477478 val_fscore_macro: 0.450190\n",
            "Epoch:  73/300 Train steps: 313 Val steps: 35 0.75s loss: 1.161404 acc: 60.287942 fscore_macro: 0.470997 val_loss: 1.260136 val_acc: 56.576577 val_fscore_macro: 0.440595\n",
            "Epoch:  74/300 Train steps: 313 Val steps: 35 0.78s loss: 1.157371 acc: 60.567886 fscore_macro: 0.473364 val_loss: 1.274041 val_acc: 55.315315 val_fscore_macro: 0.444921\n",
            "Epoch:  75/300 Train steps: 313 Val steps: 35 0.78s loss: 1.154903 acc: 60.547890 fscore_macro: 0.470389 val_loss: 1.250375 val_acc: 56.936937 val_fscore_macro: 0.445623\n",
            "Epoch:  76/300 Train steps: 313 Val steps: 35 0.73s loss: 1.148547 acc: 61.127774 fscore_macro: 0.482356 val_loss: 1.260044 val_acc: 56.756757 val_fscore_macro: 0.449154\n",
            "Epoch:  77/300 Train steps: 313 Val steps: 35 0.75s loss: 1.145613 acc: 60.547890 fscore_macro: 0.473333 val_loss: 1.263149 val_acc: 56.756757 val_fscore_macro: 0.456580\n",
            "Epoch:  78/300 Train steps: 313 Val steps: 35 0.73s loss: 1.139920 acc: 61.007798 fscore_macro: 0.475465 val_loss: 1.234333 val_acc: 58.198198 val_fscore_macro: 0.485996\n",
            "Epoch:  79/300 Train steps: 313 Val steps: 35 0.77s loss: 1.137599 acc: 61.587682 fscore_macro: 0.487079 val_loss: 1.251441 val_acc: 57.297297 val_fscore_macro: 0.476249\n",
            "Epoch:  80/300 Train steps: 313 Val steps: 35 0.74s loss: 1.133752 acc: 61.427714 fscore_macro: 0.487802 val_loss: 1.246634 val_acc: 56.936937 val_fscore_macro: 0.471226\n",
            "Epoch:  81/300 Train steps: 313 Val steps: 35 0.76s loss: 1.130343 acc: 60.807838 fscore_macro: 0.483929 val_loss: 1.231891 val_acc: 57.477478 val_fscore_macro: 0.481072\n",
            "Epoch:  82/300 Train steps: 313 Val steps: 35 0.77s loss: 1.125830 acc: 61.787642 fscore_macro: 0.496469 val_loss: 1.236483 val_acc: 56.936937 val_fscore_macro: 0.483053\n",
            "Epoch:  83/300 Train steps: 313 Val steps: 35 0.75s loss: 1.122303 acc: 61.267746 fscore_macro: 0.496448 val_loss: 1.233409 val_acc: 56.396396 val_fscore_macro: 0.471278\n",
            "Epoch:  84/300 Train steps: 313 Val steps: 35 0.75s loss: 1.118994 acc: 61.707658 fscore_macro: 0.498091 val_loss: 1.241262 val_acc: 58.198198 val_fscore_macro: 0.489660\n",
            "Epoch:  85/300 Train steps: 313 Val steps: 35 0.73s loss: 1.115188 acc: 62.167566 fscore_macro: 0.505351 val_loss: 1.210435 val_acc: 58.198198 val_fscore_macro: 0.486348\n",
            "Epoch:  86/300 Train steps: 313 Val steps: 35 0.75s loss: 1.108508 acc: 62.567487 fscore_macro: 0.512184 val_loss: 1.233713 val_acc: 57.837838 val_fscore_macro: 0.477885\n",
            "Epoch:  87/300 Train steps: 313 Val steps: 35 0.74s loss: 1.107043 acc: 62.407518 fscore_macro: 0.512469 val_loss: 1.222473 val_acc: 58.738739 val_fscore_macro: 0.489544\n",
            "Epoch 87: val_acc improved from 58.19820 to 58.73874, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_87.ckpt\n",
            "Epoch:  88/300 Train steps: 313 Val steps: 35 0.74s loss: 1.100929 acc: 62.527495 fscore_macro: 0.515774 val_loss: 1.215273 val_acc: 59.099099 val_fscore_macro: 0.490957\n",
            "Epoch 88: val_acc improved from 58.73874 to 59.09910, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_88.ckpt\n",
            "Epoch:  89/300 Train steps: 313 Val steps: 35 0.74s loss: 1.100669 acc: 62.267546 fscore_macro: 0.512852 val_loss: 1.228918 val_acc: 56.036036 val_fscore_macro: 0.474125\n",
            "Epoch:  90/300 Train steps: 313 Val steps: 35 0.78s loss: 1.095874 acc: 62.787443 fscore_macro: 0.518385 val_loss: 1.219976 val_acc: 57.477478 val_fscore_macro: 0.483164\n",
            "Epoch:  91/300 Train steps: 313 Val steps: 35 0.79s loss: 1.092061 acc: 62.407518 fscore_macro: 0.514744 val_loss: 1.217064 val_acc: 58.918919 val_fscore_macro: 0.502106\n",
            "Epoch:  92/300 Train steps: 313 Val steps: 35 0.76s loss: 1.089710 acc: 63.407319 fscore_macro: 0.522490 val_loss: 1.216097 val_acc: 58.378378 val_fscore_macro: 0.482790\n",
            "Epoch:  93/300 Train steps: 313 Val steps: 35 0.77s loss: 1.086739 acc: 62.547491 fscore_macro: 0.522478 val_loss: 1.209099 val_acc: 56.576577 val_fscore_macro: 0.468100\n",
            "Epoch:  94/300 Train steps: 313 Val steps: 35 0.78s loss: 1.081725 acc: 63.327335 fscore_macro: 0.529485 val_loss: 1.209203 val_acc: 59.099099 val_fscore_macro: 0.502302\n",
            "Epoch 94: val_acc improved from 59.09910 to 59.09910, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_94.ckpt\n",
            "Epoch:  95/300 Train steps: 313 Val steps: 35 0.77s loss: 1.078455 acc: 63.327335 fscore_macro: 0.526041 val_loss: 1.203596 val_acc: 59.639640 val_fscore_macro: 0.511093\n",
            "Epoch 95: val_acc improved from 59.09910 to 59.63964, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_95.ckpt\n",
            "Epoch:  96/300 Train steps: 313 Val steps: 35 0.75s loss: 1.078885 acc: 63.107379 fscore_macro: 0.526901 val_loss: 1.213323 val_acc: 58.198198 val_fscore_macro: 0.494983\n",
            "Epoch:  97/300 Train steps: 313 Val steps: 35 0.77s loss: 1.072978 acc: 63.267347 fscore_macro: 0.531194 val_loss: 1.242187 val_acc: 55.135135 val_fscore_macro: 0.466115\n",
            "Epoch:  98/300 Train steps: 313 Val steps: 35 0.76s loss: 1.070921 acc: 63.267347 fscore_macro: 0.532409 val_loss: 1.199342 val_acc: 59.099099 val_fscore_macro: 0.505183\n",
            "Epoch:  99/300 Train steps: 313 Val steps: 35 0.74s loss: 1.067895 acc: 63.467307 fscore_macro: 0.534203 val_loss: 1.182515 val_acc: 58.018018 val_fscore_macro: 0.490196\n",
            "Epoch: 100/300 Train steps: 313 Val steps: 35 0.75s loss: 1.064034 acc: 63.667267 fscore_macro: 0.543657 val_loss: 1.182809 val_acc: 58.738739 val_fscore_macro: 0.496385\n",
            "Epoch: 101/300 Train steps: 313 Val steps: 35 0.74s loss: 1.061211 acc: 63.887223 fscore_macro: 0.538993 val_loss: 1.184570 val_acc: 59.459460 val_fscore_macro: 0.519210\n",
            "Epoch: 102/300 Train steps: 313 Val steps: 35 0.76s loss: 1.058175 acc: 63.987203 fscore_macro: 0.542486 val_loss: 1.181772 val_acc: 60.180180 val_fscore_macro: 0.501121\n",
            "Epoch 102: val_acc improved from 59.63964 to 60.18018, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_102.ckpt\n",
            "Epoch: 103/300 Train steps: 313 Val steps: 35 0.75s loss: 1.053209 acc: 64.487103 fscore_macro: 0.552769 val_loss: 1.177684 val_acc: 58.738739 val_fscore_macro: 0.499341\n",
            "Epoch: 104/300 Train steps: 313 Val steps: 35 0.77s loss: 1.053772 acc: 63.987203 fscore_macro: 0.550004 val_loss: 1.191048 val_acc: 58.918919 val_fscore_macro: 0.499216\n",
            "Epoch: 105/300 Train steps: 313 Val steps: 35 0.74s loss: 1.048875 acc: 64.167167 fscore_macro: 0.548325 val_loss: 1.184575 val_acc: 58.918919 val_fscore_macro: 0.501421\n",
            "Epoch: 106/300 Train steps: 313 Val steps: 35 0.75s loss: 1.047024 acc: 64.307139 fscore_macro: 0.548394 val_loss: 1.175725 val_acc: 59.099099 val_fscore_macro: 0.499640\n",
            "Epoch: 107/300 Train steps: 313 Val steps: 35 0.75s loss: 1.044530 acc: 64.407119 fscore_macro: 0.554181 val_loss: 1.176788 val_acc: 58.018018 val_fscore_macro: 0.498193\n",
            "Epoch: 108/300 Train steps: 313 Val steps: 35 0.77s loss: 1.039877 acc: 64.807039 fscore_macro: 0.556708 val_loss: 1.200902 val_acc: 58.018018 val_fscore_macro: 0.489726\n",
            "Epoch: 109/300 Train steps: 313 Val steps: 35 0.93s loss: 1.039178 acc: 64.707059 fscore_macro: 0.556409 val_loss: 1.164735 val_acc: 59.639640 val_fscore_macro: 0.512600\n",
            "Epoch: 110/300 Train steps: 313 Val steps: 35 0.97s loss: 1.036152 acc: 64.987003 fscore_macro: 0.560606 val_loss: 1.175971 val_acc: 58.378378 val_fscore_macro: 0.502000\n",
            "Epoch: 111/300 Train steps: 313 Val steps: 35 1.08s loss: 1.035661 acc: 64.907019 fscore_macro: 0.563211 val_loss: 1.175452 val_acc: 58.378378 val_fscore_macro: 0.504539\n",
            "Epoch: 112/300 Train steps: 313 Val steps: 35 1.10s loss: 1.031781 acc: 64.607079 fscore_macro: 0.557438 val_loss: 1.164927 val_acc: 58.918919 val_fscore_macro: 0.497288\n",
            "Epoch: 113/300 Train steps: 313 Val steps: 35 0.95s loss: 1.029991 acc: 64.987003 fscore_macro: 0.560606 val_loss: 1.179762 val_acc: 58.738739 val_fscore_macro: 0.515489\n",
            "Epoch: 114/300 Train steps: 313 Val steps: 35 0.80s loss: 1.024478 acc: 65.226955 fscore_macro: 0.564685 val_loss: 1.153380 val_acc: 60.000000 val_fscore_macro: 0.513146\n",
            "Epoch: 115/300 Train steps: 313 Val steps: 35 0.74s loss: 1.022086 acc: 65.326935 fscore_macro: 0.567635 val_loss: 1.209010 val_acc: 58.198198 val_fscore_macro: 0.506812\n",
            "Epoch: 116/300 Train steps: 313 Val steps: 35 0.72s loss: 1.022491 acc: 65.706859 fscore_macro: 0.572744 val_loss: 1.161040 val_acc: 60.540541 val_fscore_macro: 0.518376\n",
            "Epoch 116: val_acc improved from 60.18018 to 60.54054, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_116.ckpt\n",
            "Epoch: 117/300 Train steps: 313 Val steps: 35 0.74s loss: 1.018001 acc: 65.366927 fscore_macro: 0.568636 val_loss: 1.180940 val_acc: 58.918919 val_fscore_macro: 0.513916\n",
            "Epoch: 118/300 Train steps: 313 Val steps: 35 0.74s loss: 1.015205 acc: 65.246951 fscore_macro: 0.567406 val_loss: 1.150110 val_acc: 58.558559 val_fscore_macro: 0.494289\n",
            "Epoch: 119/300 Train steps: 313 Val steps: 35 0.76s loss: 1.015336 acc: 65.466907 fscore_macro: 0.572644 val_loss: 1.158608 val_acc: 59.639640 val_fscore_macro: 0.513108\n",
            "Epoch: 120/300 Train steps: 313 Val steps: 35 0.74s loss: 1.010987 acc: 65.466907 fscore_macro: 0.570128 val_loss: 1.186068 val_acc: 56.036036 val_fscore_macro: 0.485889\n",
            "Epoch: 121/300 Train steps: 313 Val steps: 35 0.73s loss: 1.011902 acc: 65.586883 fscore_macro: 0.575575 val_loss: 1.168089 val_acc: 57.477477 val_fscore_macro: 0.493696\n",
            "Epoch: 122/300 Train steps: 313 Val steps: 35 0.74s loss: 1.006336 acc: 65.946811 fscore_macro: 0.573291 val_loss: 1.149083 val_acc: 58.918919 val_fscore_macro: 0.509574\n",
            "Epoch: 123/300 Train steps: 313 Val steps: 35 0.74s loss: 1.000987 acc: 65.426915 fscore_macro: 0.573662 val_loss: 1.164370 val_acc: 59.279279 val_fscore_macro: 0.517904\n",
            "Epoch: 124/300 Train steps: 313 Val steps: 35 0.73s loss: 1.001216 acc: 65.906819 fscore_macro: 0.579520 val_loss: 1.148105 val_acc: 59.639640 val_fscore_macro: 0.511830\n",
            "Epoch: 125/300 Train steps: 313 Val steps: 35 0.75s loss: 1.001435 acc: 65.986803 fscore_macro: 0.578036 val_loss: 1.145358 val_acc: 59.459460 val_fscore_macro: 0.504711\n",
            "Epoch: 126/300 Train steps: 313 Val steps: 35 0.75s loss: 0.998964 acc: 65.686863 fscore_macro: 0.577065 val_loss: 1.133831 val_acc: 59.819820 val_fscore_macro: 0.508214\n",
            "Epoch: 127/300 Train steps: 313 Val steps: 35 0.75s loss: 0.995676 acc: 66.026795 fscore_macro: 0.581376 val_loss: 1.137192 val_acc: 60.180180 val_fscore_macro: 0.508979\n",
            "Epoch: 128/300 Train steps: 313 Val steps: 35 0.73s loss: 0.993471 acc: 66.326735 fscore_macro: 0.586037 val_loss: 1.188875 val_acc: 57.657658 val_fscore_macro: 0.495038\n",
            "Epoch: 129/300 Train steps: 313 Val steps: 35 0.74s loss: 0.991863 acc: 66.446711 fscore_macro: 0.586130 val_loss: 1.168695 val_acc: 57.837838 val_fscore_macro: 0.494420\n",
            "Epoch: 130/300 Train steps: 313 Val steps: 35 0.83s loss: 0.987470 acc: 66.486703 fscore_macro: 0.586241 val_loss: 1.138464 val_acc: 60.000000 val_fscore_macro: 0.511978\n",
            "Epoch: 131/300 Train steps: 313 Val steps: 35 0.75s loss: 0.987688 acc: 66.406719 fscore_macro: 0.588558 val_loss: 1.134048 val_acc: 58.918919 val_fscore_macro: 0.506963\n",
            "Epoch: 132/300 Train steps: 313 Val steps: 35 0.76s loss: 0.985729 acc: 65.826835 fscore_macro: 0.578456 val_loss: 1.149035 val_acc: 60.540541 val_fscore_macro: 0.521169\n",
            "Epoch 132: val_acc improved from 60.54054 to 60.54054, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_132.ckpt\n",
            "Epoch: 133/300 Train steps: 313 Val steps: 35 0.75s loss: 0.984131 acc: 66.826635 fscore_macro: 0.589650 val_loss: 1.144303 val_acc: 59.099099 val_fscore_macro: 0.512826\n",
            "Epoch: 134/300 Train steps: 313 Val steps: 35 0.76s loss: 0.979559 acc: 66.306739 fscore_macro: 0.588700 val_loss: 1.158365 val_acc: 59.639640 val_fscore_macro: 0.514140\n",
            "Epoch: 135/300 Train steps: 313 Val steps: 35 0.73s loss: 0.981126 acc: 66.066787 fscore_macro: 0.584640 val_loss: 1.139502 val_acc: 60.720721 val_fscore_macro: 0.531599\n",
            "Epoch 135: val_acc improved from 60.54054 to 60.72072, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_135.ckpt\n",
            "Epoch: 136/300 Train steps: 313 Val steps: 35 0.73s loss: 0.975845 acc: 66.886623 fscore_macro: 0.592062 val_loss: 1.148308 val_acc: 58.558559 val_fscore_macro: 0.511456\n",
            "Epoch: 137/300 Train steps: 313 Val steps: 35 0.76s loss: 0.974793 acc: 66.746651 fscore_macro: 0.589970 val_loss: 1.130643 val_acc: 60.900901 val_fscore_macro: 0.518220\n",
            "Epoch 137: val_acc improved from 60.72072 to 60.90090, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_137.ckpt\n",
            "Epoch: 138/300 Train steps: 313 Val steps: 35 0.75s loss: 0.972482 acc: 67.146571 fscore_macro: 0.600458 val_loss: 1.156694 val_acc: 60.180180 val_fscore_macro: 0.519950\n",
            "Epoch: 139/300 Train steps: 313 Val steps: 35 0.72s loss: 0.971183 acc: 66.846631 fscore_macro: 0.591139 val_loss: 1.151997 val_acc: 60.720721 val_fscore_macro: 0.533081\n",
            "Epoch: 140/300 Train steps: 313 Val steps: 35 0.73s loss: 0.969803 acc: 67.006599 fscore_macro: 0.592164 val_loss: 1.166278 val_acc: 58.378378 val_fscore_macro: 0.523462\n",
            "Epoch: 141/300 Train steps: 313 Val steps: 35 0.75s loss: 0.967102 acc: 67.046591 fscore_macro: 0.596178 val_loss: 1.162853 val_acc: 59.099099 val_fscore_macro: 0.515271\n",
            "Epoch: 142/300 Train steps: 313 Val steps: 35 0.76s loss: 0.966486 acc: 67.126575 fscore_macro: 0.598372 val_loss: 1.147384 val_acc: 59.819820 val_fscore_macro: 0.528958\n",
            "Epoch: 143/300 Train steps: 313 Val steps: 35 0.75s loss: 0.963384 acc: 66.786643 fscore_macro: 0.598811 val_loss: 1.132149 val_acc: 60.180180 val_fscore_macro: 0.518084\n",
            "Epoch: 144/300 Train steps: 313 Val steps: 35 0.78s loss: 0.962432 acc: 67.326535 fscore_macro: 0.598866 val_loss: 1.151347 val_acc: 59.279279 val_fscore_macro: 0.511233\n",
            "Epoch: 145/300 Train steps: 313 Val steps: 35 0.75s loss: 0.959869 acc: 67.646471 fscore_macro: 0.603772 val_loss: 1.128018 val_acc: 60.000000 val_fscore_macro: 0.517482\n",
            "Epoch: 146/300 Train steps: 313 Val steps: 35 0.75s loss: 0.956222 acc: 67.526495 fscore_macro: 0.602868 val_loss: 1.129129 val_acc: 60.000000 val_fscore_macro: 0.513479\n",
            "Epoch: 147/300 Train steps: 313 Val steps: 35 0.77s loss: 0.954466 acc: 67.586483 fscore_macro: 0.608579 val_loss: 1.176622 val_acc: 57.477477 val_fscore_macro: 0.515885\n",
            "Epoch: 148/300 Train steps: 313 Val steps: 35 0.76s loss: 0.952186 acc: 67.566487 fscore_macro: 0.607039 val_loss: 1.134374 val_acc: 58.918919 val_fscore_macro: 0.508915\n",
            "Epoch: 149/300 Train steps: 313 Val steps: 35 0.77s loss: 0.952333 acc: 67.146571 fscore_macro: 0.602906 val_loss: 1.142840 val_acc: 58.018018 val_fscore_macro: 0.504624\n",
            "Epoch: 150/300 Train steps: 313 Val steps: 35 0.75s loss: 0.950099 acc: 67.146571 fscore_macro: 0.602432 val_loss: 1.120069 val_acc: 59.819820 val_fscore_macro: 0.525416\n",
            "Epoch: 151/300 Train steps: 313 Val steps: 35 0.76s loss: 0.948593 acc: 67.426515 fscore_macro: 0.604949 val_loss: 1.173274 val_acc: 57.837838 val_fscore_macro: 0.504809\n",
            "Epoch: 152/300 Train steps: 313 Val steps: 35 0.77s loss: 0.946837 acc: 67.546491 fscore_macro: 0.604698 val_loss: 1.134671 val_acc: 59.279279 val_fscore_macro: 0.509780\n",
            "Epoch: 153/300 Train steps: 313 Val steps: 35 0.75s loss: 0.943108 acc: 67.746451 fscore_macro: 0.610285 val_loss: 1.114789 val_acc: 59.459460 val_fscore_macro: 0.512926\n",
            "Epoch: 154/300 Train steps: 313 Val steps: 35 0.76s loss: 0.942190 acc: 67.326535 fscore_macro: 0.607253 val_loss: 1.143256 val_acc: 58.558559 val_fscore_macro: 0.505898\n",
            "Epoch: 155/300 Train steps: 313 Val steps: 35 0.78s loss: 0.938708 acc: 68.066387 fscore_macro: 0.609282 val_loss: 1.124030 val_acc: 58.918919 val_fscore_macro: 0.506362\n",
            "Epoch: 156/300 Train steps: 313 Val steps: 35 0.77s loss: 0.941145 acc: 67.866427 fscore_macro: 0.613128 val_loss: 1.152245 val_acc: 59.459459 val_fscore_macro: 0.527547\n",
            "Epoch: 157/300 Train steps: 313 Val steps: 35 0.78s loss: 0.940082 acc: 67.566487 fscore_macro: 0.612631 val_loss: 1.114034 val_acc: 59.639640 val_fscore_macro: 0.516874\n",
            "Epoch: 158/300 Train steps: 313 Val steps: 35 0.78s loss: 0.933900 acc: 67.826435 fscore_macro: 0.616524 val_loss: 1.112654 val_acc: 61.081081 val_fscore_macro: 0.529259\n",
            "Epoch 158: val_acc improved from 60.90090 to 61.08108, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_158.ckpt\n",
            "Epoch: 159/300 Train steps: 313 Val steps: 35 0.71s loss: 0.935560 acc: 68.126375 fscore_macro: 0.613898 val_loss: 1.123506 val_acc: 60.360360 val_fscore_macro: 0.522811\n",
            "Epoch: 160/300 Train steps: 313 Val steps: 35 0.74s loss: 0.932908 acc: 67.906419 fscore_macro: 0.612956 val_loss: 1.133804 val_acc: 59.819820 val_fscore_macro: 0.513506\n",
            "Epoch: 161/300 Train steps: 313 Val steps: 35 0.74s loss: 0.931169 acc: 68.206359 fscore_macro: 0.616105 val_loss: 1.141258 val_acc: 59.279279 val_fscore_macro: 0.534154\n",
            "Epoch: 162/300 Train steps: 313 Val steps: 35 0.82s loss: 0.924534 acc: 68.466307 fscore_macro: 0.621535 val_loss: 1.110092 val_acc: 60.540541 val_fscore_macro: 0.525958\n",
            "Epoch: 163/300 Train steps: 313 Val steps: 35 0.74s loss: 0.925000 acc: 68.406319 fscore_macro: 0.620879 val_loss: 1.126443 val_acc: 60.720721 val_fscore_macro: 0.524053\n",
            "Epoch: 164/300 Train steps: 313 Val steps: 35 0.78s loss: 0.929014 acc: 67.846431 fscore_macro: 0.612822 val_loss: 1.138353 val_acc: 59.819820 val_fscore_macro: 0.507063\n",
            "Epoch: 165/300 Train steps: 313 Val steps: 35 0.74s loss: 0.925004 acc: 68.566287 fscore_macro: 0.624675 val_loss: 1.125366 val_acc: 62.162162 val_fscore_macro: 0.549721\n",
            "Epoch 165: val_acc improved from 61.08108 to 62.16216, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_165.ckpt\n",
            "Epoch: 166/300 Train steps: 313 Val steps: 35 0.75s loss: 0.924387 acc: 68.226355 fscore_macro: 0.618859 val_loss: 1.126231 val_acc: 61.441441 val_fscore_macro: 0.539136\n",
            "Epoch: 167/300 Train steps: 313 Val steps: 35 0.83s loss: 0.920222 acc: 68.366327 fscore_macro: 0.619153 val_loss: 1.145286 val_acc: 60.180180 val_fscore_macro: 0.534585\n",
            "Epoch: 168/300 Train steps: 313 Val steps: 35 0.80s loss: 0.916524 acc: 68.766247 fscore_macro: 0.622769 val_loss: 1.121098 val_acc: 62.522523 val_fscore_macro: 0.551460\n",
            "Epoch 168: val_acc improved from 62.16216 to 62.52252, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_168.ckpt\n",
            "Epoch: 169/300 Train steps: 313 Val steps: 35 0.73s loss: 0.918000 acc: 68.506299 fscore_macro: 0.628177 val_loss: 1.113806 val_acc: 61.261261 val_fscore_macro: 0.536205\n",
            "Epoch: 170/300 Train steps: 313 Val steps: 35 0.77s loss: 0.915843 acc: 68.486303 fscore_macro: 0.623480 val_loss: 1.162012 val_acc: 58.018018 val_fscore_macro: 0.512203\n",
            "Epoch: 171/300 Train steps: 313 Val steps: 35 0.76s loss: 0.914041 acc: 68.546291 fscore_macro: 0.629052 val_loss: 1.141074 val_acc: 59.459459 val_fscore_macro: 0.528727\n",
            "Epoch: 172/300 Train steps: 313 Val steps: 35 0.75s loss: 0.913686 acc: 68.606279 fscore_macro: 0.627483 val_loss: 1.107449 val_acc: 60.360360 val_fscore_macro: 0.525689\n",
            "Epoch: 173/300 Train steps: 313 Val steps: 35 0.74s loss: 0.911242 acc: 68.966207 fscore_macro: 0.635545 val_loss: 1.127610 val_acc: 58.558559 val_fscore_macro: 0.522094\n",
            "Epoch: 174/300 Train steps: 313 Val steps: 35 0.81s loss: 0.907639 acc: 68.926215 fscore_macro: 0.632071 val_loss: 1.139692 val_acc: 60.180180 val_fscore_macro: 0.527558\n",
            "Epoch: 175/300 Train steps: 313 Val steps: 35 0.75s loss: 0.908554 acc: 68.926215 fscore_macro: 0.632952 val_loss: 1.119141 val_acc: 60.900901 val_fscore_macro: 0.534068\n",
            "Epoch: 176/300 Train steps: 313 Val steps: 35 0.76s loss: 0.910098 acc: 68.746251 fscore_macro: 0.624186 val_loss: 1.109032 val_acc: 60.900901 val_fscore_macro: 0.553123\n",
            "Epoch: 177/300 Train steps: 313 Val steps: 35 0.77s loss: 0.904544 acc: 68.806239 fscore_macro: 0.630897 val_loss: 1.126293 val_acc: 58.558559 val_fscore_macro: 0.515834\n",
            "Epoch: 178/300 Train steps: 313 Val steps: 35 0.77s loss: 0.903330 acc: 69.086183 fscore_macro: 0.633646 val_loss: 1.117982 val_acc: 61.801802 val_fscore_macro: 0.540093\n",
            "Epoch: 179/300 Train steps: 313 Val steps: 35 0.77s loss: 0.903033 acc: 69.146171 fscore_macro: 0.636779 val_loss: 1.100010 val_acc: 61.621622 val_fscore_macro: 0.525256\n",
            "Epoch: 180/300 Train steps: 313 Val steps: 35 0.77s loss: 0.899927 acc: 69.066187 fscore_macro: 0.632255 val_loss: 1.132522 val_acc: 58.378378 val_fscore_macro: 0.529948\n",
            "Epoch: 181/300 Train steps: 313 Val steps: 35 0.78s loss: 0.900454 acc: 69.106179 fscore_macro: 0.635141 val_loss: 1.115745 val_acc: 60.000000 val_fscore_macro: 0.526598\n",
            "Epoch: 182/300 Train steps: 313 Val steps: 35 0.77s loss: 0.898749 acc: 69.046191 fscore_macro: 0.633441 val_loss: 1.100386 val_acc: 60.540541 val_fscore_macro: 0.531264\n",
            "Epoch: 183/300 Train steps: 313 Val steps: 35 0.99s loss: 0.894850 acc: 69.166167 fscore_macro: 0.640917 val_loss: 1.182832 val_acc: 57.837838 val_fscore_macro: 0.501923\n",
            "Epoch: 184/300 Train steps: 313 Val steps: 35 1.05s loss: 0.895256 acc: 68.746251 fscore_macro: 0.632544 val_loss: 1.114841 val_acc: 60.000000 val_fscore_macro: 0.532549\n",
            "Epoch: 185/300 Train steps: 313 Val steps: 35 0.90s loss: 0.891146 acc: 69.426115 fscore_macro: 0.639555 val_loss: 1.103111 val_acc: 60.720721 val_fscore_macro: 0.545605\n",
            "Epoch: 186/300 Train steps: 313 Val steps: 35 0.86s loss: 0.890545 acc: 69.186163 fscore_macro: 0.638706 val_loss: 1.121760 val_acc: 59.819820 val_fscore_macro: 0.545224\n",
            "Epoch: 187/300 Train steps: 313 Val steps: 35 0.89s loss: 0.890965 acc: 69.406119 fscore_macro: 0.637567 val_loss: 1.118603 val_acc: 59.819820 val_fscore_macro: 0.532898\n",
            "Epoch: 188/300 Train steps: 313 Val steps: 35 1.01s loss: 0.886670 acc: 69.926015 fscore_macro: 0.646338 val_loss: 1.129707 val_acc: 61.441441 val_fscore_macro: 0.569754\n",
            "Epoch: 189/300 Train steps: 313 Val steps: 35 0.85s loss: 0.886192 acc: 69.686063 fscore_macro: 0.649500 val_loss: 1.114704 val_acc: 61.261261 val_fscore_macro: 0.540628\n",
            "Epoch: 190/300 Train steps: 313 Val steps: 35 0.74s loss: 0.879906 acc: 70.365927 fscore_macro: 0.651574 val_loss: 1.138457 val_acc: 57.837838 val_fscore_macro: 0.520050\n",
            "Epoch: 191/300 Train steps: 313 Val steps: 35 0.75s loss: 0.883312 acc: 69.526095 fscore_macro: 0.642269 val_loss: 1.115992 val_acc: 61.081081 val_fscore_macro: 0.545310\n",
            "Epoch: 192/300 Train steps: 313 Val steps: 35 0.73s loss: 0.883628 acc: 69.246151 fscore_macro: 0.646397 val_loss: 1.112749 val_acc: 58.918919 val_fscore_macro: 0.528418\n",
            "Epoch: 193/300 Train steps: 313 Val steps: 35 0.76s loss: 0.880424 acc: 69.726055 fscore_macro: 0.654637 val_loss: 1.092705 val_acc: 61.441441 val_fscore_macro: 0.551145\n",
            "Epoch: 194/300 Train steps: 313 Val steps: 35 0.75s loss: 0.876999 acc: 70.045991 fscore_macro: 0.654769 val_loss: 1.104486 val_acc: 60.720721 val_fscore_macro: 0.528468\n",
            "Epoch: 195/300 Train steps: 313 Val steps: 35 0.78s loss: 0.877379 acc: 69.766047 fscore_macro: 0.648369 val_loss: 1.104057 val_acc: 62.342342 val_fscore_macro: 0.548638\n",
            "Epoch: 196/300 Train steps: 313 Val steps: 35 0.76s loss: 0.873514 acc: 69.946011 fscore_macro: 0.649051 val_loss: 1.134662 val_acc: 58.018018 val_fscore_macro: 0.509974\n",
            "Epoch: 197/300 Train steps: 313 Val steps: 35 0.75s loss: 0.875223 acc: 69.686063 fscore_macro: 0.651663 val_loss: 1.119962 val_acc: 59.279279 val_fscore_macro: 0.535888\n",
            "Epoch: 198/300 Train steps: 313 Val steps: 35 0.75s loss: 0.872587 acc: 69.746051 fscore_macro: 0.650517 val_loss: 1.096733 val_acc: 62.882883 val_fscore_macro: 0.570528\n",
            "Epoch 198: val_acc improved from 62.52252 to 62.88288, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_198.ckpt\n",
            "Epoch: 199/300 Train steps: 313 Val steps: 35 0.77s loss: 0.872468 acc: 69.646071 fscore_macro: 0.649745 val_loss: 1.132204 val_acc: 59.459459 val_fscore_macro: 0.521541\n",
            "Epoch: 200/300 Train steps: 313 Val steps: 35 0.73s loss: 0.872321 acc: 70.025995 fscore_macro: 0.647726 val_loss: 1.127025 val_acc: 60.000000 val_fscore_macro: 0.527720\n",
            "Epoch: 201/300 Train steps: 313 Val steps: 35 0.76s loss: 0.867752 acc: 70.165967 fscore_macro: 0.653213 val_loss: 1.097786 val_acc: 60.360360 val_fscore_macro: 0.531660\n",
            "Epoch: 202/300 Train steps: 313 Val steps: 35 0.76s loss: 0.867399 acc: 70.565887 fscore_macro: 0.659553 val_loss: 1.098037 val_acc: 61.981982 val_fscore_macro: 0.551369\n",
            "Epoch: 203/300 Train steps: 313 Val steps: 35 0.75s loss: 0.866468 acc: 69.586083 fscore_macro: 0.649483 val_loss: 1.095482 val_acc: 61.981982 val_fscore_macro: 0.557716\n",
            "Epoch: 204/300 Train steps: 313 Val steps: 35 0.77s loss: 0.863242 acc: 70.145971 fscore_macro: 0.656071 val_loss: 1.104287 val_acc: 59.459460 val_fscore_macro: 0.518443\n",
            "Epoch: 205/300 Train steps: 313 Val steps: 35 0.75s loss: 0.862802 acc: 70.125975 fscore_macro: 0.654024 val_loss: 1.105562 val_acc: 62.522523 val_fscore_macro: 0.554885\n",
            "Epoch: 206/300 Train steps: 313 Val steps: 35 0.72s loss: 0.861360 acc: 70.485903 fscore_macro: 0.659664 val_loss: 1.115296 val_acc: 62.342342 val_fscore_macro: 0.554212\n",
            "Epoch: 207/300 Train steps: 313 Val steps: 35 0.76s loss: 0.860133 acc: 70.865827 fscore_macro: 0.667704 val_loss: 1.098636 val_acc: 59.639640 val_fscore_macro: 0.543844\n",
            "Epoch: 208/300 Train steps: 313 Val steps: 35 0.74s loss: 0.860191 acc: 70.025995 fscore_macro: 0.656947 val_loss: 1.099807 val_acc: 60.540541 val_fscore_macro: 0.542048\n",
            "Epoch: 209/300 Train steps: 313 Val steps: 35 0.74s loss: 0.855217 acc: 70.325935 fscore_macro: 0.662183 val_loss: 1.130410 val_acc: 61.261261 val_fscore_macro: 0.549795\n",
            "Epoch: 210/300 Train steps: 313 Val steps: 35 0.77s loss: 0.859204 acc: 70.425915 fscore_macro: 0.663311 val_loss: 1.100980 val_acc: 62.702703 val_fscore_macro: 0.557079\n",
            "Epoch: 211/300 Train steps: 313 Val steps: 35 0.72s loss: 0.853070 acc: 71.365727 fscore_macro: 0.665913 val_loss: 1.112298 val_acc: 61.621622 val_fscore_macro: 0.572681\n",
            "Epoch: 212/300 Train steps: 313 Val steps: 35 0.75s loss: 0.855820 acc: 70.585883 fscore_macro: 0.663899 val_loss: 1.105535 val_acc: 59.819820 val_fscore_macro: 0.526445\n",
            "Epoch: 213/300 Train steps: 313 Val steps: 35 0.74s loss: 0.850539 acc: 70.905819 fscore_macro: 0.659888 val_loss: 1.103783 val_acc: 60.540541 val_fscore_macro: 0.543879\n",
            "Epoch: 214/300 Train steps: 313 Val steps: 35 0.76s loss: 0.850940 acc: 70.885823 fscore_macro: 0.666953 val_loss: 1.112397 val_acc: 59.279279 val_fscore_macro: 0.553185\n",
            "Epoch: 215/300 Train steps: 313 Val steps: 35 0.79s loss: 0.848685 acc: 70.905819 fscore_macro: 0.665426 val_loss: 1.131059 val_acc: 60.180180 val_fscore_macro: 0.542118\n",
            "Epoch: 216/300 Train steps: 313 Val steps: 35 0.77s loss: 0.850586 acc: 70.325935 fscore_macro: 0.661717 val_loss: 1.117576 val_acc: 60.180180 val_fscore_macro: 0.537534\n",
            "Epoch: 217/300 Train steps: 313 Val steps: 35 0.76s loss: 0.846128 acc: 71.045791 fscore_macro: 0.664926 val_loss: 1.090241 val_acc: 60.540541 val_fscore_macro: 0.545038\n",
            "Epoch: 218/300 Train steps: 313 Val steps: 35 0.77s loss: 0.843401 acc: 70.905819 fscore_macro: 0.670236 val_loss: 1.098483 val_acc: 60.360360 val_fscore_macro: 0.536789\n",
            "Epoch: 219/300 Train steps: 313 Val steps: 35 0.75s loss: 0.841133 acc: 71.145771 fscore_macro: 0.666860 val_loss: 1.114267 val_acc: 60.180180 val_fscore_macro: 0.538371\n",
            "Epoch: 220/300 Train steps: 313 Val steps: 35 0.77s loss: 0.844417 acc: 70.665867 fscore_macro: 0.668731 val_loss: 1.121786 val_acc: 61.621622 val_fscore_macro: 0.552397\n",
            "Epoch: 221/300 Train steps: 313 Val steps: 35 0.76s loss: 0.841684 acc: 71.125775 fscore_macro: 0.675373 val_loss: 1.153738 val_acc: 59.819820 val_fscore_macro: 0.540383\n",
            "Epoch: 222/300 Train steps: 313 Val steps: 35 0.76s loss: 0.836848 acc: 71.005799 fscore_macro: 0.668599 val_loss: 1.148150 val_acc: 58.378378 val_fscore_macro: 0.508457\n",
            "Epoch: 223/300 Train steps: 313 Val steps: 35 0.78s loss: 0.839794 acc: 70.685863 fscore_macro: 0.669479 val_loss: 1.087453 val_acc: 62.702703 val_fscore_macro: 0.560886\n",
            "Epoch: 224/300 Train steps: 313 Val steps: 35 0.77s loss: 0.835053 acc: 71.025795 fscore_macro: 0.666857 val_loss: 1.095764 val_acc: 61.621622 val_fscore_macro: 0.552384\n",
            "Epoch: 225/300 Train steps: 313 Val steps: 35 0.75s loss: 0.835869 acc: 70.925815 fscore_macro: 0.668718 val_loss: 1.099946 val_acc: 59.819820 val_fscore_macro: 0.528867\n",
            "Epoch: 226/300 Train steps: 313 Val steps: 35 0.75s loss: 0.831555 acc: 71.405719 fscore_macro: 0.675186 val_loss: 1.107267 val_acc: 61.441441 val_fscore_macro: 0.548958\n",
            "Epoch: 227/300 Train steps: 313 Val steps: 35 0.72s loss: 0.834253 acc: 71.105779 fscore_macro: 0.670070 val_loss: 1.109934 val_acc: 62.162162 val_fscore_macro: 0.562386\n",
            "Epoch: 228/300 Train steps: 313 Val steps: 35 0.77s loss: 0.830423 acc: 71.405719 fscore_macro: 0.678886 val_loss: 1.100258 val_acc: 62.162162 val_fscore_macro: 0.558189\n",
            "Epoch: 229/300 Train steps: 313 Val steps: 35 0.74s loss: 0.830663 acc: 71.365727 fscore_macro: 0.671492 val_loss: 1.128996 val_acc: 58.558559 val_fscore_macro: 0.541766\n",
            "Epoch: 230/300 Train steps: 313 Val steps: 35 0.75s loss: 0.828365 acc: 71.605679 fscore_macro: 0.671077 val_loss: 1.140538 val_acc: 60.720721 val_fscore_macro: 0.549291\n",
            "Epoch: 231/300 Train steps: 313 Val steps: 35 0.75s loss: 0.825138 acc: 71.305739 fscore_macro: 0.679406 val_loss: 1.142625 val_acc: 60.540541 val_fscore_macro: 0.558202\n",
            "Epoch: 232/300 Train steps: 313 Val steps: 35 0.76s loss: 0.827344 acc: 71.825635 fscore_macro: 0.678631 val_loss: 1.090828 val_acc: 61.621622 val_fscore_macro: 0.558681\n",
            "Epoch: 233/300 Train steps: 313 Val steps: 35 0.78s loss: 0.826613 acc: 71.325735 fscore_macro: 0.673626 val_loss: 1.093791 val_acc: 61.981982 val_fscore_macro: 0.578529\n",
            "Epoch: 234/300 Train steps: 313 Val steps: 35 0.76s loss: 0.823819 acc: 72.065587 fscore_macro: 0.679259 val_loss: 1.111189 val_acc: 59.639640 val_fscore_macro: 0.531482\n",
            "Epoch: 235/300 Train steps: 313 Val steps: 35 0.74s loss: 0.822560 acc: 71.585683 fscore_macro: 0.675794 val_loss: 1.097593 val_acc: 62.522523 val_fscore_macro: 0.557691\n",
            "Epoch: 236/300 Train steps: 313 Val steps: 35 0.75s loss: 0.820271 acc: 71.905619 fscore_macro: 0.683794 val_loss: 1.116517 val_acc: 61.621622 val_fscore_macro: 0.553820\n",
            "Epoch: 237/300 Train steps: 313 Val steps: 35 0.73s loss: 0.818946 acc: 71.865627 fscore_macro: 0.681020 val_loss: 1.091832 val_acc: 62.522523 val_fscore_macro: 0.578402\n",
            "Epoch: 238/300 Train steps: 313 Val steps: 35 0.82s loss: 0.819624 acc: 72.345531 fscore_macro: 0.689138 val_loss: 1.138347 val_acc: 59.459459 val_fscore_macro: 0.546469\n",
            "Epoch: 239/300 Train steps: 313 Val steps: 35 0.78s loss: 0.817309 acc: 71.785643 fscore_macro: 0.681505 val_loss: 1.091885 val_acc: 62.162162 val_fscore_macro: 0.555567\n",
            "Epoch: 240/300 Train steps: 313 Val steps: 35 0.76s loss: 0.815799 acc: 71.645671 fscore_macro: 0.681094 val_loss: 1.128836 val_acc: 60.720721 val_fscore_macro: 0.547319\n",
            "Epoch: 241/300 Train steps: 313 Val steps: 35 0.75s loss: 0.816033 acc: 72.065587 fscore_macro: 0.678026 val_loss: 1.113009 val_acc: 61.261261 val_fscore_macro: 0.551037\n",
            "Epoch: 242/300 Train steps: 313 Val steps: 35 0.75s loss: 0.813979 acc: 72.525495 fscore_macro: 0.692058 val_loss: 1.102956 val_acc: 60.000000 val_fscore_macro: 0.541397\n",
            "Epoch: 243/300 Train steps: 313 Val steps: 35 0.75s loss: 0.812719 acc: 71.985603 fscore_macro: 0.683330 val_loss: 1.110954 val_acc: 60.540541 val_fscore_macro: 0.546242\n",
            "Epoch: 244/300 Train steps: 313 Val steps: 35 0.74s loss: 0.807878 acc: 72.485503 fscore_macro: 0.685661 val_loss: 1.090978 val_acc: 63.603604 val_fscore_macro: 0.582504\n",
            "Epoch 244: val_acc improved from 62.88288 to 63.60360, saving file to model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_244.ckpt\n",
            "Epoch: 245/300 Train steps: 313 Val steps: 35 0.76s loss: 0.806876 acc: 72.205559 fscore_macro: 0.688436 val_loss: 1.090700 val_acc: 60.540541 val_fscore_macro: 0.536222\n",
            "Epoch: 246/300 Train steps: 313 Val steps: 35 0.75s loss: 0.810530 acc: 72.205559 fscore_macro: 0.684722 val_loss: 1.083398 val_acc: 62.882883 val_fscore_macro: 0.581862\n",
            "Epoch: 247/300 Train steps: 313 Val steps: 35 0.77s loss: 0.808437 acc: 72.265547 fscore_macro: 0.689028 val_loss: 1.098074 val_acc: 61.981982 val_fscore_macro: 0.568912\n",
            "Epoch: 248/300 Train steps: 313 Val steps: 35 0.73s loss: 0.805338 acc: 72.385523 fscore_macro: 0.689219 val_loss: 1.118550 val_acc: 61.801802 val_fscore_macro: 0.564314\n",
            "Epoch: 249/300 Train steps: 313 Val steps: 35 0.79s loss: 0.802061 acc: 72.125575 fscore_macro: 0.687028 val_loss: 1.105620 val_acc: 59.639640 val_fscore_macro: 0.535552\n",
            "Epoch: 250/300 Train steps: 313 Val steps: 35 0.75s loss: 0.798599 acc: 72.525495 fscore_macro: 0.686147 val_loss: 1.127674 val_acc: 61.261261 val_fscore_macro: 0.549812\n",
            "Epoch: 251/300 Train steps: 313 Val steps: 35 0.75s loss: 0.801607 acc: 72.285543 fscore_macro: 0.688537 val_loss: 1.098878 val_acc: 61.441441 val_fscore_macro: 0.570817\n",
            "Epoch: 252/300 Train steps: 313 Val steps: 35 0.79s loss: 0.798581 acc: 72.705459 fscore_macro: 0.689656 val_loss: 1.083778 val_acc: 62.342342 val_fscore_macro: 0.585503\n",
            "Epoch: 253/300 Train steps: 313 Val steps: 35 0.75s loss: 0.797915 acc: 72.785443 fscore_macro: 0.688740 val_loss: 1.100517 val_acc: 60.540541 val_fscore_macro: 0.545973\n",
            "Epoch: 254/300 Train steps: 313 Val steps: 35 0.74s loss: 0.795562 acc: 73.085383 fscore_macro: 0.696120 val_loss: 1.102440 val_acc: 59.819820 val_fscore_macro: 0.532758\n",
            "Epoch: 255/300 Train steps: 313 Val steps: 35 0.77s loss: 0.795823 acc: 72.705459 fscore_macro: 0.693932 val_loss: 1.097018 val_acc: 62.522523 val_fscore_macro: 0.568239\n",
            "Epoch: 256/300 Train steps: 313 Val steps: 35 0.77s loss: 0.793212 acc: 72.705459 fscore_macro: 0.688565 val_loss: 1.110564 val_acc: 61.081081 val_fscore_macro: 0.562125\n",
            "Epoch: 257/300 Train steps: 313 Val steps: 35 0.76s loss: 0.793238 acc: 72.385523 fscore_macro: 0.693610 val_loss: 1.092621 val_acc: 61.081081 val_fscore_macro: 0.565316\n",
            "Epoch: 258/300 Train steps: 313 Val steps: 35 0.76s loss: 0.791566 acc: 72.525495 fscore_macro: 0.697003 val_loss: 1.127651 val_acc: 61.801802 val_fscore_macro: 0.558004\n",
            "Epoch: 259/300 Train steps: 313 Val steps: 35 0.96s loss: 0.793578 acc: 72.285543 fscore_macro: 0.691679 val_loss: 1.122148 val_acc: 60.360360 val_fscore_macro: 0.558575\n",
            "Epoch: 260/300 Train steps: 313 Val steps: 35 1.01s loss: 0.789859 acc: 72.965407 fscore_macro: 0.695953 val_loss: 1.128945 val_acc: 59.639640 val_fscore_macro: 0.537362\n",
            "Epoch: 261/300 Train steps: 313 Val steps: 35 0.91s loss: 0.788427 acc: 72.885423 fscore_macro: 0.697749 val_loss: 1.102616 val_acc: 60.900901 val_fscore_macro: 0.546804\n",
            "Epoch: 262/300 Train steps: 313 Val steps: 35 0.94s loss: 0.791012 acc: 72.725455 fscore_macro: 0.693092 val_loss: 1.098354 val_acc: 62.342342 val_fscore_macro: 0.589244\n",
            "Epoch: 263/300 Train steps: 313 Val steps: 35 1.07s loss: 0.785541 acc: 72.745451 fscore_macro: 0.699108 val_loss: 1.101111 val_acc: 61.801802 val_fscore_macro: 0.565624\n",
            "Epoch: 264/300 Train steps: 313 Val steps: 35 0.93s loss: 0.783651 acc: 72.865427 fscore_macro: 0.700362 val_loss: 1.143025 val_acc: 60.000000 val_fscore_macro: 0.536266\n",
            "Epoch: 265/300 Train steps: 313 Val steps: 35 0.77s loss: 0.781199 acc: 72.805439 fscore_macro: 0.695752 val_loss: 1.113623 val_acc: 63.423423 val_fscore_macro: 0.596450\n",
            "Epoch: 266/300 Train steps: 313 Val steps: 35 0.77s loss: 0.782766 acc: 72.885423 fscore_macro: 0.699964 val_loss: 1.095704 val_acc: 60.900901 val_fscore_macro: 0.538574\n",
            "Epoch: 267/300 Train steps: 313 Val steps: 35 0.83s loss: 0.781912 acc: 72.845431 fscore_macro: 0.698479 val_loss: 1.114932 val_acc: 61.441441 val_fscore_macro: 0.587485\n",
            "Epoch: 268/300 Train steps: 313 Val steps: 35 0.74s loss: 0.776951 acc: 73.325335 fscore_macro: 0.702592 val_loss: 1.109384 val_acc: 61.801802 val_fscore_macro: 0.575825\n",
            "Epoch: 269/300 Train steps: 313 Val steps: 35 0.74s loss: 0.779238 acc: 73.245351 fscore_macro: 0.706265 val_loss: 1.109300 val_acc: 61.081081 val_fscore_macro: 0.551809\n",
            "Epoch: 270/300 Train steps: 313 Val steps: 35 0.77s loss: 0.777996 acc: 72.725455 fscore_macro: 0.698060 val_loss: 1.109053 val_acc: 60.540541 val_fscore_macro: 0.548290\n",
            "Epoch: 271/300 Train steps: 313 Val steps: 35 0.76s loss: 0.775832 acc: 73.405319 fscore_macro: 0.701084 val_loss: 1.091498 val_acc: 60.360360 val_fscore_macro: 0.545075\n",
            "Epoch: 272/300 Train steps: 313 Val steps: 35 0.78s loss: 0.774069 acc: 73.285343 fscore_macro: 0.698507 val_loss: 1.126147 val_acc: 61.981982 val_fscore_macro: 0.565187\n",
            "Epoch: 273/300 Train steps: 313 Val steps: 35 0.76s loss: 0.775527 acc: 73.425315 fscore_macro: 0.706336 val_loss: 1.106936 val_acc: 60.360360 val_fscore_macro: 0.531788\n",
            "Epoch: 274/300 Train steps: 313 Val steps: 35 0.75s loss: 0.770555 acc: 73.825235 fscore_macro: 0.709467 val_loss: 1.113480 val_acc: 60.360360 val_fscore_macro: 0.555041\n",
            "Epoch: 275/300 Train steps: 313 Val steps: 35 0.75s loss: 0.771248 acc: 73.185363 fscore_macro: 0.702875 val_loss: 1.083606 val_acc: 61.981982 val_fscore_macro: 0.585534\n",
            "Epoch: 276/300 Train steps: 313 Val steps: 35 0.75s loss: 0.769223 acc: 73.385323 fscore_macro: 0.701935 val_loss: 1.130363 val_acc: 61.801802 val_fscore_macro: 0.570146\n",
            "Epoch: 277/300 Train steps: 313 Val steps: 35 0.76s loss: 0.769562 acc: 73.225355 fscore_macro: 0.702199 val_loss: 1.109532 val_acc: 61.261261 val_fscore_macro: 0.549146\n",
            "Epoch: 278/300 Train steps: 313 Val steps: 35 0.77s loss: 0.765249 acc: 73.785243 fscore_macro: 0.703957 val_loss: 1.172259 val_acc: 60.000000 val_fscore_macro: 0.567298\n",
            "Epoch: 279/300 Train steps: 313 Val steps: 35 0.77s loss: 0.766218 acc: 73.565287 fscore_macro: 0.709036 val_loss: 1.123254 val_acc: 60.000000 val_fscore_macro: 0.550415\n",
            "Epoch: 280/300 Train steps: 313 Val steps: 35 0.74s loss: 0.765937 acc: 73.585283 fscore_macro: 0.712721 val_loss: 1.122249 val_acc: 60.360360 val_fscore_macro: 0.554638\n",
            "Epoch: 281/300 Train steps: 313 Val steps: 35 0.76s loss: 0.763659 acc: 73.465307 fscore_macro: 0.700627 val_loss: 1.081194 val_acc: 62.522523 val_fscore_macro: 0.573366\n",
            "Epoch: 282/300 Train steps: 313 Val steps: 35 0.78s loss: 0.762284 acc: 73.645271 fscore_macro: 0.710510 val_loss: 1.165111 val_acc: 57.117117 val_fscore_macro: 0.544808\n",
            "Epoch: 283/300 Train steps: 313 Val steps: 35 0.75s loss: 0.763088 acc: 74.005199 fscore_macro: 0.710283 val_loss: 1.093632 val_acc: 59.459460 val_fscore_macro: 0.542456\n",
            "Epoch: 284/300 Train steps: 313 Val steps: 35 0.75s loss: 0.761221 acc: 73.505299 fscore_macro: 0.706789 val_loss: 1.108860 val_acc: 62.342342 val_fscore_macro: 0.579556\n",
            "Epoch: 285/300 Train steps: 313 Val steps: 35 0.76s loss: 0.758706 acc: 74.085183 fscore_macro: 0.715673 val_loss: 1.125735 val_acc: 58.198198 val_fscore_macro: 0.547719\n",
            "Epoch: 286/300 Train steps: 313 Val steps: 35 0.76s loss: 0.756369 acc: 74.105179 fscore_macro: 0.709241 val_loss: 1.108246 val_acc: 61.981982 val_fscore_macro: 0.549578\n",
            "Epoch: 287/300 Train steps: 313 Val steps: 35 0.75s loss: 0.749910 acc: 74.085183 fscore_macro: 0.715280 val_loss: 1.081663 val_acc: 61.981982 val_fscore_macro: 0.583594\n",
            "Epoch: 288/300 Train steps: 313 Val steps: 35 0.74s loss: 0.755554 acc: 74.145171 fscore_macro: 0.714658 val_loss: 1.115032 val_acc: 60.180180 val_fscore_macro: 0.547328\n",
            "Epoch: 289/300 Train steps: 313 Val steps: 35 0.77s loss: 0.752826 acc: 74.305139 fscore_macro: 0.716717 val_loss: 1.117837 val_acc: 62.702703 val_fscore_macro: 0.568148\n",
            "Epoch: 290/300 Train steps: 313 Val steps: 35 0.77s loss: 0.753367 acc: 73.685263 fscore_macro: 0.712180 val_loss: 1.128054 val_acc: 60.900901 val_fscore_macro: 0.564262\n",
            "Epoch: 291/300 Train steps: 313 Val steps: 35 0.74s loss: 0.752857 acc: 74.045191 fscore_macro: 0.713801 val_loss: 1.096511 val_acc: 62.342342 val_fscore_macro: 0.575793\n",
            "Epoch: 292/300 Train steps: 313 Val steps: 35 0.75s loss: 0.750167 acc: 74.145171 fscore_macro: 0.715487 val_loss: 1.121816 val_acc: 61.261261 val_fscore_macro: 0.573114\n",
            "Epoch: 293/300 Train steps: 313 Val steps: 35 0.75s loss: 0.747554 acc: 74.425115 fscore_macro: 0.718534 val_loss: 1.140290 val_acc: 61.261261 val_fscore_macro: 0.554054\n",
            "Epoch: 294/300 Train steps: 313 Val steps: 35 0.77s loss: 0.744058 acc: 74.905019 fscore_macro: 0.723318 val_loss: 1.114180 val_acc: 60.540541 val_fscore_macro: 0.563331\n",
            "Epoch: 295/300 Train steps: 313 Val steps: 35 0.75s loss: 0.743706 acc: 74.565087 fscore_macro: 0.719761 val_loss: 1.108425 val_acc: 61.981982 val_fscore_macro: 0.572980\n",
            "Epoch: 296/300 Train steps: 313 Val steps: 35 0.74s loss: 0.748167 acc: 74.185163 fscore_macro: 0.717559 val_loss: 1.122917 val_acc: 58.198198 val_fscore_macro: 0.514571\n",
            "Epoch: 297/300 Train steps: 313 Val steps: 35 0.76s loss: 0.742360 acc: 74.525095 fscore_macro: 0.716422 val_loss: 1.101850 val_acc: 62.162162 val_fscore_macro: 0.585584\n",
            "Epoch: 298/300 Train steps: 313 Val steps: 35 0.76s loss: 0.741500 acc: 74.605079 fscore_macro: 0.719378 val_loss: 1.113292 val_acc: 61.081081 val_fscore_macro: 0.565316\n",
            "Epoch: 299/300 Train steps: 313 Val steps: 35 0.77s loss: 0.739573 acc: 74.665067 fscore_macro: 0.721668 val_loss: 1.132694 val_acc: 59.639640 val_fscore_macro: 0.528520\n",
            "Epoch: 300/300 Train steps: 313 Val steps: 35 0.76s loss: 0.741059 acc: 74.405119 fscore_macro: 0.718429 val_loss: 1.143192 val_acc: 60.720721 val_fscore_macro: 0.541689\n",
            "Restoring data from model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_244.ckpt\n",
            "Found best checkpoint at epoch: 244\n",
            "lr: 0.01, loss: 0.807878, acc: 72.4855, fscore_macro: 0.685661, val_loss: 1.09098, val_acc: 63.6036, val_fscore_macro: 0.582504\n",
            "Loading checkpoint model/fasttext/maxpool_embedding_mlp_400/checkpoint_epoch_244.ckpt\n",
            "Running test\n",
            "Test steps: 32 1.52s test_loss: 0.676591 test_acc: 77.800000 test_fscore_macro: 0.694970    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQKSwiiwPfzU"
      },
      "source": [
        "#### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "execfile(\"classif_word2vec.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzSSSyiTvYP7",
        "outputId": "50a886c5-ded7-4fad-83d3-fc60a1fb3a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Taille de couche cachée égale à : 100, avec les embeddings préentraînés de word2vec aggrégés avec la méthode average :\n",
            "Using device: cuda\n",
            "Taille des plongements : 300\n",
            "Nombre de classes: 9\n",
            "Loading weights from model/word2vec/average_embedding_mlp_100/checkpoint.ckpt and starting at epoch 301.\n",
            "Loading optimizer state from model/word2vec/average_embedding_mlp_100/checkpoint.optim and starting at epoch 301.\n",
            "Loading random states from model/word2vec/average_embedding_mlp_100/checkpoint.randomstate and starting at epoch 301.\n",
            "Restoring data from model/word2vec/average_embedding_mlp_100/checkpoint_epoch_294.ckpt\n",
            "Found best checkpoint at epoch: 294\n",
            "lr: 0.01, loss: 0.437992, acc: 85.4829, fscore_macro: 0.841185, val_loss: 0.633513, val_acc: 77.1171, val_fscore_macro: 0.759666\n",
            "Loading checkpoint model/word2vec/average_embedding_mlp_100/checkpoint_epoch_294.ckpt\n",
            "Running test\n",
            "Test steps: 32 0.25s test_loss: 0.391071 test_acc: 88.800000 test_fscore_macro: 0.860782    \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}